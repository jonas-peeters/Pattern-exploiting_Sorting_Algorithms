	.text
	.file	"benchy.c"
	.globl	partition_quick_block   # -- Begin function partition_quick_block
	.p2align	4, 0x90
	.type	partition_quick_block,@function
partition_quick_block:                  # @partition_quick_block
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$1000, %rsp             # imm = 0x3E8
	.cfi_def_cfa_offset 1056
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %r10d
	movl	%esi, %r15d
	movl	%edx, %eax
	subl	%esi, %eax
	movslq	%edx, %rdx
	cmpl	$1, %eax
	jg	.LBB0_2
# %bb.1:
	movl	(%rdi,%rdx,4), %r8d
	jmp	.LBB0_3
.LBB0_2:
	leal	(%r10,%r15), %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	movslq	%r15d, %rbx
	movl	(%rdi,%rbx,4), %esi
	movslq	%ecx, %r9
	movl	(%rdi,%r9,4), %r8d
	movl	(%rdi,%rdx,4), %ebp
	cmpl	%r8d, %esi
	movl	%r8d, %eax
	cmovlel	%esi, %eax
	movl	%r8d, %ecx
	cmovgel	%esi, %ecx
	cmpl	%ebp, %eax
	cmovgl	%ebp, %eax
	cmpl	%ebp, %ecx
	cmovll	%ebp, %ecx
	addl	%esi, %r8d
	addl	%ebp, %r8d
	subl	%eax, %r8d
	subl	%ecx, %r8d
	movl	%eax, (%rdi,%rbx,4)
	movl	%r8d, (%rdi,%rdx,4)
	movl	%ecx, (%rdi,%r9,4)
.LBB0_3:
	movq	%rdx, -88(%rsp)         # 8-byte Spill
	decl	%r10d
	movl	%r10d, %ebp
	subl	%r15d, %ebp
	xorl	%r14d, %r14d
	cmpl	$256, %ebp              # imm = 0x100
	movq	%rdi, -120(%rsp)        # 8-byte Spill
	jl	.LBB0_19
# %bb.4:
	leaq	12(%rdi), %rax
	movq	%rax, -40(%rsp)         # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%r11d, %r11d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	jmp	.LBB0_6
	.p2align	4, 0x90
.LBB0_5:                                #   in Loop: Header=BB0_6 Depth=1
	addl	%ebp, %r14d
	addl	%ebp, %edx
	leal	128(%r15), %eax
	subl	%ebp, %r11d
	cmovel	%eax, %r15d
	leal	-128(%r10), %eax
	subl	%ebp, %ecx
	cmovel	%eax, %r10d
	movl	%r10d, %ebp
	subl	%r15d, %ebp
	cmpl	$255, %ebp
	jle	.LBB0_20
.LBB0_6:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_8 Depth 2
                                        #     Child Loop BB0_11 Depth 2
                                        #     Child Loop BB0_16 Depth 2
	movslq	%r15d, %r13
	testl	%r11d, %r11d
	jne	.LBB0_9
# %bb.7:                                #   in Loop: Header=BB0_6 Depth=1
	movq	-40(%rsp), %rax         # 8-byte Reload
	leaq	(%rax,%r13,4), %rax
	xorl	%r14d, %r14d
	xorl	%ebp, %ebp
	xorl	%r11d, %r11d
	.p2align	4, 0x90
.LBB0_8:                                #   Parent Loop BB0_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	%r11d, %esi
	movl	%ebp, 480(%rsp,%rsi,4)
	xorl	%esi, %esi
	cmpl	-12(%rax,%rbp,4), %r8d
	setle	%sil
	addl	%r11d, %esi
	leal	1(%rbp), %ebx
	movl	%ebx, 480(%rsp,%rsi,4)
	xorl	%ebx, %ebx
	cmpl	-8(%rax,%rbp,4), %r8d
	setle	%bl
	addl	%esi, %ebx
	leal	2(%rbp), %esi
	movl	%esi, 480(%rsp,%rbx,4)
	xorl	%esi, %esi
	cmpl	-4(%rax,%rbp,4), %r8d
	setle	%sil
	addl	%ebx, %esi
	leal	3(%rbp), %ebx
	movl	%ebx, 480(%rsp,%rsi,4)
	xorl	%r11d, %r11d
	cmpl	(%rax,%rbp,4), %r8d
	setle	%r11b
	addl	%esi, %r11d
	addq	$4, %rbp
	cmpq	$128, %rbp
	jne	.LBB0_8
.LBB0_9:                                #   in Loop: Header=BB0_6 Depth=1
	movslq	%r10d, %r12
	testl	%ecx, %ecx
	jne	.LBB0_12
# %bb.10:                               #   in Loop: Header=BB0_6 Depth=1
	leaq	(%rdi,%r12,4), %rbp
	xorl	%edx, %edx
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_11:                               #   Parent Loop BB0_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	%ecx, %esi
	movl	%eax, -32(%rsp,%rsi,4)
	xorl	%esi, %esi
	cmpl	(%rbp), %r8d
	setge	%sil
	addl	%ecx, %esi
	movl	%eax, %ecx
	orl	$1, %ecx
	movl	%ecx, -32(%rsp,%rsi,4)
	xorl	%ecx, %ecx
	cmpl	-4(%rbp), %r8d
	setge	%cl
	addl	%esi, %ecx
	movl	%eax, %esi
	orl	$2, %esi
	movl	%esi, -32(%rsp,%rcx,4)
	xorl	%esi, %esi
	cmpl	-8(%rbp), %r8d
	setge	%sil
	addl	%ecx, %esi
	movl	%eax, %ecx
	orl	$3, %ecx
	movl	%ecx, -32(%rsp,%rsi,4)
	xorl	%ecx, %ecx
	cmpl	-12(%rbp), %r8d
	setge	%cl
	addl	%esi, %ecx
	addq	$4, %rax
	addq	$-16, %rbp
	cmpq	$128, %rax
	jne	.LBB0_11
.LBB0_12:                               #   in Loop: Header=BB0_6 Depth=1
	cmpl	%ecx, %r11d
	movl	%ecx, %ebp
	cmovlel	%r11d, %ebp
	testl	%ebp, %ebp
	jle	.LBB0_5
# %bb.13:                               #   in Loop: Header=BB0_6 Depth=1
	movq	%r10, -48(%rsp)         # 8-byte Spill
	movq	%r15, -96(%rsp)         # 8-byte Spill
	movq	%rdx, -112(%rsp)        # 8-byte Spill
	movslq	%edx, %rdx
	movq	%r14, -56(%rsp)         # 8-byte Spill
	movslq	%r14d, %rsi
	movl	%ebp, %r15d
	movl	%r15d, %edi
	andl	$1, %edi
	movl	%ebp, -100(%rsp)        # 4-byte Spill
	cmpl	$1, %ebp
	movq	%rdx, -72(%rsp)         # 8-byte Spill
	movq	%rsi, -80(%rsp)         # 8-byte Spill
	movq	%rdi, -64(%rsp)         # 8-byte Spill
	jne	.LBB0_15
# %bb.14:                               #   in Loop: Header=BB0_6 Depth=1
	xorl	%r9d, %r9d
	movq	-120(%rsp), %rdi        # 8-byte Reload
	jmp	.LBB0_17
	.p2align	4, 0x90
.LBB0_15:                               #   in Loop: Header=BB0_6 Depth=1
	leaq	484(%rsp), %rax
	leaq	(%rax,%rsi,4), %r10
	leaq	-28(%rsp), %rax
	leaq	(%rax,%rdx,4), %rsi
	subq	%rdi, %r15
	xorl	%r9d, %r9d
	movq	-120(%rsp), %rdi        # 8-byte Reload
	.p2align	4, 0x90
.LBB0_16:                               #   Parent Loop BB0_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movslq	-4(%r10,%r9,4), %rbx
	addq	%r13, %rbx
	movslq	-4(%rsi,%r9,4), %rdx
	movq	%r12, %r14
	subq	%rdx, %r14
	movl	(%rdi,%rbx,4), %edx
	movl	(%rdi,%r14,4), %eax
	movl	%eax, (%rdi,%rbx,4)
	movl	%edx, (%rdi,%r14,4)
	movslq	(%r10,%r9,4), %rax
	addq	%r13, %rax
	movslq	(%rsi,%r9,4), %rdx
	movq	%r12, %rbx
	subq	%rdx, %rbx
	movl	(%rdi,%rax,4), %edx
	movl	(%rdi,%rbx,4), %ebp
	movl	%ebp, (%rdi,%rax,4)
	movl	%edx, (%rdi,%rbx,4)
	addq	$2, %r9
	cmpq	%r9, %r15
	jne	.LBB0_16
.LBB0_17:                               #   in Loop: Header=BB0_6 Depth=1
	cmpq	$0, -64(%rsp)           # 8-byte Folded Reload
	movq	-96(%rsp), %r15         # 8-byte Reload
	movq	-48(%rsp), %r10         # 8-byte Reload
	movq	-56(%rsp), %r14         # 8-byte Reload
	movq	-112(%rsp), %rdx        # 8-byte Reload
	movl	-100(%rsp), %ebp        # 4-byte Reload
	je	.LBB0_5
# %bb.18:                               #   in Loop: Header=BB0_6 Depth=1
	movq	-80(%rsp), %rax         # 8-byte Reload
	addq	%r9, %rax
	movslq	480(%rsp,%rax,4), %rax
	addq	%r13, %rax
	movq	-72(%rsp), %rsi         # 8-byte Reload
	addq	%r9, %rsi
	movslq	-32(%rsp,%rsi,4), %rsi
	subq	%rsi, %r12
	movl	(%rdi,%rax,4), %esi
	movl	(%rdi,%r12,4), %ebx
	movl	%ebx, (%rdi,%rax,4)
	movl	%esi, (%rdi,%r12,4)
	jmp	.LBB0_5
.LBB0_19:
	xorl	%edx, %edx
	xorl	%r11d, %r11d
	xorl	%ecx, %ecx
.LBB0_20:
	movl	%ecx, %eax
	orl	%r11d, %eax
	movq	%r15, -96(%rsp)         # 8-byte Spill
	je	.LBB0_25
# %bb.21:
	leal	-127(%rbp), %r12d
	testl	%ecx, %ecx
	je	.LBB0_30
# %bb.22:
	movl	$128, %r13d
	xorl	%r14d, %r14d
	cmpl	$128, %ebp
	movq	%rdx, -112(%rsp)        # 8-byte Spill
	jl	.LBB0_49
# %bb.23:
	movslq	%r15d, %r13
	movl	%r12d, %esi
	leaq	-1(%rsi), %rdx
	movl	%esi, %r15d
	andl	$3, %r15d
	movl	%r12d, %ebx
	cmpq	$3, %rdx
	jae	.LBB0_35
# %bb.24:
	xorl	%r12d, %r12d
	jmp	.LBB0_37
.LBB0_25:
	movq	%r10, %rdi
	leal	1(%rbp), %r13d
	movl	%r13d, %r12d
	shrl	$31, %r12d
	addl	%r13d, %r12d
	sarl	%r12d
	subl	%r12d, %r13d
	xorl	%r14d, %r14d
	movl	$0, %r11d
	movl	$0, %ecx
	testl	%ebp, %ebp
	jle	.LBB0_28
# %bb.26:
	movslq	-96(%rsp), %rcx         # 4-byte Folded Reload
	movslq	%edi, %rdx
	movslq	%r12d, %rbx
	movq	-120(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%rdx,4), %r15
	xorl	%r14d, %r14d
	leaq	(%rax,%rcx,4), %r10
	xorl	%ebp, %ebp
	xorl	%r9d, %r9d
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB0_27:                               # =>This Inner Loop Header: Depth=1
	movl	%esi, %ecx
	movl	%ebp, 480(%rsp,%rcx,4)
	xorl	%r11d, %r11d
	cmpl	(%r10,%rbp,4), %r8d
	setle	%r11b
	addl	%esi, %r11d
	movl	%r9d, %ecx
	movl	%ebp, -32(%rsp,%rcx,4)
	xorl	%ecx, %ecx
	cmpl	(%r15), %r8d
	setge	%cl
	addl	%r9d, %ecx
	incq	%rbp
	addq	$-4, %r15
	movl	%ecx, %r9d
	movl	%r11d, %esi
	cmpq	%rbx, %rbp
	jl	.LBB0_27
.LBB0_28:
	cmpl	%r13d, %r12d
	jge	.LBB0_33
# %bb.29:
	leal	-1(%r13), %edx
	movl	%ecx, %esi
	movl	%edx, -32(%rsp,%rsi,4)
	movq	%rdi, %r10
	movl	%r10d, %edx
	subl	%r13d, %edx
	incl	%edx
	movslq	%edx, %rdx
	xorl	%esi, %esi
	movq	-120(%rsp), %rax        # 8-byte Reload
	cmpl	(%rax,%rdx,4), %r8d
	movl	$0, %r14d
	setge	%sil
	addl	%ecx, %esi
	xorl	%eax, %eax
	movq	%rax, -112(%rsp)        # 8-byte Spill
	movl	%esi, %ecx
	jmp	.LBB0_49
.LBB0_30:
	cmpl	$128, %ebp
	jl	.LBB0_34
# %bb.31:
	movl	%r12d, %eax
	movslq	%r10d, %r12
	movl	%eax, %r9d
	movl	%eax, %r13d
	leaq	-1(%r13), %rcx
	movl	%r13d, %edi
	andl	$3, %edi
	xorl	%eax, %eax
	cmpq	$3, %rcx
	jae	.LBB0_42
# %bb.32:
	movq	%rax, -112(%rsp)        # 8-byte Spill
	xorl	%r15d, %r15d
	xorl	%ecx, %ecx
	jmp	.LBB0_44
.LBB0_33:
	xorl	%eax, %eax
	movq	%rax, -112(%rsp)        # 8-byte Spill
	movq	%rdi, %r10
	jmp	.LBB0_49
.LBB0_34:
	xorl	%eax, %eax
	movq	%rax, -112(%rsp)        # 8-byte Spill
	xorl	%ecx, %ecx
	movl	%r12d, %r13d
	jmp	.LBB0_48
.LBB0_35:
	movq	-120(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r13,4), %rbp
	addq	$12, %rbp
	subq	%r15, %rsi
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB0_36:                               # =>This Inner Loop Header: Depth=1
	movslq	%r11d, %rdx
	movl	%r12d, 480(%rsp,%rdx,4)
	xorl	%edi, %edi
	cmpl	-12(%rbp,%r12,4), %r8d
	setle	%dil
	addl	%edi, %edx
	movslq	%edx, %rdx
	leal	1(%r12), %edi
	movl	%edi, 480(%rsp,%rdx,4)
	xorl	%edi, %edi
	cmpl	-8(%rbp,%r12,4), %r8d
	setle	%dil
	addl	%edi, %edx
	movslq	%edx, %rdx
	leal	2(%r12), %edi
	movl	%edi, 480(%rsp,%rdx,4)
	xorl	%edi, %edi
	cmpl	-4(%rbp,%r12,4), %r8d
	setle	%dil
	addl	%edi, %edx
	movslq	%edx, %r11
	leal	3(%r12), %edx
	movl	%edx, 480(%rsp,%r11,4)
	xorl	%edx, %edx
	cmpl	(%rbp,%r12,4), %r8d
	setle	%dl
	addl	%edx, %r11d
	addq	$4, %r12
	cmpq	%r12, %rsi
	jne	.LBB0_36
.LBB0_37:
	xorl	%r14d, %r14d
	testq	%r15, %r15
	je	.LBB0_41
# %bb.38:
	movq	-120(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r13,4), %rsi
	.p2align	4, 0x90
.LBB0_39:                               # =>This Inner Loop Header: Depth=1
	movslq	%r11d, %r11
	movl	%r12d, 480(%rsp,%r11,4)
	xorl	%edx, %edx
	cmpl	(%rsi,%r12,4), %r8d
	setle	%dl
	addl	%edx, %r11d
	incq	%r12
	decq	%r15
	jne	.LBB0_39
# %bb.40:
	xorl	%r14d, %r14d
.LBB0_41:
	movl	%ebx, %r12d
	movl	$128, %r13d
	jmp	.LBB0_49
.LBB0_42:
	movq	%rax, -112(%rsp)        # 8-byte Spill
	movq	-120(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r12,4), %rbp
	subq	%rdi, %r13
	xorl	%r15d, %r15d
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_43:                               # =>This Inner Loop Header: Depth=1
	movl	%ecx, %edx
	movl	%r15d, -32(%rsp,%rdx,4)
	xorl	%edx, %edx
	cmpl	(%rbp), %r8d
	setge	%dl
	addl	%ecx, %edx
	leal	1(%r15), %ecx
	movl	%ecx, -32(%rsp,%rdx,4)
	xorl	%ecx, %ecx
	cmpl	-4(%rbp), %r8d
	setge	%cl
	addl	%edx, %ecx
	leal	2(%r15), %edx
	movl	%edx, -32(%rsp,%rcx,4)
	xorl	%edx, %edx
	cmpl	-8(%rbp), %r8d
	setge	%dl
	addl	%ecx, %edx
	leal	3(%r15), %ecx
	movl	%ecx, -32(%rsp,%rdx,4)
	xorl	%ecx, %ecx
	cmpl	-12(%rbp), %r8d
	setge	%cl
	addl	%edx, %ecx
	addq	$4, %r15
	addq	$-16, %rbp
	cmpq	%r15, %r13
	jne	.LBB0_43
.LBB0_44:
	testq	%rdi, %rdi
	je	.LBB0_47
# %bb.45:
	subq	%r15, %r12
	movq	-120(%rsp), %rax        # 8-byte Reload
	leaq	(%rax,%r12,4), %rsi
	negq	%rdi
	xorl	%eax, %eax
	movq	%rax, -112(%rsp)        # 8-byte Spill
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB0_46:                               # =>This Inner Loop Header: Depth=1
	movl	%ecx, %edx
	movl	%ecx, %ecx
	movl	%r15d, -32(%rsp,%rcx,4)
	xorl	%ecx, %ecx
	cmpl	(%rsi,%rbp,4), %r8d
	setge	%cl
	addl	%edx, %ecx
	decq	%rbp
	incl	%r15d
	cmpq	%rbp, %rdi
	jne	.LBB0_46
.LBB0_47:
	movl	%r9d, %r13d
.LBB0_48:
	movl	$128, %r12d
.LBB0_49:
	cmpl	%ecx, %r11d
	movl	%ecx, %r15d
	cmovlel	%r11d, %r15d
	testl	%r15d, %r15d
	jle	.LBB0_56
# %bb.50:
	movq	%r13, -64(%rsp)         # 8-byte Spill
	movslq	-112(%rsp), %rax        # 4-byte Folded Reload
	movslq	%r14d, %rsi
	movl	%r15d, %r9d
	movl	%r9d, %r13d
	andl	$1, %r13d
	cmpl	$1, %r15d
	movq	%r10, -48(%rsp)         # 8-byte Spill
	movq	%r14, -56(%rsp)         # 8-byte Spill
	movl	%r12d, -100(%rsp)       # 4-byte Spill
	movq	%rax, -72(%rsp)         # 8-byte Spill
	movq	%rsi, -80(%rsp)         # 8-byte Spill
	jne	.LBB0_52
# %bb.51:
	xorl	%ebp, %ebp
	jmp	.LBB0_54
.LBB0_52:
	movq	%r10, %rdx
	leaq	(%rsp,%rsi,4), %r10
	addq	$484, %r10              # imm = 0x1E4
	leaq	(%rsp,%rax,4), %r14
	addq	$-28, %r14
	subq	%r13, %r9
	xorl	%ebp, %ebp
	movslq	-96(%rsp), %rsi         # 4-byte Folded Reload
	movslq	%edx, %r8
	movq	-120(%rsp), %r12        # 8-byte Reload
	.p2align	4, 0x90
.LBB0_53:                               # =>This Inner Loop Header: Depth=1
	movslq	-4(%r10,%rbp,4), %rdx
	addq	%rsi, %rdx
	movslq	-4(%r14,%rbp,4), %rdi
	movq	%r8, %rbx
	subq	%rdi, %rbx
	movl	(%r12,%rdx,4), %edi
	movl	(%r12,%rbx,4), %eax
	movl	%eax, (%r12,%rdx,4)
	movl	%edi, (%r12,%rbx,4)
	movslq	(%r10,%rbp,4), %rax
	addq	%rsi, %rax
	movslq	(%r14,%rbp,4), %rdx
	movq	%r8, %rdi
	subq	%rdx, %rdi
	movl	(%r12,%rax,4), %edx
	movl	(%r12,%rdi,4), %ebx
	movl	%ebx, (%r12,%rax,4)
	movl	%edx, (%r12,%rdi,4)
	addq	$2, %rbp
	cmpq	%rbp, %r9
	jne	.LBB0_53
.LBB0_54:
	testq	%r13, %r13
	movq	-48(%rsp), %r10         # 8-byte Reload
	movq	-56(%rsp), %r14         # 8-byte Reload
	movl	-100(%rsp), %r12d       # 4-byte Reload
	movq	-64(%rsp), %r13         # 8-byte Reload
	je	.LBB0_56
# %bb.55:
	movq	-80(%rsp), %rax         # 8-byte Reload
	addq	%rbp, %rax
	movslq	480(%rsp,%rax,4), %rdx
	movslq	-96(%rsp), %rsi         # 4-byte Folded Reload
	addq	%rdx, %rsi
	movq	-72(%rsp), %rax         # 8-byte Reload
	addq	%rbp, %rax
	movslq	-32(%rsp,%rax,4), %rdx
	movslq	%r10d, %rdi
	subq	%rdx, %rdi
	movq	-120(%rsp), %rax        # 8-byte Reload
	movl	(%rax,%rsi,4), %edx
	movl	(%rax,%rdi,4), %ebp
	movl	%ebp, (%rax,%rsi,4)
	movl	%edx, (%rax,%rdi,4)
.LBB0_56:
	movq	-96(%rsp), %rax         # 8-byte Reload
	addl	%eax, %r12d
	xorl	%ebp, %ebp
	cmpl	%r11d, %ecx
	cmovlel	%r13d, %ebp
	movl	%r10d, %ebx
	subl	%ebp, %ebx
	cmpl	%ecx, %r11d
	cmovlel	%r12d, %eax
	jg	.LBB0_59
# %bb.57:
	movq	%rax, %r14
	cmpl	%r11d, %ecx
	jg	.LBB0_64
# %bb.58:
	movq	-120(%rsp), %r11        # 8-byte Reload
	movq	-88(%rsp), %rax         # 8-byte Reload
	leaq	(%r11,%rax,4), %rcx
	jmp	.LBB0_80
.LBB0_59:
	leal	(%r15,%r14), %ecx
	leal	(%r11,%r14), %esi
	subl	%eax, %ebx
	movslq	%ecx, %r9
	cmpl	%ecx, %esi
	jle	.LBB0_69
# %bb.60:
	movslq	%esi, %rsi
	addl	%r15d, %r10d
	subl	%ebp, %r10d
	subl	%eax, %r10d
	subl	%r11d, %r10d
	leal	-1(%rcx), %r8d
	leal	(%r11,%r14), %ebp
	decl	%ebp
	movq	-120(%rsp), %r11        # 8-byte Reload
	movq	%rax, %r14
	.p2align	4, 0x90
.LBB0_61:                               # =>This Inner Loop Header: Depth=1
	cmpl	476(%rsp,%rsi,4), %ebx
	jne	.LBB0_70
# %bb.62:                               #   in Loop: Header=BB0_61 Depth=1
	decq	%rsi
	decl	%ebx
	decl	%ebp
	cmpq	%r9, %rsi
	jg	.LBB0_61
# %bb.63:
	movl	%r10d, %ebx
	movl	%r8d, %ebp
	jmp	.LBB0_70
.LBB0_64:
	movq	%r10, %rax
	movq	-112(%rsp), %rdx        # 8-byte Reload
	leal	(%r15,%rdx), %r8d
	leal	(%rcx,%rdx), %esi
	movl	%ebx, %r10d
	subl	%r14d, %r10d
	movslq	%r8d, %r9
	cmpl	%r8d, %esi
	jle	.LBB0_74
# %bb.65:
	movslq	%esi, %rsi
	addl	%r15d, %eax
	subl	%ebp, %eax
	subl	%r14d, %eax
	subl	%ecx, %eax
	leal	-1(%r8), %ebp
	addl	%edx, %ecx
	decl	%ecx
	movq	-120(%rsp), %r11        # 8-byte Reload
	.p2align	4, 0x90
.LBB0_66:                               # =>This Inner Loop Header: Depth=1
	cmpl	-36(%rsp,%rsi,4), %r10d
	jne	.LBB0_75
# %bb.67:                               #   in Loop: Header=BB0_66 Depth=1
	decq	%rsi
	decl	%r10d
	decl	%ecx
	cmpq	%r9, %rsi
	jg	.LBB0_66
# %bb.68:
	movl	%eax, %r10d
	movl	%ebp, %ecx
	jmp	.LBB0_75
.LBB0_69:
	leal	(%r11,%r14), %ebp
	decl	%ebp
	movq	-120(%rsp), %r11        # 8-byte Reload
	movq	%rax, %r14
.LBB0_70:
	cmpl	%ecx, %ebp
	jl	.LBB0_73
# %bb.71:
	movslq	%ebx, %rsi
	movslq	%r14d, %rcx
	movslq	%ebp, %rdx
	incq	%rdx
	addq	%rcx, %rsi
	leaq	(%r11,%rsi,4), %rsi
	.p2align	4, 0x90
.LBB0_72:                               # =>This Inner Loop Header: Depth=1
	movslq	476(%rsp,%rdx,4), %rdi
	addq	%rcx, %rdi
	movl	(%rsi), %ebp
	movl	(%r11,%rdi,4), %eax
	movl	%eax, (%rsi)
	movl	%ebp, (%r11,%rdi,4)
	decq	%rdx
	addq	$-4, %rsi
	decl	%ebx
	cmpq	%r9, %rdx
	jg	.LBB0_72
.LBB0_73:
	movq	-88(%rsp), %rax         # 8-byte Reload
	leaq	(%r11,%rax,4), %rcx
	addl	%ebx, %r14d
	incl	%r14d
	movl	%r14d, %r12d
	jmp	.LBB0_80
.LBB0_74:
	addl	%edx, %ecx
	decl	%ecx
	movq	-120(%rsp), %r11        # 8-byte Reload
.LBB0_75:
	cmpl	%r8d, %ecx
	jl	.LBB0_79
# %bb.76:
	movslq	%r10d, %rsi
	movslq	%ecx, %rcx
	incq	%rcx
	movslq	%ebx, %r8
	movq	%r8, %rdi
	subq	%rsi, %rdi
	leaq	(%r11,%rdi,4), %rbp
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB0_77:                               # =>This Inner Loop Header: Depth=1
	movslq	-36(%rsp,%rcx,4), %rdi
	movq	%r8, %rdx
	subq	%rdi, %rdx
	movl	(%rbp,%rsi,4), %edi
	movl	(%r11,%rdx,4), %eax
	movl	%eax, (%rbp,%rsi,4)
	movl	%edi, (%r11,%rdx,4)
	decq	%rcx
	incq	%rsi
	cmpq	%r9, %rcx
	jg	.LBB0_77
# %bb.78:
	subl	%esi, %r10d
.LBB0_79:
	movq	-88(%rsp), %rax         # 8-byte Reload
	leaq	(%r11,%rax,4), %rcx
	subl	%r10d, %ebx
	movl	%ebx, %r14d
	movl	%ebx, %r12d
.LBB0_80:
	movslq	%r14d, %rax
	movl	(%rcx), %edx
	movl	(%r11,%rax,4), %esi
	movl	%esi, (%rcx)
	movl	%edx, (%r11,%rax,4)
	movl	%r12d, %eax
	addq	$1000, %rsp             # imm = 0x3E8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end0:
	.size	partition_quick_block, .Lfunc_end0-partition_quick_block
	.cfi_endproc
                                        # -- End function
	.globl	median_of_three         # -- Begin function median_of_three
	.p2align	4, 0x90
	.type	median_of_three,@function
median_of_three:                        # @median_of_three
	.cfi_startproc
# %bb.0:
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$1, %eax
	jg	.LBB1_2
# %bb.1:
	movslq	%edx, %rax
	movl	(%rdi,%rax,4), %eax
	retq
.LBB1_2:
	leal	(%rdx,%rsi), %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	movslq	%esi, %r8
	movl	(%rdi,%r8,4), %r11d
	movslq	%ecx, %r9
	movl	(%rdi,%r9,4), %eax
	movslq	%edx, %r10
	movl	(%rdi,%r10,4), %ecx
	cmpl	%eax, %r11d
	movl	%eax, %edx
	cmovlel	%r11d, %edx
	movl	%eax, %esi
	cmovgel	%r11d, %esi
	cmpl	%ecx, %edx
	cmovgl	%ecx, %edx
	cmpl	%ecx, %esi
	cmovll	%ecx, %esi
	addl	%r11d, %eax
	addl	%ecx, %eax
	subl	%edx, %eax
	subl	%esi, %eax
	movl	%edx, (%rdi,%r8,4)
	movl	%eax, (%rdi,%r10,4)
	movl	%esi, (%rdi,%r9,4)
	retq
.Lfunc_end1:
	.size	median_of_three, .Lfunc_end1-median_of_three
	.cfi_endproc
                                        # -- End function
	.globl	min                     # -- Begin function min
	.p2align	4, 0x90
	.type	min,@function
min:                                    # @min
	.cfi_startproc
# %bb.0:
	movl	%esi, %eax
	cmpl	%esi, %edi
	cmovlel	%edi, %eax
	retq
.Lfunc_end2:
	.size	min, .Lfunc_end2-min
	.cfi_endproc
                                        # -- End function
	.globl	swap                    # -- Begin function swap
	.p2align	4, 0x90
	.type	swap,@function
swap:                                   # @swap
	.cfi_startproc
# %bb.0:
	movl	(%rdi), %eax
	movl	(%rsi), %ecx
	movl	%ecx, (%rdi)
	movl	%eax, (%rsi)
	retq
.Lfunc_end3:
	.size	swap, .Lfunc_end3-swap
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_block        # -- Begin function sort_quick_block
	.p2align	4, 0x90
	.type	sort_quick_block,@function
sort_quick_block:                       # @sort_quick_block
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	%esi, %edx
	jle	.LBB4_22
# %bb.1:
	movl	%edx, %r15d
	movl	%esi, %ebx
	movq	%rdi, %r14
	.p2align	4, 0x90
.LBB4_2:                                # =>This Inner Loop Header: Depth=1
	movl	%r15d, %esi
	subl	%ebx, %esi
	cmpl	$21, %esi
	jl	.LBB4_4
# %bb.3:                                #   in Loop: Header=BB4_2 Depth=1
	movq	%r14, %rdi
	movl	%ebx, %esi
	movl	%r15d, %edx
	callq	partition_quick_block
	movl	%eax, %ebp
	leal	-1(%rbp), %edx
	movq	%r14, %rdi
	movl	%ebx, %esi
	callq	sort_quick_block
	incl	%ebp
	movl	%ebp, %ebx
	cmpl	%r15d, %ebp
	jl	.LBB4_2
	jmp	.LBB4_22
.LBB4_4:
	testl	%esi, %esi
	jle	.LBB4_22
# %bb.5:
	movslq	%ebx, %rax
	leaq	(%r14,%rax,4), %rax
	movabsq	$-4294967296, %rcx      # imm = 0xFFFFFFFF00000000
	movl	%esi, %r10d
	movl	%r10d, %r8d
	andl	$1, %r8d
	movl	$1, %edx
	cmpl	$1, %esi
	jne	.LBB4_6
.LBB4_16:
	testq	%r8, %r8
	je	.LBB4_22
# %bb.17:
	movl	(%rax,%rdx,4), %esi
	movq	%rdx, %rbx
	shlq	$32, %rbx
	addq	%rcx, %rbx
	.p2align	4, 0x90
.LBB4_18:                               # =>This Inner Loop Header: Depth=1
	movq	%rbx, %rdi
	sarq	$30, %rdi
	movl	(%rax,%rdi), %ebp
	cmpl	%esi, %ebp
	jle	.LBB4_21
# %bb.19:                               #   in Loop: Header=BB4_18 Depth=1
	movl	%ebp, (%rax,%rdx,4)
	addq	%rcx, %rbx
	cmpq	$1, %rdx
	leaq	-1(%rdx), %rdx
	jg	.LBB4_18
# %bb.20:
	xorl	%edx, %edx
.LBB4_21:
	movslq	%edx, %rcx
	movl	%esi, (%rax,%rcx,4)
.LBB4_22:
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB4_6:
	.cfi_def_cfa_offset 48
	movabsq	$8589934592, %r9        # imm = 0x200000000
	subq	%r8, %r10
	movl	$1, %edx
	movabsq	$4294967296, %r11       # imm = 0x100000000
	xorl	%r14d, %r14d
	jmp	.LBB4_7
	.p2align	4, 0x90
.LBB4_15:                               #   in Loop: Header=BB4_7 Depth=1
	movslq	%esi, %rsi
	movl	%ebp, (%rax,%rsi,4)
	addq	$2, %rdx
	addq	%r9, %r14
	addq	%r9, %r11
	addq	$-2, %r10
	je	.LBB4_16
.LBB4_7:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB4_8 Depth 2
                                        #     Child Loop BB4_12 Depth 2
	movl	(%rax,%rdx,4), %ebp
	movq	%r14, %rbx
	movq	%rdx, %rsi
	.p2align	4, 0x90
.LBB4_8:                                #   Parent Loop BB4_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rbx, %rdi
	sarq	$30, %rdi
	movl	(%rax,%rdi), %edi
	cmpl	%ebp, %edi
	jle	.LBB4_11
# %bb.9:                                #   in Loop: Header=BB4_8 Depth=2
	movl	%edi, (%rax,%rsi,4)
	addq	%rcx, %rbx
	cmpq	$1, %rsi
	leaq	-1(%rsi), %rsi
	jg	.LBB4_8
# %bb.10:                               #   in Loop: Header=BB4_7 Depth=1
	xorl	%esi, %esi
.LBB4_11:                               #   in Loop: Header=BB4_7 Depth=1
	movslq	%esi, %rsi
	movl	%ebp, (%rax,%rsi,4)
	leaq	1(%rdx), %rsi
	movl	4(%rax,%rdx,4), %ebp
	movq	%r11, %rbx
	.p2align	4, 0x90
.LBB4_12:                               #   Parent Loop BB4_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rbx, %rdi
	sarq	$30, %rdi
	movl	(%rax,%rdi), %edi
	cmpl	%ebp, %edi
	jle	.LBB4_15
# %bb.13:                               #   in Loop: Header=BB4_12 Depth=2
	movl	%edi, (%rax,%rsi,4)
	addq	%rcx, %rbx
	cmpq	$1, %rsi
	leaq	-1(%rsi), %rsi
	jg	.LBB4_12
# %bb.14:                               #   in Loop: Header=BB4_7 Depth=1
	xorl	%esi, %esi
	jmp	.LBB4_15
.Lfunc_end4:
	.size	sort_quick_block, .Lfunc_end4-sort_quick_block
	.cfi_endproc
                                        # -- End function
	.globl	insertionSort           # -- Begin function insertionSort
	.p2align	4, 0x90
	.type	insertionSort,@function
insertionSort:                          # @insertionSort
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	$2, %esi
	jl	.LBB5_18
# %bb.1:
	movabsq	$-4294967296, %rax      # imm = 0xFFFFFFFF00000000
	movl	%esi, %r10d
	decq	%r10
	movl	%r10d, %r8d
	andl	$1, %r8d
	movl	$1, %r15d
	cmpl	$2, %esi
	jne	.LBB5_2
.LBB5_12:
	testq	%r8, %r8
	je	.LBB5_18
# %bb.13:
	movl	(%rdi,%r15,4), %edx
	movq	%r15, %rsi
	shlq	$32, %rsi
	addq	%rax, %rsi
	.p2align	4, 0x90
.LBB5_14:                               # =>This Inner Loop Header: Depth=1
	movq	%rsi, %rcx
	sarq	$30, %rcx
	movl	(%rdi,%rcx), %ebp
	cmpl	%edx, %ebp
	jle	.LBB5_17
# %bb.15:                               #   in Loop: Header=BB5_14 Depth=1
	movl	%ebp, (%rdi,%r15,4)
	addq	%rax, %rsi
	cmpq	$1, %r15
	leaq	-1(%r15), %r15
	jg	.LBB5_14
# %bb.16:
	xorl	%r15d, %r15d
.LBB5_17:
	movslq	%r15d, %rax
	movl	%edx, (%rdi,%rax,4)
.LBB5_18:
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB5_2:
	.cfi_def_cfa_offset 40
	movabsq	$8589934592, %r9        # imm = 0x200000000
	subq	%r8, %r10
	movl	$1, %r15d
	movabsq	$4294967296, %r11       # imm = 0x100000000
	xorl	%r14d, %r14d
	jmp	.LBB5_3
	.p2align	4, 0x90
.LBB5_11:                               #   in Loop: Header=BB5_3 Depth=1
	movslq	%ebx, %rcx
	movl	%esi, (%rdi,%rcx,4)
	addq	$2, %r15
	addq	%r9, %r14
	addq	%r9, %r11
	addq	$-2, %r10
	je	.LBB5_12
.LBB5_3:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB5_4 Depth 2
                                        #     Child Loop BB5_8 Depth 2
	movl	(%rdi,%r15,4), %esi
	movq	%r14, %rcx
	movq	%r15, %rbx
	.p2align	4, 0x90
.LBB5_4:                                #   Parent Loop BB5_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rcx, %rdx
	sarq	$30, %rdx
	movl	(%rdi,%rdx), %ebp
	cmpl	%esi, %ebp
	jle	.LBB5_7
# %bb.5:                                #   in Loop: Header=BB5_4 Depth=2
	movl	%ebp, (%rdi,%rbx,4)
	addq	%rax, %rcx
	cmpq	$1, %rbx
	leaq	-1(%rbx), %rbx
	jg	.LBB5_4
# %bb.6:                                #   in Loop: Header=BB5_3 Depth=1
	xorl	%ebx, %ebx
.LBB5_7:                                #   in Loop: Header=BB5_3 Depth=1
	movslq	%ebx, %rcx
	movl	%esi, (%rdi,%rcx,4)
	leaq	1(%r15), %rbx
	movl	4(%rdi,%r15,4), %esi
	movq	%r11, %rdx
	.p2align	4, 0x90
.LBB5_8:                                #   Parent Loop BB5_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rdx, %rcx
	sarq	$30, %rcx
	movl	(%rdi,%rcx), %ebp
	cmpl	%esi, %ebp
	jle	.LBB5_11
# %bb.9:                                #   in Loop: Header=BB5_8 Depth=2
	movl	%ebp, (%rdi,%rbx,4)
	addq	%rax, %rdx
	cmpq	$1, %rbx
	leaq	-1(%rbx), %rbx
	jg	.LBB5_8
# %bb.10:                               #   in Loop: Header=BB5_3 Depth=1
	xorl	%ebx, %ebx
	jmp	.LBB5_11
.Lfunc_end5:
	.size	insertionSort, .Lfunc_end5-insertionSort
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5               # -- Begin function random_data
.LCPI6_0:
	.long	0                       # 0x0
	.long	1                       # 0x1
	.long	2                       # 0x2
	.long	3                       # 0x3
	.long	4                       # 0x4
	.long	5                       # 0x5
	.long	6                       # 0x6
	.long	7                       # 0x7
.LCPI6_1:
	.quad	4                       # 0x4
	.quad	5                       # 0x5
	.quad	6                       # 0x6
	.quad	7                       # 0x7
.LCPI6_2:
	.quad	0                       # 0x0
	.quad	1                       # 0x1
	.quad	2                       # 0x2
	.quad	3                       # 0x3
.LCPI6_3:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	6                       # 0x6
	.long	7                       # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI6_4:
	.long	9                       # 0x9
.LCPI6_5:
	.long	17                      # 0x11
.LCPI6_6:
	.long	25                      # 0x19
.LCPI6_7:
	.long	33                      # 0x21
.LCPI6_8:
	.long	41                      # 0x29
.LCPI6_9:
	.long	49                      # 0x31
.LCPI6_10:
	.long	57                      # 0x39
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI6_11:
	.quad	64                      # 0x40
	.text
	.globl	random_data
	.p2align	4, 0x90
	.type	random_data,@function
random_data:                            # @random_data
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	pushq	%rax
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -24
	.cfi_offset %rbp, -16
	movl	%esi, %ebp
	movq	%rdi, %rbx
	callq	clock
	movl	%eax, %edi
	callq	srand
	testl	%ebp, %ebp
	jle	.LBB6_12
# %bb.1:
	movl	%ebp, %eax
	cmpl	$31, %ebp
	ja	.LBB6_3
# %bb.2:
	xorl	%ecx, %ecx
	jmp	.LBB6_11
.LBB6_3:
	movl	%eax, %ecx
	andl	$-32, %ecx
	leaq	-32(%rcx), %rdi
	movq	%rdi, %rdx
	shrq	$5, %rdx
	incq	%rdx
	movl	%edx, %esi
	andl	$1, %esi
	testq	%rdi, %rdi
	je	.LBB6_4
# %bb.5:
	movq	%rsi, %rdi
	subq	%rdx, %rdi
	vmovdqa	.LCPI6_1(%rip), %ymm0   # ymm0 = [4,5,6,7]
	vmovdqa	.LCPI6_2(%rip), %ymm1   # ymm1 = [0,1,2,3]
	xorl	%edx, %edx
	vmovdqa	.LCPI6_3(%rip), %ymm2   # ymm2 = [0,2,4,6,4,6,6,7]
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpbroadcastd	.LCPI6_4(%rip), %ymm4 # ymm4 = [9,9,9,9,9,9,9,9]
	vpbroadcastd	.LCPI6_5(%rip), %ymm5 # ymm5 = [17,17,17,17,17,17,17,17]
	vpbroadcastd	.LCPI6_6(%rip), %ymm6 # ymm6 = [25,25,25,25,25,25,25,25]
	vpbroadcastd	.LCPI6_7(%rip), %ymm7 # ymm7 = [33,33,33,33,33,33,33,33]
	vpbroadcastd	.LCPI6_8(%rip), %ymm8 # ymm8 = [41,41,41,41,41,41,41,41]
	vpbroadcastd	.LCPI6_9(%rip), %ymm9 # ymm9 = [49,49,49,49,49,49,49,49]
	vpbroadcastd	.LCPI6_10(%rip), %ymm10 # ymm10 = [57,57,57,57,57,57,57,57]
	vpbroadcastq	.LCPI6_11(%rip), %ymm11 # ymm11 = [64,64,64,64]
	.p2align	4, 0x90
.LBB6_6:                                # =>This Inner Loop Header: Depth=1
	vpermd	%ymm1, %ymm2, %ymm12
	vpermd	%ymm0, %ymm2, %ymm13
	vinserti128	$1, %xmm13, %ymm12, %ymm12
	vpsubd	%ymm3, %ymm12, %ymm13
	vmovdqu	%ymm13, (%rbx,%rdx,4)
	vpaddd	%ymm4, %ymm12, %ymm13
	vmovdqu	%ymm13, 32(%rbx,%rdx,4)
	vpaddd	%ymm5, %ymm12, %ymm13
	vmovdqu	%ymm13, 64(%rbx,%rdx,4)
	vpaddd	%ymm6, %ymm12, %ymm13
	vmovdqu	%ymm13, 96(%rbx,%rdx,4)
	vpaddd	%ymm7, %ymm12, %ymm13
	vmovdqu	%ymm13, 128(%rbx,%rdx,4)
	vpaddd	%ymm8, %ymm12, %ymm13
	vmovdqu	%ymm13, 160(%rbx,%rdx,4)
	vpaddd	%ymm9, %ymm12, %ymm13
	vmovdqu	%ymm13, 192(%rbx,%rdx,4)
	vpaddd	%ymm10, %ymm12, %ymm12
	vmovdqu	%ymm12, 224(%rbx,%rdx,4)
	addq	$64, %rdx
	vpaddq	%ymm1, %ymm11, %ymm1
	vpaddq	%ymm0, %ymm11, %ymm0
	addq	$2, %rdi
	jne	.LBB6_6
# %bb.7:
	vpermd	%ymm1, %ymm2, %ymm1
	vpermd	%ymm0, %ymm2, %ymm0
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	testq	%rsi, %rsi
	je	.LBB6_10
.LBB6_9:
	vpcmpeqd	%ymm1, %ymm1, %ymm1
	vpsubd	%ymm1, %ymm0, %ymm1
	vpbroadcastd	.LCPI6_4(%rip), %ymm2 # ymm2 = [9,9,9,9,9,9,9,9]
	vpaddd	%ymm2, %ymm0, %ymm2
	vpbroadcastd	.LCPI6_5(%rip), %ymm3 # ymm3 = [17,17,17,17,17,17,17,17]
	vpaddd	%ymm3, %ymm0, %ymm3
	vpbroadcastd	.LCPI6_6(%rip), %ymm4 # ymm4 = [25,25,25,25,25,25,25,25]
	vpaddd	%ymm4, %ymm0, %ymm0
	vmovdqu	%ymm1, (%rbx,%rdx,4)
	vmovdqu	%ymm2, 32(%rbx,%rdx,4)
	vmovdqu	%ymm3, 64(%rbx,%rdx,4)
	vmovdqu	%ymm0, 96(%rbx,%rdx,4)
.LBB6_10:
	cmpq	%rax, %rcx
	je	.LBB6_12
	.p2align	4, 0x90
.LBB6_11:                               # =>This Inner Loop Header: Depth=1
	leaq	1(%rcx), %rdx
	movl	%edx, (%rbx,%rcx,4)
	movq	%rdx, %rcx
	cmpq	%rdx, %rax
	jne	.LBB6_11
.LBB6_12:
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.LBB6_4:
	.cfi_def_cfa_offset 32
	vmovdqa	.LCPI6_0(%rip), %ymm0   # ymm0 = [0,1,2,3,4,5,6,7]
	xorl	%edx, %edx
	testq	%rsi, %rsi
	jne	.LBB6_9
	jmp	.LBB6_10
.Lfunc_end6:
	.size	random_data, .Lfunc_end6-random_data
	.cfi_endproc
                                        # -- End function
	.globl	insertionSortOptimized  # -- Begin function insertionSortOptimized
	.p2align	4, 0x90
	.type	insertionSortOptimized,@function
insertionSortOptimized:                 # @insertionSortOptimized
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	$2, %esi
	jl	.LBB7_18
# %bb.1:
	movabsq	$-4294967296, %rax      # imm = 0xFFFFFFFF00000000
	movl	%esi, %r10d
	decq	%r10
	movl	%r10d, %r8d
	andl	$1, %r8d
	movl	$1, %r15d
	cmpl	$2, %esi
	jne	.LBB7_2
.LBB7_12:
	testq	%r8, %r8
	je	.LBB7_18
# %bb.13:
	movl	(%rdi,%r15,4), %edx
	movq	%r15, %rsi
	shlq	$32, %rsi
	addq	%rax, %rsi
	.p2align	4, 0x90
.LBB7_14:                               # =>This Inner Loop Header: Depth=1
	movq	%rsi, %rcx
	sarq	$30, %rcx
	movl	(%rdi,%rcx), %ebp
	cmpl	%edx, %ebp
	jle	.LBB7_17
# %bb.15:                               #   in Loop: Header=BB7_14 Depth=1
	movl	%ebp, (%rdi,%r15,4)
	addq	%rax, %rsi
	cmpq	$1, %r15
	leaq	-1(%r15), %r15
	jg	.LBB7_14
# %bb.16:
	xorl	%r15d, %r15d
.LBB7_17:
	movslq	%r15d, %rax
	movl	%edx, (%rdi,%rax,4)
.LBB7_18:
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB7_2:
	.cfi_def_cfa_offset 40
	movabsq	$8589934592, %r9        # imm = 0x200000000
	subq	%r8, %r10
	movl	$1, %r15d
	movabsq	$4294967296, %r11       # imm = 0x100000000
	xorl	%r14d, %r14d
	jmp	.LBB7_3
	.p2align	4, 0x90
.LBB7_11:                               #   in Loop: Header=BB7_3 Depth=1
	movslq	%ebx, %rcx
	movl	%esi, (%rdi,%rcx,4)
	addq	$2, %r15
	addq	%r9, %r14
	addq	%r9, %r11
	addq	$-2, %r10
	je	.LBB7_12
.LBB7_3:                                # =>This Loop Header: Depth=1
                                        #     Child Loop BB7_4 Depth 2
                                        #     Child Loop BB7_8 Depth 2
	movl	(%rdi,%r15,4), %esi
	movq	%r14, %rcx
	movq	%r15, %rbx
	.p2align	4, 0x90
.LBB7_4:                                #   Parent Loop BB7_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rcx, %rdx
	sarq	$30, %rdx
	movl	(%rdi,%rdx), %ebp
	cmpl	%esi, %ebp
	jle	.LBB7_7
# %bb.5:                                #   in Loop: Header=BB7_4 Depth=2
	movl	%ebp, (%rdi,%rbx,4)
	addq	%rax, %rcx
	cmpq	$1, %rbx
	leaq	-1(%rbx), %rbx
	jg	.LBB7_4
# %bb.6:                                #   in Loop: Header=BB7_3 Depth=1
	xorl	%ebx, %ebx
.LBB7_7:                                #   in Loop: Header=BB7_3 Depth=1
	movslq	%ebx, %rcx
	movl	%esi, (%rdi,%rcx,4)
	leaq	1(%r15), %rbx
	movl	4(%rdi,%r15,4), %esi
	movq	%r11, %rdx
	.p2align	4, 0x90
.LBB7_8:                                #   Parent Loop BB7_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rdx, %rcx
	sarq	$30, %rcx
	movl	(%rdi,%rcx), %ebp
	cmpl	%esi, %ebp
	jle	.LBB7_11
# %bb.9:                                #   in Loop: Header=BB7_8 Depth=2
	movl	%ebp, (%rdi,%rbx,4)
	addq	%rax, %rdx
	cmpq	$1, %rbx
	leaq	-1(%rbx), %rbx
	jg	.LBB7_8
# %bb.10:                               #   in Loop: Header=BB7_3 Depth=1
	xorl	%ebx, %ebx
	jmp	.LBB7_11
.Lfunc_end7:
	.size	insertionSortOptimized, .Lfunc_end7-insertionSortOptimized
	.cfi_endproc
                                        # -- End function
	.globl	max                     # -- Begin function max
	.p2align	4, 0x90
	.type	max,@function
max:                                    # @max
	.cfi_startproc
# %bb.0:
	movl	%esi, %eax
	cmpl	%esi, %edi
	cmovgel	%edi, %eax
	retq
.Lfunc_end8:
	.size	max, .Lfunc_end8-max
	.cfi_endproc
                                        # -- End function
	.globl	sort_pair               # -- Begin function sort_pair
	.p2align	4, 0x90
	.type	sort_pair,@function
sort_pair:                              # @sort_pair
	.cfi_startproc
# %bb.0:
	movl	(%rdi), %eax
	movl	(%rsi), %ecx
	cmpl	%ecx, %eax
	movl	%ecx, %edx
	cmovll	%eax, %edx
	movl	%edx, (%rdi)
	cmovll	%ecx, %eax
	movl	%eax, (%rsi)
	retq
.Lfunc_end9:
	.size	sort_pair, .Lfunc_end9-sort_pair
	.cfi_endproc
                                        # -- End function
	.globl	median_of_three_auto_finish # -- Begin function median_of_three_auto_finish
	.p2align	4, 0x90
	.type	median_of_three_auto_finish,@function
median_of_three_auto_finish:            # @median_of_three_auto_finish
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset %rbx, -16
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %r8d
	subl	%esi, %r8d
	cmpl	$1, %r8d
	jne	.LBB10_2
# %bb.1:
	movl	$1, (%rcx)
	movslq	%esi, %rcx
	movslq	%edx, %r8
	movl	(%rdi,%rcx,4), %eax
	movl	(%rdi,%r8,4), %edx
	cmpl	%edx, %eax
	movl	%edx, %esi
	cmovll	%eax, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovll	%edx, %eax
	movl	%eax, (%rdi,%r8,4)
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.LBB10_2:
	.cfi_def_cfa_offset 16
	leal	(%rdx,%rsi), %r9d
	movl	%r9d, %eax
	shrl	$31, %eax
	addl	%r9d, %eax
	sarl	%eax
	movslq	%esi, %r10
	movl	(%rdi,%r10,4), %esi
	movslq	%eax, %r9
	movl	(%rdi,%r9,4), %eax
	movslq	%edx, %r11
	movl	(%rdi,%r11,4), %edx
	cmpl	%eax, %esi
	movl	%eax, %ebx
	cmovll	%esi, %ebx
	cmovll	%eax, %esi
	cmpl	%edx, %esi
	movl	%edx, %eax
	cmovll	%esi, %eax
	cmovll	%edx, %esi
	cmpl	%eax, %ebx
	movl	%eax, %edx
	cmovll	%ebx, %edx
	cmovgel	%ebx, %eax
	cmpl	$2, %r8d
	jne	.LBB10_4
# %bb.3:
	movl	$1, (%rcx)
	movl	%edx, (%rdi,%r10,4)
	movl	%eax, (%rdi,%r9,4)
	movl	%esi, (%rdi,%r11,4)
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.LBB10_4:
	.cfi_def_cfa_offset 16
	movl	%edx, (%rdi,%r10,4)
	movl	%eax, (%rdi,%r11,4)
	movl	%esi, (%rdi,%r9,4)
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end10:
	.size	median_of_three_auto_finish, .Lfunc_end10-median_of_three_auto_finish
	.cfi_endproc
                                        # -- End function
	.globl	sign                    # -- Begin function sign
	.p2align	4, 0x90
	.type	sign,@function
sign:                                   # @sign
	.cfi_startproc
# %bb.0:
	movl	%edi, %ecx
	sarl	$31, %ecx
	xorl	%eax, %eax
	testl	%edi, %edi
	setne	%al
	orl	%ecx, %eax
	retq
.Lfunc_end11:
	.size	sign, .Lfunc_end11-sign
	.cfi_endproc
                                        # -- End function
	.globl	merging_optimzed        # -- Begin function merging_optimzed
	.p2align	4, 0x90
	.type	merging_optimzed,@function
merging_optimzed:                       # @merging_optimzed
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
	leal	1(%rdx), %r14d
	cmpl	%ecx, %edx
	jge	.LBB12_1
# %bb.8:
	cmpl	%edx, %esi
	jg	.LBB12_1
# %bb.9:
	movslq	%esi, %rax
	leaq	(%r8,%rax,4), %r11
	movl	%esi, %r9d
	movl	%esi, %ebx
	.p2align	4, 0x90
.LBB12_10:                              # =>This Inner Loop Header: Depth=1
	movslq	%r14d, %r14
	movslq	%ebx, %rbx
	movl	(%rdi,%r14,4), %r10d
	movl	(%rdi,%rbx,4), %eax
	xorl	%ebp, %ebp
	xorl	%r15d, %r15d
	cmpl	%r10d, %eax
	setle	%bpl
	setg	%r15b
	cmovgl	%r10d, %eax
	addl	%ebp, %ebx
	addl	%r15d, %r14d
	movl	%eax, (%r11)
	incl	%r9d
	cmpl	%ecx, %r14d
	jg	.LBB12_2
# %bb.11:                               #   in Loop: Header=BB12_10 Depth=1
	addq	$4, %r11
	cmpl	%edx, %ebx
	jle	.LBB12_10
	jmp	.LBB12_2
.LBB12_1:
	movl	%esi, %r9d
	movl	%esi, %ebx
.LBB12_2:
	cmpl	%edx, %ebx
	jg	.LBB12_21
# %bb.3:
	movslq	%ebx, %r10
	movslq	%edx, %r13
	movslq	%r9d, %r9
	cmpq	%r13, %r10
	movq	%r13, %rbx
	cmovgeq	%r10, %rbx
	subq	%r10, %rbx
	incq	%rbx
	cmpq	$31, %rbx
	jbe	.LBB12_19
# %bb.4:
	leaq	(%r8,%r9,4), %rax
	cmpq	%r13, %r10
	movq	%r13, %rdx
	cmovgeq	%r10, %rdx
	leaq	(%rdi,%r10,4), %r12
	leaq	(%rdi,%rdx,4), %rbp
	addq	$4, %rbp
	cmpq	%rbp, %rax
	jae	.LBB12_6
# %bb.5:
	addq	%r9, %rdx
	subq	%r10, %rdx
	leaq	(%r8,%rdx,4), %rdx
	addq	$4, %rdx
	cmpq	%rdx, %r12
	jb	.LBB12_19
.LBB12_6:
	movq	%rbx, -16(%rsp)         # 8-byte Spill
	andq	$-32, %rbx
	movq	%rbx, -8(%rsp)          # 8-byte Spill
	addq	$-32, %rbx
	movq	%rbx, %rdx
	shrq	$5, %rdx
	incq	%rdx
	movl	%edx, %r11d
	andl	$3, %r11d
	cmpq	$96, %rbx
	jae	.LBB12_12
# %bb.7:
	xorl	%edx, %edx
	jmp	.LBB12_14
.LBB12_12:
	leaq	(%r8,%r9,4), %rbp
	addq	$480, %rbp              # imm = 0x1E0
	leaq	(%rdi,%r10,4), %r15
	addq	$480, %r15              # imm = 0x1E0
	movq	%r11, %rbx
	subq	%rdx, %rbx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB12_13:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%r15,%rdx,4), %ymm0
	vmovups	-448(%r15,%rdx,4), %ymm1
	vmovups	-416(%r15,%rdx,4), %ymm2
	vmovups	-384(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -480(%rbp,%rdx,4)
	vmovups	%ymm1, -448(%rbp,%rdx,4)
	vmovups	%ymm2, -416(%rbp,%rdx,4)
	vmovups	%ymm3, -384(%rbp,%rdx,4)
	vmovups	-352(%r15,%rdx,4), %ymm0
	vmovups	-320(%r15,%rdx,4), %ymm1
	vmovups	-288(%r15,%rdx,4), %ymm2
	vmovups	-256(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -352(%rbp,%rdx,4)
	vmovups	%ymm1, -320(%rbp,%rdx,4)
	vmovups	%ymm2, -288(%rbp,%rdx,4)
	vmovups	%ymm3, -256(%rbp,%rdx,4)
	vmovups	-224(%r15,%rdx,4), %ymm0
	vmovups	-192(%r15,%rdx,4), %ymm1
	vmovups	-160(%r15,%rdx,4), %ymm2
	vmovups	-128(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -224(%rbp,%rdx,4)
	vmovups	%ymm1, -192(%rbp,%rdx,4)
	vmovups	%ymm2, -160(%rbp,%rdx,4)
	vmovups	%ymm3, -128(%rbp,%rdx,4)
	vmovups	-96(%r15,%rdx,4), %ymm0
	vmovups	-64(%r15,%rdx,4), %ymm1
	vmovups	-32(%r15,%rdx,4), %ymm2
	vmovups	(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -96(%rbp,%rdx,4)
	vmovups	%ymm1, -64(%rbp,%rdx,4)
	vmovups	%ymm2, -32(%rbp,%rdx,4)
	vmovups	%ymm3, (%rbp,%rdx,4)
	subq	$-128, %rdx
	addq	$4, %rbx
	jne	.LBB12_13
.LBB12_14:
	testq	%r11, %r11
	je	.LBB12_17
# %bb.15:
	leaq	96(,%rdx,4), %rdx
	negq	%r11
	.p2align	4, 0x90
.LBB12_16:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%r12,%rdx), %ymm0
	vmovups	-64(%r12,%rdx), %ymm1
	vmovups	-32(%r12,%rdx), %ymm2
	vmovups	(%r12,%rdx), %ymm3
	vmovups	%ymm0, -96(%rax,%rdx)
	vmovups	%ymm1, -64(%rax,%rdx)
	vmovups	%ymm2, -32(%rax,%rdx)
	vmovups	%ymm3, (%rax,%rdx)
	subq	$-128, %rdx
	incq	%r11
	jne	.LBB12_16
.LBB12_17:
	movq	-8(%rsp), %rax          # 8-byte Reload
	addq	%rax, %r9
	cmpq	%rax, -16(%rsp)         # 8-byte Folded Reload
	je	.LBB12_21
# %bb.18:
	addq	%rax, %r10
.LBB12_19:
	decq	%r10
	.p2align	4, 0x90
.LBB12_20:                              # =>This Inner Loop Header: Depth=1
	movl	4(%rdi,%r10,4), %eax
	movl	%eax, (%r8,%r9,4)
	incq	%r9
	incq	%r10
	cmpq	%r13, %r10
	jl	.LBB12_20
.LBB12_21:
	cmpl	%ecx, %r14d
	jg	.LBB12_25
# %bb.22:
	movslq	%r9d, %r9
	movslq	%r14d, %rax
	movslq	%ecx, %r12
	cmpq	%r12, %rax
	movq	%r12, %r10
	cmovgeq	%rax, %r10
	subq	%rax, %r10
	incq	%r10
	cmpq	$31, %r10
	jbe	.LBB12_23
# %bb.30:
	movl	%esi, -16(%rsp)         # 4-byte Spill
	leaq	(%r8,%r9,4), %r11
	cmpq	%r12, %rax
	movq	%r12, %rdx
	cmovgeq	%rax, %rdx
	leaq	(%rdi,%rax,4), %r15
	leaq	(%rdi,%rdx,4), %rsi
	addq	$4, %rsi
	cmpq	%rsi, %r11
	jae	.LBB12_33
# %bb.31:
	addq	%r9, %rdx
	subq	%rax, %rdx
	leaq	(%r8,%rdx,4), %rdx
	addq	$4, %rdx
	cmpq	%rdx, %r15
	jae	.LBB12_33
# %bb.32:
	movl	-16(%rsp), %esi         # 4-byte Reload
	jmp	.LBB12_23
.LBB12_33:
	movq	%r10, %r14
	andq	$-32, %r14
	leaq	-32(%r14), %rsi
	movq	%rsi, %rdx
	shrq	$5, %rdx
	incq	%rdx
	movl	%edx, %r13d
	andl	$3, %r13d
	cmpq	$96, %rsi
	jae	.LBB12_35
# %bb.34:
	xorl	%edx, %edx
	jmp	.LBB12_37
.LBB12_35:
	leaq	(%rdi,%rax,4), %rbp
	addq	$480, %rbp              # imm = 0x1E0
	leaq	(%r8,%r9,4), %rbx
	addq	$480, %rbx              # imm = 0x1E0
	movq	%r13, %rsi
	subq	%rdx, %rsi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB12_36:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rbp,%rdx,4), %ymm0
	vmovups	-448(%rbp,%rdx,4), %ymm1
	vmovups	-416(%rbp,%rdx,4), %ymm2
	vmovups	-384(%rbp,%rdx,4), %ymm3
	vmovups	%ymm0, -480(%rbx,%rdx,4)
	vmovups	%ymm1, -448(%rbx,%rdx,4)
	vmovups	%ymm2, -416(%rbx,%rdx,4)
	vmovups	%ymm3, -384(%rbx,%rdx,4)
	vmovups	-352(%rbp,%rdx,4), %ymm0
	vmovups	-320(%rbp,%rdx,4), %ymm1
	vmovups	-288(%rbp,%rdx,4), %ymm2
	vmovups	-256(%rbp,%rdx,4), %ymm3
	vmovups	%ymm0, -352(%rbx,%rdx,4)
	vmovups	%ymm1, -320(%rbx,%rdx,4)
	vmovups	%ymm2, -288(%rbx,%rdx,4)
	vmovups	%ymm3, -256(%rbx,%rdx,4)
	vmovups	-224(%rbp,%rdx,4), %ymm0
	vmovups	-192(%rbp,%rdx,4), %ymm1
	vmovups	-160(%rbp,%rdx,4), %ymm2
	vmovups	-128(%rbp,%rdx,4), %ymm3
	vmovups	%ymm0, -224(%rbx,%rdx,4)
	vmovups	%ymm1, -192(%rbx,%rdx,4)
	vmovups	%ymm2, -160(%rbx,%rdx,4)
	vmovups	%ymm3, -128(%rbx,%rdx,4)
	vmovups	-96(%rbp,%rdx,4), %ymm0
	vmovups	-64(%rbp,%rdx,4), %ymm1
	vmovups	-32(%rbp,%rdx,4), %ymm2
	vmovups	(%rbp,%rdx,4), %ymm3
	vmovups	%ymm0, -96(%rbx,%rdx,4)
	vmovups	%ymm1, -64(%rbx,%rdx,4)
	vmovups	%ymm2, -32(%rbx,%rdx,4)
	vmovups	%ymm3, (%rbx,%rdx,4)
	subq	$-128, %rdx
	addq	$4, %rsi
	jne	.LBB12_36
.LBB12_37:
	testq	%r13, %r13
	je	.LBB12_40
# %bb.38:
	leaq	96(,%rdx,4), %rdx
	negq	%r13
	.p2align	4, 0x90
.LBB12_39:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%r15,%rdx), %ymm0
	vmovups	-64(%r15,%rdx), %ymm1
	vmovups	-32(%r15,%rdx), %ymm2
	vmovups	(%r15,%rdx), %ymm3
	vmovups	%ymm0, -96(%r11,%rdx)
	vmovups	%ymm1, -64(%r11,%rdx)
	vmovups	%ymm2, -32(%r11,%rdx)
	vmovups	%ymm3, (%r11,%rdx)
	subq	$-128, %rdx
	incq	%r13
	jne	.LBB12_39
.LBB12_40:
	cmpq	%r14, %r10
	movl	-16(%rsp), %esi         # 4-byte Reload
	je	.LBB12_25
# %bb.41:
	addq	%r14, %rax
	addq	%r14, %r9
.LBB12_23:
	decq	%rax
	leaq	(%r8,%r9,4), %rdx
	.p2align	4, 0x90
.LBB12_24:                              # =>This Inner Loop Header: Depth=1
	movl	4(%rdi,%rax,4), %ebp
	movl	%ebp, (%rdx)
	incq	%rax
	addq	$4, %rdx
	cmpq	%r12, %rax
	jl	.LBB12_24
.LBB12_25:
	cmpl	%ecx, %esi
	jg	.LBB12_29
# %bb.26:
	movslq	%esi, %rax
	movslq	%ecx, %r11
	cmpq	%rax, %r11
	movq	%rax, %r10
	cmovgeq	%r11, %r10
	subq	%rax, %r10
	incq	%r10
	cmpq	$31, %r10
	jbe	.LBB12_27
# %bb.42:
	leaq	(%rdi,%rax,4), %rdx
	cmpq	%rax, %r11
	movq	%rax, %rcx
	cmovgeq	%r11, %rcx
	leaq	(%r8,%rcx,4), %rsi
	addq	$4, %rsi
	cmpq	%rsi, %rdx
	jae	.LBB12_44
# %bb.43:
	leaq	(%rdi,%rcx,4), %rcx
	addq	$4, %rcx
	leaq	(%r8,%rax,4), %rdx
	cmpq	%rcx, %rdx
	jb	.LBB12_27
.LBB12_44:
	movq	%r10, %r9
	andq	$-32, %r9
	leaq	-32(%r9), %rdx
	movq	%rdx, %rsi
	shrq	$5, %rsi
	incq	%rsi
	movl	%esi, %ecx
	andl	$3, %ecx
	cmpq	$96, %rdx
	jae	.LBB12_46
# %bb.45:
	xorl	%esi, %esi
	jmp	.LBB12_48
.LBB12_46:
	leaq	(%rdi,%rax,4), %rbp
	addq	$480, %rbp              # imm = 0x1E0
	leaq	(%r8,%rax,4), %rdx
	addq	$480, %rdx              # imm = 0x1E0
	movq	%rcx, %rbx
	subq	%rsi, %rbx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB12_47:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdx,%rsi,4), %ymm0
	vmovups	-448(%rdx,%rsi,4), %ymm1
	vmovups	-416(%rdx,%rsi,4), %ymm2
	vmovups	-384(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -480(%rbp,%rsi,4)
	vmovups	%ymm1, -448(%rbp,%rsi,4)
	vmovups	%ymm2, -416(%rbp,%rsi,4)
	vmovups	%ymm3, -384(%rbp,%rsi,4)
	vmovups	-352(%rdx,%rsi,4), %ymm0
	vmovups	-320(%rdx,%rsi,4), %ymm1
	vmovups	-288(%rdx,%rsi,4), %ymm2
	vmovups	-256(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -352(%rbp,%rsi,4)
	vmovups	%ymm1, -320(%rbp,%rsi,4)
	vmovups	%ymm2, -288(%rbp,%rsi,4)
	vmovups	%ymm3, -256(%rbp,%rsi,4)
	vmovups	-224(%rdx,%rsi,4), %ymm0
	vmovups	-192(%rdx,%rsi,4), %ymm1
	vmovups	-160(%rdx,%rsi,4), %ymm2
	vmovups	-128(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -224(%rbp,%rsi,4)
	vmovups	%ymm1, -192(%rbp,%rsi,4)
	vmovups	%ymm2, -160(%rbp,%rsi,4)
	vmovups	%ymm3, -128(%rbp,%rsi,4)
	vmovups	-96(%rdx,%rsi,4), %ymm0
	vmovups	-64(%rdx,%rsi,4), %ymm1
	vmovups	-32(%rdx,%rsi,4), %ymm2
	vmovups	(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -96(%rbp,%rsi,4)
	vmovups	%ymm1, -64(%rbp,%rsi,4)
	vmovups	%ymm2, -32(%rbp,%rsi,4)
	vmovups	%ymm3, (%rbp,%rsi,4)
	subq	$-128, %rsi
	addq	$4, %rbx
	jne	.LBB12_47
.LBB12_48:
	testq	%rcx, %rcx
	je	.LBB12_51
# %bb.49:
	addq	%rax, %rsi
	leaq	96(,%rsi,4), %rdx
	negq	%rcx
	.p2align	4, 0x90
.LBB12_50:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%r8,%rdx), %ymm0
	vmovups	-64(%r8,%rdx), %ymm1
	vmovups	-32(%r8,%rdx), %ymm2
	vmovups	(%r8,%rdx), %ymm3
	vmovups	%ymm0, -96(%rdi,%rdx)
	vmovups	%ymm1, -64(%rdi,%rdx)
	vmovups	%ymm2, -32(%rdi,%rdx)
	vmovups	%ymm3, (%rdi,%rdx)
	subq	$-128, %rdx
	incq	%rcx
	jne	.LBB12_50
.LBB12_51:
	cmpq	%r9, %r10
	je	.LBB12_29
# %bb.52:
	addq	%r9, %rax
.LBB12_27:
	decq	%rax
	.p2align	4, 0x90
.LBB12_28:                              # =>This Inner Loop Header: Depth=1
	movl	4(%r8,%rax,4), %ecx
	movl	%ecx, 4(%rdi,%rax,4)
	incq	%rax
	cmpq	%r11, %rax
	jl	.LBB12_28
.LBB12_29:
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.Lfunc_end12:
	.size	merging_optimzed, .Lfunc_end12-merging_optimzed
	.cfi_endproc
                                        # -- End function
	.globl	sort_merge_o            # -- Begin function sort_merge_o
	.p2align	4, 0x90
	.type	sort_merge_o,@function
sort_merge_o:                           # @sort_merge_o
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	%edx, %esi
	jge	.LBB13_43
# %bb.1:
	movq	%rcx, %r14
	movl	%edx, %r15d
	movl	%esi, %ebp
	movq	%rdi, %r13
	leal	(%r15,%rbp), %eax
	movl	%eax, %r12d
	shrl	$31, %r12d
	addl	%eax, %r12d
	sarl	%r12d
	movl	%r12d, %edx
	callq	sort_merge_o
	leal	1(%r12), %ebx
	movq	%r13, %rdi
	movl	%ebx, %esi
	movl	%r15d, %edx
	movq	%r14, %rcx
	callq	sort_merge_o
	cmpl	%r15d, %r12d
	movq	%rbp, 8(%rsp)           # 8-byte Spill
	jge	.LBB13_2
# %bb.11:
	cmpl	%ebp, %r12d
	jl	.LBB13_2
# %bb.12:
	movslq	%ebp, %rax
	leaq	(%r14,%rax,4), %rdx
	movl	%ebp, %r9d
	movl	%ebp, %ecx
	.p2align	4, 0x90
.LBB13_13:                              # =>This Inner Loop Header: Depth=1
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	movl	(%r13,%rbx,4), %esi
	movl	(%r13,%rcx,4), %edi
	xorl	%ebp, %ebp
	xorl	%eax, %eax
	cmpl	%esi, %edi
	setle	%bpl
	setg	%al
	cmovgl	%esi, %edi
	addl	%ebp, %ecx
	addl	%eax, %ebx
	movl	%edi, (%rdx)
	incl	%r9d
	cmpl	%r15d, %ebx
	jg	.LBB13_3
# %bb.14:                               #   in Loop: Header=BB13_13 Depth=1
	addq	$4, %rdx
	cmpl	%r12d, %ecx
	jle	.LBB13_13
.LBB13_3:
	cmpl	%r12d, %ecx
	jle	.LBB13_15
.LBB13_4:
	movq	8(%rsp), %rdi           # 8-byte Reload
	jmp	.LBB13_5
.LBB13_2:
	movl	%ebp, %r9d
	movl	%ebp, %ecx
	cmpl	%r12d, %ecx
	jg	.LBB13_4
.LBB13_15:
	movslq	%ecx, %rcx
	movslq	%r12d, %rdx
	movslq	%r9d, %r9
	cmpq	%rdx, %rcx
	movq	%rdx, %rax
	cmovgeq	%rcx, %rax
	subq	%rcx, %rax
	incq	%rax
	cmpq	$31, %rax
	jbe	.LBB13_16
# %bb.19:
	leaq	(%r14,%r9,4), %r10
	cmpq	%rdx, %rcx
	movq	%rdx, %rsi
	cmovgeq	%rcx, %rsi
	leaq	(,%rcx,4), %r11
	addq	%r13, %r11
	leaq	4(,%rsi,4), %rdi
	addq	%r13, %rdi
	cmpq	%rdi, %r10
	jae	.LBB13_21
# %bb.20:
	addq	%r9, %rsi
	subq	%rcx, %rsi
	leaq	(%r14,%rsi,4), %rsi
	addq	$4, %rsi
	cmpq	%rsi, %r11
	jae	.LBB13_21
.LBB13_16:
	movq	8(%rsp), %rdi           # 8-byte Reload
.LBB13_17:
	decq	%rcx
	.p2align	4, 0x90
.LBB13_18:                              # =>This Inner Loop Header: Depth=1
	movl	4(%r13,%rcx,4), %eax
	movl	%eax, (%r14,%r9,4)
	incq	%r9
	incq	%rcx
	cmpq	%rdx, %rcx
	jl	.LBB13_18
.LBB13_5:
	cmpl	%r15d, %ebx
	jg	.LBB13_39
# %bb.6:
	movslq	%r9d, %r9
	movslq	%ebx, %rax
	movslq	%r15d, %rcx
	cmpq	%rcx, %rax
	movq	%rcx, %r10
	cmovgeq	%rax, %r10
	subq	%rax, %r10
	incq	%r10
	cmpq	$31, %r10
	jbe	.LBB13_37
# %bb.7:
	leaq	(%r14,%r9,4), %r11
	cmpq	%rcx, %rax
	movq	%rcx, %rdx
	cmovgeq	%rax, %rdx
	leaq	(,%rax,4), %r12
	addq	%r13, %r12
	leaq	4(,%rdx,4), %rsi
	addq	%r13, %rsi
	cmpq	%rsi, %r11
	jae	.LBB13_9
# %bb.8:
	addq	%r9, %rdx
	subq	%rax, %rdx
	leaq	(%r14,%rdx,4), %rdx
	addq	$4, %rdx
	cmpq	%rdx, %r12
	jb	.LBB13_37
.LBB13_9:
	movq	%r10, %r8
	andq	$-32, %r8
	leaq	-32(%r8), %rsi
	movq	%rsi, %rdx
	shrq	$5, %rdx
	incq	%rdx
	movl	%edx, %ebp
	andl	$3, %ebp
	cmpq	$96, %rsi
	jae	.LBB13_30
# %bb.10:
	xorl	%edx, %edx
	jmp	.LBB13_32
.LBB13_21:
	movq	%rax, %rsi
	andq	$-32, %rsi
	movq	%rsi, 16(%rsp)          # 8-byte Spill
	addq	$-32, %rsi
	movq	%rsi, %rdi
	shrq	$5, %rdi
	incq	%rdi
	movl	%edi, %r12d
	andl	$3, %r12d
	cmpq	$96, %rsi
	jae	.LBB13_23
# %bb.22:
	xorl	%edi, %edi
	jmp	.LBB13_25
.LBB13_30:
	leaq	480(,%rax,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%r9,4), %rdi
	addq	$480, %rdi              # imm = 0x1E0
	movq	%rbp, %rbx
	subq	%rdx, %rbx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB13_31:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rsi,%rdx,4), %ymm0
	vmovups	-448(%rsi,%rdx,4), %ymm1
	vmovups	-416(%rsi,%rdx,4), %ymm2
	vmovups	-384(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -480(%rdi,%rdx,4)
	vmovups	%ymm1, -448(%rdi,%rdx,4)
	vmovups	%ymm2, -416(%rdi,%rdx,4)
	vmovups	%ymm3, -384(%rdi,%rdx,4)
	vmovups	-352(%rsi,%rdx,4), %ymm0
	vmovups	-320(%rsi,%rdx,4), %ymm1
	vmovups	-288(%rsi,%rdx,4), %ymm2
	vmovups	-256(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -352(%rdi,%rdx,4)
	vmovups	%ymm1, -320(%rdi,%rdx,4)
	vmovups	%ymm2, -288(%rdi,%rdx,4)
	vmovups	%ymm3, -256(%rdi,%rdx,4)
	vmovups	-224(%rsi,%rdx,4), %ymm0
	vmovups	-192(%rsi,%rdx,4), %ymm1
	vmovups	-160(%rsi,%rdx,4), %ymm2
	vmovups	-128(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -224(%rdi,%rdx,4)
	vmovups	%ymm1, -192(%rdi,%rdx,4)
	vmovups	%ymm2, -160(%rdi,%rdx,4)
	vmovups	%ymm3, -128(%rdi,%rdx,4)
	vmovups	-96(%rsi,%rdx,4), %ymm0
	vmovups	-64(%rsi,%rdx,4), %ymm1
	vmovups	-32(%rsi,%rdx,4), %ymm2
	vmovups	(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -96(%rdi,%rdx,4)
	vmovups	%ymm1, -64(%rdi,%rdx,4)
	vmovups	%ymm2, -32(%rdi,%rdx,4)
	vmovups	%ymm3, (%rdi,%rdx,4)
	subq	$-128, %rdx
	addq	$4, %rbx
	jne	.LBB13_31
.LBB13_32:
	testq	%rbp, %rbp
	je	.LBB13_35
# %bb.33:
	leaq	96(,%rdx,4), %rdx
	negq	%rbp
	.p2align	4, 0x90
.LBB13_34:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%r12,%rdx), %ymm0
	vmovups	-64(%r12,%rdx), %ymm1
	vmovups	-32(%r12,%rdx), %ymm2
	vmovups	(%r12,%rdx), %ymm3
	vmovups	%ymm0, -96(%r11,%rdx)
	vmovups	%ymm1, -64(%r11,%rdx)
	vmovups	%ymm2, -32(%r11,%rdx)
	vmovups	%ymm3, (%r11,%rdx)
	subq	$-128, %rdx
	incq	%rbp
	jne	.LBB13_34
.LBB13_35:
	cmpq	%r8, %r10
	movq	8(%rsp), %rdi           # 8-byte Reload
	je	.LBB13_39
# %bb.36:
	addq	%r8, %rax
	addq	%r8, %r9
.LBB13_37:
	decq	%rax
	leaq	(%r14,%r9,4), %rdx
	.p2align	4, 0x90
.LBB13_38:                              # =>This Inner Loop Header: Depth=1
	movl	4(%r13,%rax,4), %esi
	movl	%esi, (%rdx)
	incq	%rax
	addq	$4, %rdx
	cmpq	%rcx, %rax
	jl	.LBB13_38
.LBB13_39:
	cmpl	%r15d, %edi
	jg	.LBB13_43
# %bb.40:
	movslq	%edi, %rax
	movslq	%r15d, %rcx
	cmpq	%rax, %rcx
	movq	%rax, %r9
	cmovgeq	%rcx, %r9
	subq	%rax, %r9
	incq	%r9
	cmpq	$31, %r9
	jbe	.LBB13_41
# %bb.44:
	leaq	(,%rax,4), %rsi
	addq	%r13, %rsi
	cmpq	%rax, %rcx
	movq	%rax, %rdx
	cmovgeq	%rcx, %rdx
	leaq	(%r14,%rdx,4), %rdi
	addq	$4, %rdi
	cmpq	%rdi, %rsi
	jae	.LBB13_46
# %bb.45:
	leaq	4(,%rdx,4), %rdx
	addq	%r13, %rdx
	leaq	(%r14,%rax,4), %rsi
	cmpq	%rdx, %rsi
	jb	.LBB13_41
.LBB13_46:
	movq	%r9, %r8
	andq	$-32, %r8
	leaq	-32(%r8), %rsi
	movq	%rsi, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %edx
	andl	$3, %edx
	cmpq	$96, %rsi
	jae	.LBB13_48
# %bb.47:
	xorl	%ebp, %ebp
	jmp	.LBB13_50
.LBB13_23:
	leaq	(%r14,%r9,4), %rbp
	addq	$480, %rbp              # imm = 0x1E0
	leaq	480(,%rcx,4), %r8
	addq	%r13, %r8
	movq	%r12, %rsi
	subq	%rdi, %rsi
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB13_24:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%r8,%rdi,4), %ymm0
	vmovups	-448(%r8,%rdi,4), %ymm1
	vmovups	-416(%r8,%rdi,4), %ymm2
	vmovups	-384(%r8,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rbp,%rdi,4)
	vmovups	%ymm1, -448(%rbp,%rdi,4)
	vmovups	%ymm2, -416(%rbp,%rdi,4)
	vmovups	%ymm3, -384(%rbp,%rdi,4)
	vmovups	-352(%r8,%rdi,4), %ymm0
	vmovups	-320(%r8,%rdi,4), %ymm1
	vmovups	-288(%r8,%rdi,4), %ymm2
	vmovups	-256(%r8,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rbp,%rdi,4)
	vmovups	%ymm1, -320(%rbp,%rdi,4)
	vmovups	%ymm2, -288(%rbp,%rdi,4)
	vmovups	%ymm3, -256(%rbp,%rdi,4)
	vmovups	-224(%r8,%rdi,4), %ymm0
	vmovups	-192(%r8,%rdi,4), %ymm1
	vmovups	-160(%r8,%rdi,4), %ymm2
	vmovups	-128(%r8,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rbp,%rdi,4)
	vmovups	%ymm1, -192(%rbp,%rdi,4)
	vmovups	%ymm2, -160(%rbp,%rdi,4)
	vmovups	%ymm3, -128(%rbp,%rdi,4)
	vmovups	-96(%r8,%rdi,4), %ymm0
	vmovups	-64(%r8,%rdi,4), %ymm1
	vmovups	-32(%r8,%rdi,4), %ymm2
	vmovups	(%r8,%rdi,4), %ymm3
	vmovups	%ymm0, -96(%rbp,%rdi,4)
	vmovups	%ymm1, -64(%rbp,%rdi,4)
	vmovups	%ymm2, -32(%rbp,%rdi,4)
	vmovups	%ymm3, (%rbp,%rdi,4)
	subq	$-128, %rdi
	addq	$4, %rsi
	jne	.LBB13_24
.LBB13_25:
	testq	%r12, %r12
	je	.LBB13_28
# %bb.26:
	leaq	96(,%rdi,4), %rsi
	negq	%r12
	.p2align	4, 0x90
.LBB13_27:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%r11,%rsi), %ymm0
	vmovups	-64(%r11,%rsi), %ymm1
	vmovups	-32(%r11,%rsi), %ymm2
	vmovups	(%r11,%rsi), %ymm3
	vmovups	%ymm0, -96(%r10,%rsi)
	vmovups	%ymm1, -64(%r10,%rsi)
	vmovups	%ymm2, -32(%r10,%rsi)
	vmovups	%ymm3, (%r10,%rsi)
	subq	$-128, %rsi
	incq	%r12
	jne	.LBB13_27
.LBB13_28:
	movq	16(%rsp), %rsi          # 8-byte Reload
	addq	%rsi, %r9
	cmpq	%rsi, %rax
	movq	8(%rsp), %rdi           # 8-byte Reload
	je	.LBB13_5
# %bb.29:
	addq	%rsi, %rcx
	jmp	.LBB13_17
.LBB13_48:
	leaq	480(,%rax,4), %rbx
	addq	%r13, %rbx
	leaq	(%r14,%rax,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	movq	%rdx, %rdi
	subq	%rbp, %rdi
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB13_49:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rsi,%rbp,4), %ymm0
	vmovups	-448(%rsi,%rbp,4), %ymm1
	vmovups	-416(%rsi,%rbp,4), %ymm2
	vmovups	-384(%rsi,%rbp,4), %ymm3
	vmovups	%ymm0, -480(%rbx,%rbp,4)
	vmovups	%ymm1, -448(%rbx,%rbp,4)
	vmovups	%ymm2, -416(%rbx,%rbp,4)
	vmovups	%ymm3, -384(%rbx,%rbp,4)
	vmovups	-352(%rsi,%rbp,4), %ymm0
	vmovups	-320(%rsi,%rbp,4), %ymm1
	vmovups	-288(%rsi,%rbp,4), %ymm2
	vmovups	-256(%rsi,%rbp,4), %ymm3
	vmovups	%ymm0, -352(%rbx,%rbp,4)
	vmovups	%ymm1, -320(%rbx,%rbp,4)
	vmovups	%ymm2, -288(%rbx,%rbp,4)
	vmovups	%ymm3, -256(%rbx,%rbp,4)
	vmovups	-224(%rsi,%rbp,4), %ymm0
	vmovups	-192(%rsi,%rbp,4), %ymm1
	vmovups	-160(%rsi,%rbp,4), %ymm2
	vmovups	-128(%rsi,%rbp,4), %ymm3
	vmovups	%ymm0, -224(%rbx,%rbp,4)
	vmovups	%ymm1, -192(%rbx,%rbp,4)
	vmovups	%ymm2, -160(%rbx,%rbp,4)
	vmovups	%ymm3, -128(%rbx,%rbp,4)
	vmovups	-96(%rsi,%rbp,4), %ymm0
	vmovups	-64(%rsi,%rbp,4), %ymm1
	vmovups	-32(%rsi,%rbp,4), %ymm2
	vmovups	(%rsi,%rbp,4), %ymm3
	vmovups	%ymm0, -96(%rbx,%rbp,4)
	vmovups	%ymm1, -64(%rbx,%rbp,4)
	vmovups	%ymm2, -32(%rbx,%rbp,4)
	vmovups	%ymm3, (%rbx,%rbp,4)
	subq	$-128, %rbp
	addq	$4, %rdi
	jne	.LBB13_49
.LBB13_50:
	testq	%rdx, %rdx
	je	.LBB13_53
# %bb.51:
	addq	%rax, %rbp
	leaq	96(,%rbp,4), %rsi
	negq	%rdx
	.p2align	4, 0x90
.LBB13_52:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%r14,%rsi), %ymm0
	vmovups	-64(%r14,%rsi), %ymm1
	vmovups	-32(%r14,%rsi), %ymm2
	vmovups	(%r14,%rsi), %ymm3
	vmovups	%ymm0, -96(%r13,%rsi)
	vmovups	%ymm1, -64(%r13,%rsi)
	vmovups	%ymm2, -32(%r13,%rsi)
	vmovups	%ymm3, (%r13,%rsi)
	subq	$-128, %rsi
	incq	%rdx
	jne	.LBB13_52
.LBB13_53:
	cmpq	%r8, %r9
	je	.LBB13_43
# %bb.54:
	addq	%r8, %rax
.LBB13_41:
	decq	%rax
	.p2align	4, 0x90
.LBB13_42:                              # =>This Inner Loop Header: Depth=1
	movl	4(%r14,%rax,4), %edx
	movl	%edx, 4(%r13,%rax,4)
	incq	%rax
	cmpq	%rcx, %rax
	jl	.LBB13_42
.LBB13_43:
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.Lfunc_end13:
	.size	sort_merge_o, .Lfunc_end13-sort_merge_o
	.cfi_endproc
                                        # -- End function
	.globl	sort_merge_optimized    # -- Begin function sort_merge_optimized
	.p2align	4, 0x90
	.type	sort_merge_optimized,@function
sort_merge_optimized:                   # @sort_merge_optimized
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %r15d
	movl	%esi, %ebp
	movq	%rdi, %r14
	movl	%edx, %eax
	subl	%esi, %eax
	movslq	%eax, %rdi
	shlq	$2, %rdi
	callq	malloc
	movq	%rax, %rbx
	movq	%r14, %rdi
	movl	%ebp, %esi
	movl	%r15d, %edx
	movq	%rax, %rcx
	callq	sort_merge_o
	movq	%rbx, %rdi
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	free                    # TAILCALL
.Lfunc_end14:
	.size	sort_merge_optimized, .Lfunc_end14-sort_merge_optimized
	.cfi_endproc
                                        # -- End function
	.globl	merging_standard        # -- Begin function merging_standard
	.p2align	4, 0x90
	.type	merging_standard,@function
merging_standard:                       # @merging_standard
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
	leal	1(%rdx), %r11d
	cmpl	%edx, %esi
	jg	.LBB15_1
# %bb.8:
	cmpl	%ecx, %edx
	jge	.LBB15_1
# %bb.9:
	movslq	%esi, %rax
	leaq	b(,%rax,4), %r9
	movl	%esi, %r8d
	movl	%esi, %ebx
	.p2align	4, 0x90
.LBB15_10:                              # =>This Inner Loop Header: Depth=1
	movslq	%ebx, %rbx
	movl	(%rdi,%rbx,4), %ebp
	movslq	%r11d, %r11
	movl	(%rdi,%r11,4), %r10d
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	cmpl	%r10d, %ebp
	setg	%r14b
	setle	%al
	cmovgl	%r10d, %ebp
	addl	%eax, %ebx
	addl	%r14d, %r11d
	movl	%ebp, (%r9)
	incl	%r8d
	cmpl	%edx, %ebx
	jg	.LBB15_2
# %bb.11:                               #   in Loop: Header=BB15_10 Depth=1
	addq	$4, %r9
	cmpl	%ecx, %r11d
	jle	.LBB15_10
	jmp	.LBB15_2
.LBB15_1:
	movl	%esi, %ebx
	movl	%esi, %r8d
.LBB15_2:
	cmpl	%edx, %ebx
	jg	.LBB15_21
# %bb.3:
	movslq	%r8d, %r8
	movslq	%ebx, %r9
	movslq	%edx, %r14
	cmpq	%r14, %r9
	movq	%r14, %r10
	cmovgeq	%r9, %r10
	subq	%r9, %r10
	incq	%r10
	cmpq	$31, %r10
	jbe	.LBB15_19
# %bb.4:
	leaq	b(,%r8,4), %rdx
	cmpq	%r14, %r9
	movq	%r14, %rax
	cmovgeq	%r9, %rax
	leaq	(%rdi,%rax,4), %rbx
	addq	$4, %rbx
	cmpq	%rbx, %rdx
	jae	.LBB15_6
# %bb.5:
	addq	%r8, %rax
	subq	%r9, %rax
	leaq	b+4(,%rax,4), %rax
	leaq	(%rdi,%r9,4), %rdx
	cmpq	%rax, %rdx
	jb	.LBB15_19
.LBB15_6:
	movq	%r10, %rbp
	andq	$-32, %rbp
	leaq	-32(%rbp), %rdx
	movq	%rdx, %rax
	shrq	$5, %rax
	incq	%rax
	movl	%eax, %r15d
	andl	$3, %r15d
	cmpq	$96, %rdx
	jae	.LBB15_12
# %bb.7:
	xorl	%edx, %edx
	jmp	.LBB15_14
.LBB15_12:
	leaq	(%rdi,%r9,4), %r12
	addq	$480, %r12              # imm = 0x1E0
	leaq	b+480(,%r8,4), %rbx
	movq	%r15, %r13
	subq	%rax, %r13
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB15_13:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%r12,%rdx,4), %ymm0
	vmovups	-448(%r12,%rdx,4), %ymm1
	vmovups	-416(%r12,%rdx,4), %ymm2
	vmovups	-384(%r12,%rdx,4), %ymm3
	vmovups	%ymm0, -480(%rbx,%rdx,4)
	vmovups	%ymm1, -448(%rbx,%rdx,4)
	vmovups	%ymm2, -416(%rbx,%rdx,4)
	vmovups	%ymm3, -384(%rbx,%rdx,4)
	vmovups	-352(%r12,%rdx,4), %ymm0
	vmovups	-320(%r12,%rdx,4), %ymm1
	vmovups	-288(%r12,%rdx,4), %ymm2
	vmovups	-256(%r12,%rdx,4), %ymm3
	vmovups	%ymm0, -352(%rbx,%rdx,4)
	vmovups	%ymm1, -320(%rbx,%rdx,4)
	vmovups	%ymm2, -288(%rbx,%rdx,4)
	vmovups	%ymm3, -256(%rbx,%rdx,4)
	vmovups	-224(%r12,%rdx,4), %ymm0
	vmovups	-192(%r12,%rdx,4), %ymm1
	vmovups	-160(%r12,%rdx,4), %ymm2
	vmovups	-128(%r12,%rdx,4), %ymm3
	vmovups	%ymm0, -224(%rbx,%rdx,4)
	vmovups	%ymm1, -192(%rbx,%rdx,4)
	vmovups	%ymm2, -160(%rbx,%rdx,4)
	vmovups	%ymm3, -128(%rbx,%rdx,4)
	vmovups	-96(%r12,%rdx,4), %ymm0
	vmovups	-64(%r12,%rdx,4), %ymm1
	vmovups	-32(%r12,%rdx,4), %ymm2
	vmovups	(%r12,%rdx,4), %ymm3
	vmovups	%ymm0, -96(%rbx,%rdx,4)
	vmovups	%ymm1, -64(%rbx,%rdx,4)
	vmovups	%ymm2, -32(%rbx,%rdx,4)
	vmovups	%ymm3, (%rbx,%rdx,4)
	subq	$-128, %rdx
	addq	$4, %r13
	jne	.LBB15_13
.LBB15_14:
	testq	%r15, %r15
	je	.LBB15_17
# %bb.15:
	leaq	(%rdx,%r8), %rax
	addq	%r9, %rdx
	leaq	(%rdi,%rdx,4), %rdx
	negq	%r15
	movl	$96, %ebx
	.p2align	4, 0x90
.LBB15_16:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%rdx,%rbx), %ymm0
	vmovups	-64(%rdx,%rbx), %ymm1
	vmovups	-32(%rdx,%rbx), %ymm2
	vmovups	(%rdx,%rbx), %ymm3
	vmovups	%ymm0, b-96(%rbx,%rax,4)
	vmovups	%ymm1, b-64(%rbx,%rax,4)
	vmovups	%ymm2, b-32(%rbx,%rax,4)
	vmovups	%ymm3, b(%rbx,%rax,4)
	subq	$-128, %rbx
	incq	%r15
	jne	.LBB15_16
.LBB15_17:
	addq	%rbp, %r8
	cmpq	%rbp, %r10
	je	.LBB15_21
# %bb.18:
	addq	%rbp, %r9
.LBB15_19:
	decq	%r9
	.p2align	4, 0x90
.LBB15_20:                              # =>This Inner Loop Header: Depth=1
	movl	4(%rdi,%r9,4), %eax
	movl	%eax, b(,%r8,4)
	incq	%r8
	incq	%r9
	cmpq	%r14, %r9
	jl	.LBB15_20
.LBB15_21:
	cmpl	%ecx, %r11d
	jg	.LBB15_25
# %bb.22:
	movslq	%r8d, %r8
	movslq	%r11d, %rax
	movslq	%ecx, %r11
	cmpq	%r11, %rax
	movq	%r11, %r9
	cmovgeq	%rax, %r9
	subq	%rax, %r9
	incq	%r9
	cmpq	$31, %r9
	jbe	.LBB15_23
# %bb.30:
	leaq	b(,%r8,4), %rbp
	cmpq	%r11, %rax
	movq	%r11, %rdx
	cmovgeq	%rax, %rdx
	leaq	(%rdi,%rdx,4), %rbx
	addq	$4, %rbx
	cmpq	%rbx, %rbp
	jae	.LBB15_32
# %bb.31:
	addq	%r8, %rdx
	subq	%rax, %rdx
	leaq	b+4(,%rdx,4), %rdx
	leaq	(%rdi,%rax,4), %rbp
	cmpq	%rdx, %rbp
	jb	.LBB15_23
.LBB15_32:
	movq	%r9, %r10
	andq	$-32, %r10
	leaq	-32(%r10), %rbx
	movq	%rbx, %rdx
	shrq	$5, %rdx
	incq	%rdx
	movl	%edx, %r14d
	andl	$3, %r14d
	cmpq	$96, %rbx
	jae	.LBB15_34
# %bb.33:
	xorl	%edx, %edx
	jmp	.LBB15_36
.LBB15_34:
	leaq	(%rdi,%rax,4), %r15
	addq	$480, %r15              # imm = 0x1E0
	leaq	b+480(,%r8,4), %rbx
	movq	%r14, %rbp
	subq	%rdx, %rbp
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB15_35:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%r15,%rdx,4), %ymm0
	vmovups	-448(%r15,%rdx,4), %ymm1
	vmovups	-416(%r15,%rdx,4), %ymm2
	vmovups	-384(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -480(%rbx,%rdx,4)
	vmovups	%ymm1, -448(%rbx,%rdx,4)
	vmovups	%ymm2, -416(%rbx,%rdx,4)
	vmovups	%ymm3, -384(%rbx,%rdx,4)
	vmovups	-352(%r15,%rdx,4), %ymm0
	vmovups	-320(%r15,%rdx,4), %ymm1
	vmovups	-288(%r15,%rdx,4), %ymm2
	vmovups	-256(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -352(%rbx,%rdx,4)
	vmovups	%ymm1, -320(%rbx,%rdx,4)
	vmovups	%ymm2, -288(%rbx,%rdx,4)
	vmovups	%ymm3, -256(%rbx,%rdx,4)
	vmovups	-224(%r15,%rdx,4), %ymm0
	vmovups	-192(%r15,%rdx,4), %ymm1
	vmovups	-160(%r15,%rdx,4), %ymm2
	vmovups	-128(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -224(%rbx,%rdx,4)
	vmovups	%ymm1, -192(%rbx,%rdx,4)
	vmovups	%ymm2, -160(%rbx,%rdx,4)
	vmovups	%ymm3, -128(%rbx,%rdx,4)
	vmovups	-96(%r15,%rdx,4), %ymm0
	vmovups	-64(%r15,%rdx,4), %ymm1
	vmovups	-32(%r15,%rdx,4), %ymm2
	vmovups	(%r15,%rdx,4), %ymm3
	vmovups	%ymm0, -96(%rbx,%rdx,4)
	vmovups	%ymm1, -64(%rbx,%rdx,4)
	vmovups	%ymm2, -32(%rbx,%rdx,4)
	vmovups	%ymm3, (%rbx,%rdx,4)
	subq	$-128, %rdx
	addq	$4, %rbp
	jne	.LBB15_35
.LBB15_36:
	testq	%r14, %r14
	je	.LBB15_39
# %bb.37:
	leaq	(%rdx,%r8), %rbx
	addq	%rax, %rdx
	leaq	(%rdi,%rdx,4), %rdx
	negq	%r14
	movl	$96, %ebp
	.p2align	4, 0x90
.LBB15_38:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%rdx,%rbp), %ymm0
	vmovups	-64(%rdx,%rbp), %ymm1
	vmovups	-32(%rdx,%rbp), %ymm2
	vmovups	(%rdx,%rbp), %ymm3
	vmovups	%ymm0, b-96(%rbp,%rbx,4)
	vmovups	%ymm1, b-64(%rbp,%rbx,4)
	vmovups	%ymm2, b-32(%rbp,%rbx,4)
	vmovups	%ymm3, b(%rbp,%rbx,4)
	subq	$-128, %rbp
	incq	%r14
	jne	.LBB15_38
.LBB15_39:
	cmpq	%r10, %r9
	je	.LBB15_25
# %bb.40:
	addq	%r10, %rax
	addq	%r10, %r8
.LBB15_23:
	decq	%rax
	leaq	b(,%r8,4), %rdx
	.p2align	4, 0x90
.LBB15_24:                              # =>This Inner Loop Header: Depth=1
	movl	4(%rdi,%rax,4), %ebp
	movl	%ebp, (%rdx)
	incq	%rax
	addq	$4, %rdx
	cmpq	%r11, %rax
	jl	.LBB15_24
.LBB15_25:
	cmpl	%ecx, %esi
	jg	.LBB15_29
# %bb.26:
	movslq	%esi, %rax
	movslq	%ecx, %r10
	cmpq	%rax, %r10
	movq	%rax, %r9
	cmovgeq	%r10, %r9
	subq	%rax, %r9
	incq	%r9
	cmpq	$31, %r9
	jbe	.LBB15_27
# %bb.41:
	leaq	(%rdi,%rax,4), %rdx
	cmpq	%rax, %r10
	movq	%rax, %rcx
	cmovgeq	%r10, %rcx
	leaq	b+4(,%rcx,4), %rsi
	cmpq	%rsi, %rdx
	jae	.LBB15_43
# %bb.42:
	leaq	(%rdi,%rcx,4), %rcx
	addq	$4, %rcx
	leaq	b(,%rax,4), %rdx
	cmpq	%rcx, %rdx
	jb	.LBB15_27
.LBB15_43:
	movq	%r9, %r8
	andq	$-32, %r8
	leaq	-32(%r8), %rcx
	movq	%rcx, %rsi
	shrq	$5, %rsi
	incq	%rsi
	movl	%esi, %ebp
	andl	$3, %ebp
	cmpq	$96, %rcx
	jae	.LBB15_45
# %bb.44:
	xorl	%esi, %esi
	jmp	.LBB15_47
.LBB15_45:
	leaq	(%rdi,%rax,4), %rdx
	addq	$480, %rdx              # imm = 0x1E0
	leaq	b+480(,%rax,4), %rcx
	movq	%rbp, %rbx
	subq	%rsi, %rbx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB15_46:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rcx,%rsi,4), %ymm0
	vmovups	-448(%rcx,%rsi,4), %ymm1
	vmovups	-416(%rcx,%rsi,4), %ymm2
	vmovups	-384(%rcx,%rsi,4), %ymm3
	vmovups	%ymm0, -480(%rdx,%rsi,4)
	vmovups	%ymm1, -448(%rdx,%rsi,4)
	vmovups	%ymm2, -416(%rdx,%rsi,4)
	vmovups	%ymm3, -384(%rdx,%rsi,4)
	vmovups	-352(%rcx,%rsi,4), %ymm0
	vmovups	-320(%rcx,%rsi,4), %ymm1
	vmovups	-288(%rcx,%rsi,4), %ymm2
	vmovups	-256(%rcx,%rsi,4), %ymm3
	vmovups	%ymm0, -352(%rdx,%rsi,4)
	vmovups	%ymm1, -320(%rdx,%rsi,4)
	vmovups	%ymm2, -288(%rdx,%rsi,4)
	vmovups	%ymm3, -256(%rdx,%rsi,4)
	vmovups	-224(%rcx,%rsi,4), %ymm0
	vmovups	-192(%rcx,%rsi,4), %ymm1
	vmovups	-160(%rcx,%rsi,4), %ymm2
	vmovups	-128(%rcx,%rsi,4), %ymm3
	vmovups	%ymm0, -224(%rdx,%rsi,4)
	vmovups	%ymm1, -192(%rdx,%rsi,4)
	vmovups	%ymm2, -160(%rdx,%rsi,4)
	vmovups	%ymm3, -128(%rdx,%rsi,4)
	vmovups	-96(%rcx,%rsi,4), %ymm0
	vmovups	-64(%rcx,%rsi,4), %ymm1
	vmovups	-32(%rcx,%rsi,4), %ymm2
	vmovups	(%rcx,%rsi,4), %ymm3
	vmovups	%ymm0, -96(%rdx,%rsi,4)
	vmovups	%ymm1, -64(%rdx,%rsi,4)
	vmovups	%ymm2, -32(%rdx,%rsi,4)
	vmovups	%ymm3, (%rdx,%rsi,4)
	subq	$-128, %rsi
	addq	$4, %rbx
	jne	.LBB15_46
.LBB15_47:
	testq	%rbp, %rbp
	je	.LBB15_50
# %bb.48:
	addq	%rax, %rsi
	shlq	$2, %rsi
	negq	%rbp
	.p2align	4, 0x90
.LBB15_49:                              # =>This Inner Loop Header: Depth=1
	vmovups	b(%rsi), %ymm0
	vmovups	b+32(%rsi), %ymm1
	vmovups	b+64(%rsi), %ymm2
	vmovups	b+96(%rsi), %ymm3
	vmovups	%ymm0, (%rdi,%rsi)
	vmovups	%ymm1, 32(%rdi,%rsi)
	vmovups	%ymm2, 64(%rdi,%rsi)
	vmovups	%ymm3, 96(%rdi,%rsi)
	subq	$-128, %rsi
	incq	%rbp
	jne	.LBB15_49
.LBB15_50:
	cmpq	%r8, %r9
	je	.LBB15_29
# %bb.51:
	addq	%r8, %rax
.LBB15_27:
	decq	%rax
	.p2align	4, 0x90
.LBB15_28:                              # =>This Inner Loop Header: Depth=1
	movl	b+4(,%rax,4), %ecx
	movl	%ecx, 4(%rdi,%rax,4)
	incq	%rax
	cmpq	%r10, %rax
	jl	.LBB15_28
.LBB15_29:
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.Lfunc_end15:
	.size	merging_standard, .Lfunc_end15-merging_standard
	.cfi_endproc
                                        # -- End function
	.globl	sort_merge_standard     # -- Begin function sort_merge_standard
	.p2align	4, 0x90
	.type	sort_merge_standard,@function
sort_merge_standard:                    # @sort_merge_standard
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	pushq	%rax
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	%edx, %esi
	jge	.LBB16_43
# %bb.1:
	movl	%edx, %r14d
	movl	%esi, %ebp
	movq	%rdi, %r12
	leal	(%r14,%rbp), %eax
	movl	%eax, %r13d
	shrl	$31, %r13d
	addl	%eax, %r13d
	sarl	%r13d
	movl	%r13d, %edx
	callq	sort_merge_standard
	leal	1(%r13), %ebx
	movq	%r12, %rdi
	movl	%ebx, %esi
	movl	%r14d, %edx
	callq	sort_merge_standard
	cmpl	%ebp, %r13d
	jl	.LBB16_2
# %bb.11:
	cmpl	%r14d, %r13d
	jge	.LBB16_2
# %bb.12:
	movslq	%ebp, %rax
	leaq	b(,%rax,4), %rdx
	movl	%ebp, %r15d
	movq	%rbp, %r8
	movl	%ebp, %ecx
	.p2align	4, 0x90
.LBB16_13:                              # =>This Inner Loop Header: Depth=1
	movslq	%ecx, %rcx
	movl	(%r12,%rcx,4), %esi
	movslq	%ebx, %rbx
	movl	(%r12,%rbx,4), %edi
	xorl	%eax, %eax
	xorl	%ebp, %ebp
	cmpl	%edi, %esi
	setg	%al
	setle	%bpl
	cmovgl	%edi, %esi
	addl	%ebp, %ecx
	addl	%eax, %ebx
	movl	%esi, (%rdx)
	incl	%r15d
	cmpl	%r13d, %ecx
	jg	.LBB16_3
# %bb.14:                               #   in Loop: Header=BB16_13 Depth=1
	addq	$4, %rdx
	cmpl	%r14d, %ebx
	jle	.LBB16_13
.LBB16_3:
	cmpl	%r13d, %ecx
	jle	.LBB16_15
.LBB16_4:
	movq	%r8, %rbp
	jmp	.LBB16_5
.LBB16_2:
	movl	%ebp, %ecx
	movq	%rbp, %r8
	movl	%ebp, %r15d
	cmpl	%r13d, %ecx
	jg	.LBB16_4
.LBB16_15:
	movslq	%r15d, %r15
	movslq	%ecx, %rcx
	movslq	%r13d, %r10
	cmpq	%r10, %rcx
	movq	%r10, %r9
	cmovgeq	%rcx, %r9
	subq	%rcx, %r9
	incq	%r9
	cmpq	$31, %r9
	jbe	.LBB16_16
# %bb.19:
	leaq	b(,%r15,4), %rsi
	cmpq	%r10, %rcx
	movq	%r10, %rdx
	cmovgeq	%rcx, %rdx
	leaq	(%r12,%rdx,4), %rdi
	addq	$4, %rdi
	cmpq	%rdi, %rsi
	jae	.LBB16_21
# %bb.20:
	addq	%r15, %rdx
	subq	%rcx, %rdx
	leaq	b+4(,%rdx,4), %rdx
	leaq	(%r12,%rcx,4), %rsi
	cmpq	%rdx, %rsi
	jae	.LBB16_21
.LBB16_16:
	movq	%r8, %rbp
.LBB16_17:
	decq	%rcx
	.p2align	4, 0x90
.LBB16_18:                              # =>This Inner Loop Header: Depth=1
	movl	4(%r12,%rcx,4), %eax
	movl	%eax, b(,%r15,4)
	incq	%r15
	incq	%rcx
	cmpq	%r10, %rcx
	jl	.LBB16_18
.LBB16_5:
	cmpl	%r14d, %ebx
	jg	.LBB16_39
# %bb.6:
	movslq	%r15d, %r9
	movslq	%ebx, %rax
	movslq	%r14d, %r11
	cmpq	%r11, %rax
	movq	%r11, %r10
	cmovgeq	%rax, %r10
	subq	%rax, %r10
	incq	%r10
	cmpq	$31, %r10
	jbe	.LBB16_37
# %bb.7:
	leaq	b(,%r9,4), %rdx
	cmpq	%r11, %rax
	movq	%r11, %rcx
	cmovgeq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rsi
	addq	$4, %rsi
	cmpq	%rsi, %rdx
	jae	.LBB16_9
# %bb.8:
	addq	%r9, %rcx
	subq	%rax, %rcx
	leaq	b+4(,%rcx,4), %rcx
	leaq	(%r12,%rax,4), %rdx
	cmpq	%rcx, %rdx
	jb	.LBB16_37
.LBB16_9:
	movq	%r10, %r8
	andq	$-32, %r8
	leaq	-32(%r8), %rdx
	movq	%rdx, %rdi
	shrq	$5, %rdi
	incq	%rdi
	movl	%edi, %ecx
	andl	$3, %ecx
	cmpq	$96, %rdx
	jae	.LBB16_30
# %bb.10:
	xorl	%edi, %edi
	jmp	.LBB16_32
.LBB16_21:
	movq	%r9, %rax
	andq	$-32, %rax
	leaq	-32(%rax), %rdx
	movq	%rdx, %rsi
	shrq	$5, %rsi
	incq	%rsi
	movl	%esi, %r11d
	andl	$3, %r11d
	cmpq	$96, %rdx
	movq	%r8, %rbp
	jae	.LBB16_23
# %bb.22:
	xorl	%esi, %esi
	jmp	.LBB16_25
.LBB16_30:
	leaq	(%r12,%rax,4), %rdx
	addq	$480, %rdx              # imm = 0x1E0
	leaq	b+480(,%r9,4), %rsi
	movq	%rcx, %rbx
	subq	%rdi, %rbx
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB16_31:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdx,%rdi,4), %ymm0
	vmovups	-448(%rdx,%rdi,4), %ymm1
	vmovups	-416(%rdx,%rdi,4), %ymm2
	vmovups	-384(%rdx,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rdi,4)
	vmovups	%ymm1, -448(%rsi,%rdi,4)
	vmovups	%ymm2, -416(%rsi,%rdi,4)
	vmovups	%ymm3, -384(%rsi,%rdi,4)
	vmovups	-352(%rdx,%rdi,4), %ymm0
	vmovups	-320(%rdx,%rdi,4), %ymm1
	vmovups	-288(%rdx,%rdi,4), %ymm2
	vmovups	-256(%rdx,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rdi,4)
	vmovups	%ymm1, -320(%rsi,%rdi,4)
	vmovups	%ymm2, -288(%rsi,%rdi,4)
	vmovups	%ymm3, -256(%rsi,%rdi,4)
	vmovups	-224(%rdx,%rdi,4), %ymm0
	vmovups	-192(%rdx,%rdi,4), %ymm1
	vmovups	-160(%rdx,%rdi,4), %ymm2
	vmovups	-128(%rdx,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rdi,4)
	vmovups	%ymm1, -192(%rsi,%rdi,4)
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovups	-96(%rdx,%rdi,4), %ymm0
	vmovups	-64(%rdx,%rdi,4), %ymm1
	vmovups	-32(%rdx,%rdi,4), %ymm2
	vmovups	(%rdx,%rdi,4), %ymm3
	vmovups	%ymm0, -96(%rsi,%rdi,4)
	vmovups	%ymm1, -64(%rsi,%rdi,4)
	vmovups	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	subq	$-128, %rdi
	addq	$4, %rbx
	jne	.LBB16_31
.LBB16_32:
	testq	%rcx, %rcx
	je	.LBB16_35
# %bb.33:
	leaq	(%rdi,%r9), %rdx
	addq	%rax, %rdi
	leaq	(%r12,%rdi,4), %rsi
	negq	%rcx
	movl	$96, %edi
	.p2align	4, 0x90
.LBB16_34:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%rsi,%rdi), %ymm0
	vmovups	-64(%rsi,%rdi), %ymm1
	vmovups	-32(%rsi,%rdi), %ymm2
	vmovups	(%rsi,%rdi), %ymm3
	vmovups	%ymm0, b-96(%rdi,%rdx,4)
	vmovups	%ymm1, b-64(%rdi,%rdx,4)
	vmovups	%ymm2, b-32(%rdi,%rdx,4)
	vmovups	%ymm3, b(%rdi,%rdx,4)
	subq	$-128, %rdi
	incq	%rcx
	jne	.LBB16_34
.LBB16_35:
	cmpq	%r8, %r10
	je	.LBB16_39
# %bb.36:
	addq	%r8, %rax
	addq	%r8, %r9
.LBB16_37:
	decq	%rax
	leaq	b(,%r9,4), %rcx
	.p2align	4, 0x90
.LBB16_38:                              # =>This Inner Loop Header: Depth=1
	movl	4(%r12,%rax,4), %edx
	movl	%edx, (%rcx)
	incq	%rax
	addq	$4, %rcx
	cmpq	%r11, %rax
	jl	.LBB16_38
.LBB16_39:
	cmpl	%r14d, %ebp
	jg	.LBB16_43
# %bb.40:
	movslq	%ebp, %rax
	movslq	%r14d, %r10
	cmpq	%rax, %r10
	movq	%rax, %r9
	cmovgeq	%r10, %r9
	subq	%rax, %r9
	incq	%r9
	cmpq	$31, %r9
	jbe	.LBB16_41
# %bb.44:
	leaq	(%r12,%rax,4), %rdx
	cmpq	%rax, %r10
	movq	%rax, %rcx
	cmovgeq	%r10, %rcx
	leaq	b+4(,%rcx,4), %rsi
	cmpq	%rsi, %rdx
	jae	.LBB16_46
# %bb.45:
	leaq	(%r12,%rcx,4), %rcx
	addq	$4, %rcx
	leaq	b(,%rax,4), %rdx
	cmpq	%rcx, %rdx
	jb	.LBB16_41
.LBB16_46:
	movq	%r9, %r8
	andq	$-32, %r8
	leaq	-32(%r8), %rdx
	movq	%rdx, %rbx
	shrq	$5, %rbx
	incq	%rbx
	movl	%ebx, %ecx
	andl	$3, %ecx
	cmpq	$96, %rdx
	jae	.LBB16_48
# %bb.47:
	xorl	%ebx, %ebx
	jmp	.LBB16_50
.LBB16_23:
	leaq	(%r12,%rcx,4), %rdx
	addq	$480, %rdx              # imm = 0x1E0
	leaq	b+480(,%r15,4), %rdi
	movq	%r11, %r13
	subq	%rsi, %r13
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB16_24:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdx,%rsi,4), %ymm0
	vmovups	-448(%rdx,%rsi,4), %ymm1
	vmovups	-416(%rdx,%rsi,4), %ymm2
	vmovups	-384(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -480(%rdi,%rsi,4)
	vmovups	%ymm1, -448(%rdi,%rsi,4)
	vmovups	%ymm2, -416(%rdi,%rsi,4)
	vmovups	%ymm3, -384(%rdi,%rsi,4)
	vmovups	-352(%rdx,%rsi,4), %ymm0
	vmovups	-320(%rdx,%rsi,4), %ymm1
	vmovups	-288(%rdx,%rsi,4), %ymm2
	vmovups	-256(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -352(%rdi,%rsi,4)
	vmovups	%ymm1, -320(%rdi,%rsi,4)
	vmovups	%ymm2, -288(%rdi,%rsi,4)
	vmovups	%ymm3, -256(%rdi,%rsi,4)
	vmovups	-224(%rdx,%rsi,4), %ymm0
	vmovups	-192(%rdx,%rsi,4), %ymm1
	vmovups	-160(%rdx,%rsi,4), %ymm2
	vmovups	-128(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -224(%rdi,%rsi,4)
	vmovups	%ymm1, -192(%rdi,%rsi,4)
	vmovups	%ymm2, -160(%rdi,%rsi,4)
	vmovups	%ymm3, -128(%rdi,%rsi,4)
	vmovups	-96(%rdx,%rsi,4), %ymm0
	vmovups	-64(%rdx,%rsi,4), %ymm1
	vmovups	-32(%rdx,%rsi,4), %ymm2
	vmovups	(%rdx,%rsi,4), %ymm3
	vmovups	%ymm0, -96(%rdi,%rsi,4)
	vmovups	%ymm1, -64(%rdi,%rsi,4)
	vmovups	%ymm2, -32(%rdi,%rsi,4)
	vmovups	%ymm3, (%rdi,%rsi,4)
	subq	$-128, %rsi
	addq	$4, %r13
	jne	.LBB16_24
.LBB16_25:
	testq	%r11, %r11
	je	.LBB16_28
# %bb.26:
	leaq	(%rsi,%r15), %rdx
	addq	%rcx, %rsi
	leaq	(%r12,%rsi,4), %rsi
	negq	%r11
	movl	$96, %edi
	.p2align	4, 0x90
.LBB16_27:                              # =>This Inner Loop Header: Depth=1
	vmovups	-96(%rsi,%rdi), %ymm0
	vmovups	-64(%rsi,%rdi), %ymm1
	vmovups	-32(%rsi,%rdi), %ymm2
	vmovups	(%rsi,%rdi), %ymm3
	vmovups	%ymm0, b-96(%rdi,%rdx,4)
	vmovups	%ymm1, b-64(%rdi,%rdx,4)
	vmovups	%ymm2, b-32(%rdi,%rdx,4)
	vmovups	%ymm3, b(%rdi,%rdx,4)
	subq	$-128, %rdi
	incq	%r11
	jne	.LBB16_27
.LBB16_28:
	addq	%rax, %r15
	cmpq	%rax, %r9
	je	.LBB16_5
# %bb.29:
	addq	%rax, %rcx
	jmp	.LBB16_17
.LBB16_48:
	leaq	(%r12,%rax,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	leaq	b+480(,%rax,4), %rdx
	movq	%rcx, %rdi
	subq	%rbx, %rdi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB16_49:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdx,%rbx,4), %ymm0
	vmovups	-448(%rdx,%rbx,4), %ymm1
	vmovups	-416(%rdx,%rbx,4), %ymm2
	vmovups	-384(%rdx,%rbx,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rbx,4)
	vmovups	%ymm1, -448(%rsi,%rbx,4)
	vmovups	%ymm2, -416(%rsi,%rbx,4)
	vmovups	%ymm3, -384(%rsi,%rbx,4)
	vmovups	-352(%rdx,%rbx,4), %ymm0
	vmovups	-320(%rdx,%rbx,4), %ymm1
	vmovups	-288(%rdx,%rbx,4), %ymm2
	vmovups	-256(%rdx,%rbx,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rbx,4)
	vmovups	%ymm1, -320(%rsi,%rbx,4)
	vmovups	%ymm2, -288(%rsi,%rbx,4)
	vmovups	%ymm3, -256(%rsi,%rbx,4)
	vmovups	-224(%rdx,%rbx,4), %ymm0
	vmovups	-192(%rdx,%rbx,4), %ymm1
	vmovups	-160(%rdx,%rbx,4), %ymm2
	vmovups	-128(%rdx,%rbx,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rbx,4)
	vmovups	%ymm1, -192(%rsi,%rbx,4)
	vmovups	%ymm2, -160(%rsi,%rbx,4)
	vmovups	%ymm3, -128(%rsi,%rbx,4)
	vmovups	-96(%rdx,%rbx,4), %ymm0
	vmovups	-64(%rdx,%rbx,4), %ymm1
	vmovups	-32(%rdx,%rbx,4), %ymm2
	vmovups	(%rdx,%rbx,4), %ymm3
	vmovups	%ymm0, -96(%rsi,%rbx,4)
	vmovups	%ymm1, -64(%rsi,%rbx,4)
	vmovups	%ymm2, -32(%rsi,%rbx,4)
	vmovups	%ymm3, (%rsi,%rbx,4)
	subq	$-128, %rbx
	addq	$4, %rdi
	jne	.LBB16_49
.LBB16_50:
	testq	%rcx, %rcx
	je	.LBB16_53
# %bb.51:
	addq	%rax, %rbx
	shlq	$2, %rbx
	negq	%rcx
	.p2align	4, 0x90
.LBB16_52:                              # =>This Inner Loop Header: Depth=1
	vmovups	b(%rbx), %ymm0
	vmovups	b+32(%rbx), %ymm1
	vmovups	b+64(%rbx), %ymm2
	vmovups	b+96(%rbx), %ymm3
	vmovups	%ymm0, (%r12,%rbx)
	vmovups	%ymm1, 32(%r12,%rbx)
	vmovups	%ymm2, 64(%r12,%rbx)
	vmovups	%ymm3, 96(%r12,%rbx)
	subq	$-128, %rbx
	incq	%rcx
	jne	.LBB16_52
.LBB16_53:
	cmpq	%r8, %r9
	je	.LBB16_43
# %bb.54:
	addq	%r8, %rax
.LBB16_41:
	decq	%rax
	.p2align	4, 0x90
.LBB16_42:                              # =>This Inner Loop Header: Depth=1
	movl	b+4(,%rax,4), %ecx
	movl	%ecx, 4(%r12,%rax,4)
	incq	%rax
	cmpq	%r10, %rax
	jl	.LBB16_42
.LBB16_43:
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.Lfunc_end16:
	.size	sort_merge_standard, .Lfunc_end16-sort_merge_standard
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_optimized # -- Begin function partition_quick_optimized
	.p2align	4, 0x90
	.type	partition_quick_optimized,@function
partition_quick_optimized:              # @partition_quick_optimized
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %r10d
	subl	%esi, %r10d
	cmpl	$1, %r10d
	jne	.LBB17_2
# %bb.1:
	movslq	%esi, %r9
	movslq	%edx, %r8
	movl	(%rdi,%r9,4), %ebx
	movl	(%rdi,%r8,4), %eax
	cmpl	%eax, %ebx
	movl	%eax, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, (%rdi,%r9,4)
	cmovll	%eax, %ebx
	movl	%ebx, (%rdi,%r8,4)
	addl	%esi, %edx
	movl	%edx, %eax
	shrl	$31, %eax
	addl	%edx, %eax
	sarl	%eax
	jmp	.LBB17_9
.LBB17_2:
	leal	(%rdx,%rsi), %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	sarl	%eax
	movslq	%esi, %rcx
	movl	(%rdi,%rcx,4), %r11d
	movslq	%eax, %r14
	movl	(%rdi,%r14,4), %ebp
	movslq	%edx, %r8
	movl	(%rdi,%r8,4), %r15d
	cmpl	%ebp, %r11d
	movl	%ebp, %ebx
	cmovll	%r11d, %ebx
	cmovll	%ebp, %r11d
	cmpl	%r15d, %r11d
	movl	%r15d, %r9d
	cmovll	%r11d, %r9d
	cmovll	%r15d, %r11d
	cmpl	%r9d, %ebx
	movl	%r9d, %ebp
	cmovll	%ebx, %ebp
	cmovgel	%ebx, %r9d
	movl	%ebp, (%rdi,%rcx,4)
	cmpl	$2, %r10d
	jne	.LBB17_4
# %bb.3:
	movl	%r9d, (%rdi,%r14,4)
	movl	%r11d, (%rdi,%r8,4)
	jmp	.LBB17_9
.LBB17_4:
	movl	%r9d, (%rdi,%r8,4)
	movl	%r11d, (%rdi,%r14,4)
	subl	%esi, %edx
	jle	.LBB17_8
# %bb.5:
	movq	%rcx, %r10
	notq	%r10
	addq	%r8, %r10
	movq	%rcx, %rax
	andq	$3, %rdx
	je	.LBB17_7
	.p2align	4, 0x90
.LBB17_6:                               # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rax,4), %ebp
	xorl	%ebx, %ebx
	cmpl	%ebp, %r9d
	setg	%bl
	movl	(%rdi,%rcx,4), %r11d
	movl	%r11d, %esi
	cmovgl	%ebp, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovgl	%r11d, %ebp
	movl	%ebp, (%rdi,%rax,4)
	addq	%rbx, %rcx
	incq	%rax
	decq	%rdx
	jne	.LBB17_6
.LBB17_7:
	cmpq	$3, %r10
	jb	.LBB17_8
	.p2align	4, 0x90
.LBB17_10:                              # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rax,4), %edx
	xorl	%esi, %esi
	cmpl	%edx, %r9d
	setg	%sil
	movl	(%rdi,%rcx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%edx, %ebp
	movl	%ebp, (%rdi,%rcx,4)
	cmovgl	%ebx, %edx
	movl	%edx, (%rdi,%rax,4)
	addq	%rcx, %rsi
	movl	4(%rdi,%rax,4), %ecx
	xorl	%edx, %edx
	cmpl	%ecx, %r9d
	setg	%dl
	movl	(%rdi,%rsi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%ecx, %ebp
	movl	%ebp, (%rdi,%rsi,4)
	cmovgl	%ebx, %ecx
	movl	%ecx, 4(%rdi,%rax,4)
	addq	%rsi, %rdx
	movl	8(%rdi,%rax,4), %ecx
	xorl	%esi, %esi
	cmpl	%ecx, %r9d
	setg	%sil
	movl	(%rdi,%rdx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%ecx, %ebp
	movl	%ebp, (%rdi,%rdx,4)
	cmovgl	%ebx, %ecx
	movl	%ecx, 8(%rdi,%rax,4)
	addq	%rdx, %rsi
	movl	12(%rdi,%rax,4), %edx
	xorl	%ecx, %ecx
	cmpl	%edx, %r9d
	setg	%cl
	movl	(%rdi,%rsi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%edx, %ebp
	movl	%ebp, (%rdi,%rsi,4)
	cmovgl	%ebx, %edx
	movl	%edx, 12(%rdi,%rax,4)
	addq	%rsi, %rcx
	addq	$4, %rax
	cmpq	%rax, %r8
	jne	.LBB17_10
.LBB17_8:
	movl	(%rdi,%rcx,4), %eax
	movl	(%rdi,%r8,4), %edx
	movl	%edx, (%rdi,%rcx,4)
	movl	%eax, (%rdi,%r8,4)
	movl	%ecx, %eax
.LBB17_9:
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end17:
	.size	partition_quick_optimized, .Lfunc_end17-partition_quick_optimized
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_optimized    # -- Begin function sort_quick_optimized
	.p2align	4, 0x90
	.type	sort_quick_optimized,@function
sort_quick_optimized:                   # @sort_quick_optimized
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	pushq	%rax
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $esi killed $esi def $rsi
	cmpl	%esi, %edx
	jle	.LBB18_12
# %bb.1:
	movl	%edx, %ebx
	movq	%rdi, %r13
	movslq	%edx, %r15
	movq	%rbx, (%rsp)            # 8-byte Spill
	jmp	.LBB18_2
	.p2align	4, 0x90
.LBB18_4:                               #   in Loop: Header=BB18_2 Depth=1
	movslq	%esi, %rax
	movl	(%r13,%rax,4), %ecx
	movl	(%r13,%r15,4), %edx
	cmpl	%edx, %ecx
	movl	%edx, %edi
	cmovll	%ecx, %edi
	movl	%edi, (%r13,%rax,4)
	cmovll	%edx, %ecx
	movl	%ecx, (%r13,%r15,4)
	leal	(%rsi,%rbx), %eax
	movl	%eax, %r12d
	shrl	$31, %r12d
	addl	%eax, %r12d
	sarl	%r12d
.LBB18_11:                              #   in Loop: Header=BB18_2 Depth=1
	leal	-1(%r12), %edx
	movq	%r13, %rdi
                                        # kill: def $esi killed $esi killed $rsi
	callq	sort_quick_optimized
	incl	%r12d
	movl	%r12d, %esi
	cmpl	%ebx, %r12d
	jge	.LBB18_12
.LBB18_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB18_8 Depth 2
                                        #     Child Loop BB18_13 Depth 2
	movl	%ebx, %r14d
	subl	%esi, %r14d
	jle	.LBB18_12
# %bb.3:                                #   in Loop: Header=BB18_2 Depth=1
	cmpl	$1, %r14d
	je	.LBB18_4
# %bb.5:                                #   in Loop: Header=BB18_2 Depth=1
	leal	(%rsi,%rbx), %eax
	movl	%eax, %r12d
	shrl	$31, %r12d
	addl	%eax, %r12d
	sarl	%r12d
	movslq	%esi, %rcx
	movl	(%r13,%rcx,4), %edx
	movslq	%r12d, %r8
	movl	(%r13,%r8,4), %eax
	movl	(%r13,%r15,4), %r9d
	cmpl	%eax, %edx
	movl	%eax, %r10d
	cmovll	%edx, %r10d
	cmovll	%eax, %edx
	cmpl	%r9d, %edx
	movl	%r9d, %r11d
	cmovll	%edx, %r11d
	cmovll	%r9d, %edx
	cmpl	%r11d, %r10d
	movl	%r11d, %r9d
	cmovll	%r10d, %r9d
	cmovgel	%r10d, %r11d
	movl	%r9d, (%r13,%rcx,4)
	cmpl	$2, %r14d
	jne	.LBB18_7
# %bb.6:                                #   in Loop: Header=BB18_2 Depth=1
	movl	%r11d, (%r13,%r8,4)
	movl	%edx, (%r13,%r15,4)
	jmp	.LBB18_11
	.p2align	4, 0x90
.LBB18_7:                               #   in Loop: Header=BB18_2 Depth=1
	movl	%r11d, (%r13,%r15,4)
	movl	%edx, (%r13,%r8,4)
	movq	%rcx, %r8
	notq	%r8
	addq	%r15, %r8
	movq	%rcx, %rdx
	andq	$3, %r14
	je	.LBB18_9
	.p2align	4, 0x90
.LBB18_8:                               #   Parent Loop BB18_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r13,%rdx,4), %eax
	xorl	%ebx, %ebx
	cmpl	%eax, %r11d
	setg	%bl
	movl	(%r13,%rcx,4), %ebp
	movl	%ebp, %edi
	cmovgl	%eax, %edi
	movl	%edi, (%r13,%rcx,4)
	cmovgl	%ebp, %eax
	movl	%eax, (%r13,%rdx,4)
	addq	%rbx, %rcx
	incq	%rdx
	decq	%r14
	jne	.LBB18_8
.LBB18_9:                               #   in Loop: Header=BB18_2 Depth=1
	cmpq	$3, %r8
	jb	.LBB18_10
	.p2align	4, 0x90
.LBB18_13:                              #   Parent Loop BB18_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r13,%rdx,4), %eax
	xorl	%edi, %edi
	cmpl	%eax, %r11d
	setg	%dil
	movl	(%r13,%rcx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rcx,4)
	cmovgl	%ebx, %eax
	movl	%eax, (%r13,%rdx,4)
	addq	%rcx, %rdi
	movl	4(%r13,%rdx,4), %eax
	xorl	%ecx, %ecx
	cmpl	%eax, %r11d
	setg	%cl
	movl	(%r13,%rdi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rdi,4)
	cmovgl	%ebx, %eax
	movl	%eax, 4(%r13,%rdx,4)
	addq	%rdi, %rcx
	movl	8(%r13,%rdx,4), %eax
	xorl	%edi, %edi
	cmpl	%eax, %r11d
	setg	%dil
	movl	(%r13,%rcx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rcx,4)
	cmovgl	%ebx, %eax
	movl	%eax, 8(%r13,%rdx,4)
	addq	%rcx, %rdi
	movl	12(%r13,%rdx,4), %eax
	xorl	%ecx, %ecx
	cmpl	%eax, %r11d
	setg	%cl
	movl	(%r13,%rdi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rdi,4)
	cmovgl	%ebx, %eax
	movl	%eax, 12(%r13,%rdx,4)
	addq	%rdi, %rcx
	addq	$4, %rdx
	cmpq	%rdx, %r15
	jne	.LBB18_13
.LBB18_10:                              #   in Loop: Header=BB18_2 Depth=1
	movl	(%r13,%rcx,4), %eax
	movl	(%r13,%r15,4), %edx
	movl	%edx, (%r13,%rcx,4)
	movl	%eax, (%r13,%r15,4)
	movl	%ecx, %r12d
	movq	(%rsp), %rbx            # 8-byte Reload
	jmp	.LBB18_11
.LBB18_12:
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end18:
	.size	sort_quick_optimized, .Lfunc_end18-sort_quick_optimized
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_optimized_dual # -- Begin function partition_quick_optimized_dual
	.p2align	4, 0x90
	.type	partition_quick_optimized_dual,@function
partition_quick_optimized_dual:         # @partition_quick_optimized_dual
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r12
	.cfi_def_cfa_offset 40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movslq	%esi, %r10
	movl	(%rdi,%r10,4), %r14d
	movslq	%edx, %r9
	movl	(%rdi,%r9,4), %edx
	cmpl	%edx, %r14d
	movl	%edx, %r11d
	cmovll	%r14d, %r11d
	cmovll	%edx, %r14d
	movl	%r11d, (%rdi,%r10,4)
	movl	%r14d, (%rdi,%r9,4)
	leal	1(%r10), %edx
	movslq	%edx, %r15
	leal	-1(%r9), %edx
	movslq	%edx, %r12
	cmpl	%r10d, %r12d
	jle	.LBB19_3
# %bb.1:
	movq	%r15, %rbx
	.p2align	4, 0x90
.LBB19_2:                               # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rbx,4), %ebp
	xorl	%eax, %eax
	cmpl	%ebp, %r11d
	setg	%al
	movl	(%rdi,%r15,4), %edx
	movl	%edx, %esi
	cmovgl	%ebp, %esi
	movl	%esi, (%rdi,%r15,4)
	cmovgl	%edx, %ebp
	movl	%ebp, (%rdi,%rbx,4)
	addq	%rax, %r15
	xorl	%eax, %eax
	cmpl	%ebp, %r14d
	setl	%al
	movl	(%rdi,%r12,4), %edx
	movl	%edx, %esi
	cmovll	%ebp, %esi
	movl	%esi, (%rdi,%r12,4)
	cmovll	%edx, %ebp
	movl	%ebp, (%rdi,%rbx,4)
	subq	%rax, %r12
	subq	%rax, %rbx
	incq	%rbx
	cmpq	%r12, %rbx
	jle	.LBB19_2
.LBB19_3:
	movl	-4(%rdi,%r15,4), %eax
	movl	(%rdi,%r10,4), %edx
	movl	%edx, -4(%rdi,%r15,4)
	leaq	-1(%r15), %rdx
	movl	%eax, (%rdi,%r10,4)
	movl	4(%rdi,%r12,4), %eax
	movl	(%rdi,%r9,4), %esi
	movl	%esi, 4(%rdi,%r12,4)
	leaq	1(%r12), %rsi
	movl	%eax, (%rdi,%r9,4)
	movl	%edx, (%rcx)
	movl	%esi, (%r8)
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%r12
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end19:
	.size	partition_quick_optimized_dual, .Lfunc_end19-partition_quick_optimized_dual
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_optimized_dual # -- Begin function sort_quick_optimized_dual
	.p2align	4, 0x90
	.type	sort_quick_optimized_dual,@function
sort_quick_optimized_dual:              # @sort_quick_optimized_dual
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$24, %rsp
	.cfi_def_cfa_offset 80
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $esi killed $esi def $rsi
	cmpl	%esi, %edx
	jle	.LBB20_8
# %bb.1:
	movl	%edx, %r14d
	movq	%rdi, %r13
	movslq	%edx, %r12
	leal	-1(%r14), %eax
	movl	%eax, 12(%rsp)          # 4-byte Spill
	cltq
	movq	%rax, 16(%rsp)          # 8-byte Spill
	jmp	.LBB20_2
	.p2align	4, 0x90
.LBB20_4:                               #   in Loop: Header=BB20_2 Depth=1
	movq	16(%rsp), %rdi          # 8-byte Reload
.LBB20_7:                               #   in Loop: Header=BB20_2 Depth=1
	movl	-4(%r13,%r11,4), %eax
	movl	(%r13,%r8,4), %ecx
	movl	%ecx, -4(%r13,%r11,4)
	leaq	-1(%r11), %r15
	movl	%eax, (%r13,%r8,4)
	movl	4(%r13,%rdi,4), %eax
	movl	(%r13,%r12,4), %ecx
	movl	%ecx, 4(%r13,%rdi,4)
	leaq	1(%rdi), %rbx
	movl	%eax, (%r13,%r12,4)
	leal	-1(%r15), %edx
	movq	%r13, %rdi
                                        # kill: def $esi killed $esi killed $rsi
	callq	sort_quick_optimized_dual
	incl	%r15d
	leal	-1(%rbx), %edx
	movq	%r13, %rdi
	movl	%r15d, %esi
	callq	sort_quick_optimized_dual
	incl	%ebx
	movl	%ebx, %esi
	cmpl	%r14d, %ebx
	jge	.LBB20_8
.LBB20_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB20_6 Depth 2
	movl	%r14d, %eax
	subl	%esi, %eax
	cmpl	$1001, %eax             # imm = 0x3E9
	jl	.LBB20_9
# %bb.3:                                #   in Loop: Header=BB20_2 Depth=1
	movslq	%esi, %r8
	movl	(%r13,%r8,4), %r10d
	movl	(%r13,%r12,4), %eax
	cmpl	%eax, %r10d
	movl	%eax, %r9d
	cmovll	%r10d, %r9d
	cmovll	%eax, %r10d
	movl	%r9d, (%r13,%r8,4)
	movl	%r10d, (%r13,%r12,4)
	leal	1(%rsi), %eax
	movslq	%eax, %r11
	cmpl	%esi, 12(%rsp)          # 4-byte Folded Reload
	jle	.LBB20_4
# %bb.5:                                #   in Loop: Header=BB20_2 Depth=1
	movq	16(%rsp), %rdi          # 8-byte Reload
	movq	%r11, %rdx
	.p2align	4, 0x90
.LBB20_6:                               #   Parent Loop BB20_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r13,%rdx,4), %ebp
	xorl	%ecx, %ecx
	cmpl	%ebp, %r9d
	setg	%cl
	movl	(%r13,%r11,4), %eax
	movl	%eax, %ebx
	cmovgl	%ebp, %ebx
	movl	%ebx, (%r13,%r11,4)
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rdx,4)
	addq	%rcx, %r11
	xorl	%eax, %eax
	cmpl	%ebp, %r10d
	setl	%al
	movl	(%r13,%rdi,4), %ecx
	movl	%ecx, %ebx
	cmovll	%ebp, %ebx
	movl	%ebx, (%r13,%rdi,4)
	cmovll	%ecx, %ebp
	movl	%ebp, (%r13,%rdx,4)
	subq	%rax, %rdi
	subq	%rax, %rdx
	incq	%rdx
	cmpq	%rdi, %rdx
	jle	.LBB20_6
	jmp	.LBB20_7
.LBB20_8:
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB20_9:
	.cfi_def_cfa_offset 80
	movq	%r13, %rdi
                                        # kill: def $esi killed $esi killed $rsi
	movl	%r14d, %edx
	addq	$24, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	sort_quick_optimized    # TAILCALL
.Lfunc_end20:
	.size	sort_quick_optimized_dual, .Lfunc_end20-sort_quick_optimized_dual
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_multi   # -- Begin function partition_quick_multi
	.p2align	4, 0x90
	.type	partition_quick_multi,@function
partition_quick_multi:                  # @partition_quick_multi
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset %rbx, -16
	movl	%edx, %r9d
	movslq	%esi, %rax
	subl	%esi, %r9d
	jle	.LBB21_1
# %bb.7:
	vmovdqu	(%rdi,%rax,4), %ymm0
	movslq	%edx, %r10
	vmovd	%xmm0, %r11d
	leaq	1(%rax), %rsi
	.p2align	4, 0x90
.LBB21_8:                               # =>This Inner Loop Header: Depth=1
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm1, %ymm0, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vpshufd	$238, %xmm1, %xmm2      # xmm2 = xmm1[2,3,2,3]
	vpaddd	%xmm2, %xmm1, %xmm1
	vpshufd	$229, %xmm1, %xmm2      # xmm2 = xmm1[1,1,2,3]
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovd	%xmm1, %edx
	addl	$8, %edx
	movslq	(%rcx,%rdx,4), %rbx
	movl	%r11d, (%r8,%rbx,4)
	incl	(%rcx,%rdx,4)
	cmpq	%rsi, %r10
	je	.LBB21_1
# %bb.9:                                #   in Loop: Header=BB21_8 Depth=1
	movl	(%rdi,%rsi,4), %r11d
	incq	%rsi
	jmp	.LBB21_8
.LBB21_1:
	movslq	%r9d, %r9
	xorl	%r11d, %r11d
	jmp	.LBB21_2
	.p2align	4, 0x90
.LBB21_5:                               #   in Loop: Header=BB21_2 Depth=1
	subl	%r10d, %esi
	movl	%esi, (%rcx,%r11,4)
	incq	%r11
	cmpq	$9, %r11
	je	.LBB21_6
.LBB21_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB21_10 Depth 2
                                        #     Child Loop BB21_4 Depth 2
	movq	%r11, %r10
	imulq	%r9, %r10
	movslq	(%rcx,%r11,4), %rsi
	movq	%rsi, %rbx
	addq	$-7, %rbx
	movq	%r10, %rdx
	cmpq	%rbx, %r10
	jge	.LBB21_3
	.p2align	4, 0x90
.LBB21_10:                              #   Parent Loop BB21_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqu	(%r8,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%rdi,%rax,4)
	addq	$8, %rax
	addq	$8, %rdx
	movslq	(%rcx,%r11,4), %rsi
	leaq	-7(%rsi), %rbx
	cmpq	%rbx, %rdx
	jl	.LBB21_10
.LBB21_3:                               #   in Loop: Header=BB21_2 Depth=1
	movslq	%esi, %rbx
	cmpq	%rbx, %rdx
	jge	.LBB21_5
	.p2align	4, 0x90
.LBB21_4:                               #   Parent Loop BB21_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r8,%rdx,4), %esi
	movl	%esi, (%rdi,%rax,4)
	incq	%rax
	incq	%rdx
	movslq	(%rcx,%r11,4), %rsi
	cmpq	%rsi, %rdx
	jl	.LBB21_4
	jmp	.LBB21_5
.LBB21_6:
	popq	%rbx
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.Lfunc_end21:
	.size	partition_quick_multi, .Lfunc_end21-partition_quick_multi
	.cfi_endproc
                                        # -- End function
	.globl	hsum_8x32               # -- Begin function hsum_8x32
	.p2align	4, 0x90
	.type	hsum_8x32,@function
hsum_8x32:                              # @hsum_8x32
	.cfi_startproc
# %bb.0:
	vextracti128	$1, %ymm0, %xmm1
	vpaddd	%xmm0, %xmm1, %xmm0
	vpshufd	$238, %xmm0, %xmm1      # xmm1 = xmm0[2,3,2,3]
	vpaddd	%xmm1, %xmm0, %xmm0
	vpshufd	$229, %xmm0, %xmm1      # xmm1 = xmm0[1,1,2,3]
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%xmm0, %eax
	vzeroupper
	retq
.Lfunc_end22:
	.size	hsum_8x32, .Lfunc_end22-hsum_8x32
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4               # -- Begin function sort_quick_multi_h
.LCPI23_0:
	.long	2                       # 0x2
	.long	3                       # 0x3
	.long	4                       # 0x4
	.long	5                       # 0x5
	.text
	.globl	sort_quick_multi_h
	.p2align	4, 0x90
	.type	sort_quick_multi_h,@function
sort_quick_multi_h:                     # @sort_quick_multi_h
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$136, %rsp
	.cfi_def_cfa_offset 192
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %eax
	subl	%esi, %eax
	jle	.LBB23_271
# %bb.1:
	movl	%esi, %r12d
	movq	%rdi, %r14
	cmpl	$51, %eax
	jl	.LBB23_5
# %bb.2:
	movq	%rcx, %r13
	leal	1(%rax), %ecx
	movl	$0, 64(%rsp)
	movl	%ecx, 68(%rsp)
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %xmm0
	vpmulld	.LCPI23_0(%rip), %xmm0, %xmm0
	vmovdqu	%xmm0, 72(%rsp)
	leal	2(%rax,%rax), %esi
	leal	(%rsi,%rsi,2), %esi
	movl	%esi, 88(%rsp)
	leal	8(,%rax,8), %eax
	movl	%eax, %esi
	subl	%ecx, %esi
	movl	%esi, 92(%rsp)
	movl	%eax, 96(%rsp)
	incl	%edx
	movslq	%edx, %r15
	subl	%r12d, %edx
	movslq	%edx, %r8
	movslq	%r12d, %r11
	vmovdqu	(%r14,%r11,4), %ymm0
	vmovdqu	(%r14,%r11,4), %xmm1
	vmovd	%xmm1, %ecx
	leaq	1(%r11), %rax
	.p2align	4, 0x90
.LBB23_3:                               # =>This Inner Loop Header: Depth=1
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm1, %ymm0, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vpshufd	$238, %xmm1, %xmm2      # xmm2 = xmm1[2,3,2,3]
	vpaddd	%xmm2, %xmm1, %xmm1
	vpshufd	$229, %xmm1, %xmm2      # xmm2 = xmm1[1,1,2,3]
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovd	%xmm1, %esi
	addl	$8, %esi
	movslq	64(%rsp,%rsi,4), %rdi
	movl	%ecx, (%r13,%rdi,4)
	leal	1(%rdi), %ecx
	movl	%ecx, 64(%rsp,%rsi,4)
	cmpq	%rax, %r15
	je	.LBB23_6
# %bb.4:                                #   in Loop: Header=BB23_3 Depth=1
	movl	(%r14,%rax,4), %ecx
	incq	%rax
	jmp	.LBB23_3
.LBB23_5:
	movq	%r14, %rdi
	movl	%r12d, %esi
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	sort_quick_optimized    # TAILCALL
.LBB23_6:
	.cfi_def_cfa_offset 192
	movslq	64(%rsp), %rbp
	cmpq	$8, %rbp
	jl	.LBB23_10
# %bb.7:
	leaq	-7(%rbp), %rcx
	leaq	(%r14,%r11,4), %rsi
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB23_8:                               # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rax,4), %ymm0
	vmovdqu	%ymm0, (%rsi,%rax,4)
	addq	$8, %rax
	cmpq	%rcx, %rax
	jl	.LBB23_8
# %bb.9:
	leaq	(%r11,%rax), %rcx
	cmpq	%rbp, %rax
	movq	%rbp, 56(%rsp)          # 8-byte Spill
	movq	%r8, (%rsp)             # 8-byte Spill
	jl	.LBB23_11
	jmp	.LBB23_32
.LBB23_10:
	xorl	%eax, %eax
	movq	%r11, %rcx
	cmpq	%rbp, %rax
	movq	%rbp, 56(%rsp)          # 8-byte Spill
	movq	%r8, (%rsp)             # 8-byte Spill
	jge	.LBB23_32
.LBB23_11:
	movq	%rbp, %r8
	subq	%rax, %r8
	cmpq	$31, %r8
	jbe	.LBB23_24
# %bb.12:
	leaq	(%r14,%rcx,4), %rsi
	leaq	(,%rbp,4), %rdi
	addq	%r13, %rdi
	cmpq	%rdi, %rsi
	jae	.LBB23_14
# %bb.13:
	leaq	(%rcx,%rbp), %rsi
	subq	%rax, %rsi
	leaq	(%r14,%rsi,4), %rsi
	leaq	(,%rax,4), %rdi
	addq	%r13, %rdi
	cmpq	%rsi, %rdi
	jb	.LBB23_24
.LBB23_14:
	movq	%r12, %rbx
	movq	%r8, %r10
	andq	$-32, %r10
	leaq	-32(%r10), %rsi
	movq	%rsi, %r9
	shrq	$5, %r9
	incq	%r9
	movl	%r9d, %r12d
	andl	$3, %r12d
	cmpq	$96, %rsi
	jae	.LBB23_16
# %bb.15:
	xorl	%ebp, %ebp
	jmp	.LBB23_18
.LBB23_16:
	subq	%r12, %r9
	leaq	480(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB23_17:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdi,%rbp,4), %ymm0
	vmovups	-448(%rdi,%rbp,4), %ymm1
	vmovups	-416(%rdi,%rbp,4), %ymm2
	vmovups	-384(%rdi,%rbp,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rbp,4)
	vmovups	%ymm1, -448(%rsi,%rbp,4)
	vmovups	%ymm2, -416(%rsi,%rbp,4)
	vmovups	%ymm3, -384(%rsi,%rbp,4)
	vmovups	-352(%rdi,%rbp,4), %ymm0
	vmovups	-320(%rdi,%rbp,4), %ymm1
	vmovups	-288(%rdi,%rbp,4), %ymm2
	vmovups	-256(%rdi,%rbp,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rbp,4)
	vmovups	%ymm1, -320(%rsi,%rbp,4)
	vmovups	%ymm2, -288(%rsi,%rbp,4)
	vmovups	%ymm3, -256(%rsi,%rbp,4)
	vmovups	-224(%rdi,%rbp,4), %ymm0
	vmovups	-192(%rdi,%rbp,4), %ymm1
	vmovups	-160(%rdi,%rbp,4), %ymm2
	vmovups	-128(%rdi,%rbp,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rbp,4)
	vmovups	%ymm1, -192(%rsi,%rbp,4)
	vmovups	%ymm2, -160(%rsi,%rbp,4)
	vmovups	%ymm3, -128(%rsi,%rbp,4)
	vmovdqu	-96(%rdi,%rbp,4), %ymm0
	vmovdqu	-64(%rdi,%rbp,4), %ymm1
	vmovdqu	-32(%rdi,%rbp,4), %ymm2
	vmovups	(%rdi,%rbp,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rbp,4)
	vmovdqu	%ymm1, -64(%rsi,%rbp,4)
	vmovdqu	%ymm2, -32(%rsi,%rbp,4)
	vmovups	%ymm3, (%rsi,%rbp,4)
	subq	$-128, %rbp
	addq	$-4, %r9
	jne	.LBB23_17
.LBB23_18:
	testq	%r12, %r12
	je	.LBB23_21
# %bb.19:
	leaq	(%rcx,%rbp), %rsi
	leaq	(%r14,%rsi,4), %rsi
	addq	%rax, %rbp
	leaq	(,%rbp,4), %rdi
	addq	%r13, %rdi
	negq	%r12
	movl	$96, %ebp
	.p2align	4, 0x90
.LBB23_20:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%rdi,%rbp), %ymm0
	vmovdqu	-64(%rdi,%rbp), %ymm1
	vmovdqu	-32(%rdi,%rbp), %ymm2
	vmovups	(%rdi,%rbp), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rbp)
	vmovdqu	%ymm1, -64(%rsi,%rbp)
	vmovdqu	%ymm2, -32(%rsi,%rbp)
	vmovups	%ymm3, (%rsi,%rbp)
	subq	$-128, %rbp
	incq	%r12
	jne	.LBB23_20
.LBB23_21:
	addq	%r10, %rcx
	cmpq	%r10, %r8
	movq	%rbx, %r12
	jne	.LBB23_23
# %bb.22:
	movq	(%rsp), %r8             # 8-byte Reload
	jmp	.LBB23_32
.LBB23_23:
	addq	%r10, %rax
	movq	56(%rsp), %rbp          # 8-byte Reload
.LBB23_24:
	movl	%ebp, %edi
	subl	%eax, %edi
	movq	%rax, %r8
	notq	%r8
	addq	%rbp, %r8
	andq	$7, %rdi
	je	.LBB23_28
# %bb.25:
	leaq	(,%rax,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %r10
	negq	%rdi
	xorl	%ebp, %ebp
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_26:                              # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rbp), %esi
	movl	%esi, (%r10,%rbp)
	decq	%rbx
	addq	$4, %rbp
	cmpq	%rbx, %rdi
	jne	.LBB23_26
# %bb.27:
	subq	%rbx, %rax
	subq	%rbx, %rcx
	movq	56(%rsp), %rbp          # 8-byte Reload
.LBB23_28:
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_32
# %bb.29:
	movq	%rbp, %rsi
	subq	%rax, %rsi
	leaq	28(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(%r14,%rcx,4), %rbp
	addq	$28, %rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB23_30:                              # =>This Inner Loop Header: Depth=1
	movl	-28(%rdi,%rax,4), %ebx
	movl	%ebx, -28(%rbp,%rax,4)
	movl	-24(%rdi,%rax,4), %ebx
	movl	%ebx, -24(%rbp,%rax,4)
	movl	-20(%rdi,%rax,4), %ebx
	movl	%ebx, -20(%rbp,%rax,4)
	movl	-16(%rdi,%rax,4), %ebx
	movl	%ebx, -16(%rbp,%rax,4)
	movl	-12(%rdi,%rax,4), %ebx
	movl	%ebx, -12(%rbp,%rax,4)
	movl	-8(%rdi,%rax,4), %ebx
	movl	%ebx, -8(%rbp,%rax,4)
	movl	-4(%rdi,%rax,4), %ebx
	movl	%ebx, -4(%rbp,%rax,4)
	movl	(%rdi,%rax,4), %ebx
	movl	%ebx, (%rbp,%rax,4)
	addq	$8, %rax
	cmpq	%rax, %rsi
	jne	.LBB23_30
# %bb.31:
	addq	%rax, %rcx
.LBB23_32:
	movslq	68(%rsp), %r9
	leaq	-7(%r9), %rsi
	movq	%r8, %rax
	cmpl	%esi, %edx
	jge	.LBB23_34
	.p2align	4, 0x90
.LBB23_33:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rax,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rax
	cmpq	%rsi, %rax
	jl	.LBB23_33
.LBB23_34:
	cmpq	%r9, %rax
	movq	%r11, 16(%rsp)          # 8-byte Spill
	jge	.LBB23_56
# %bb.35:
	movq	%r9, %r8
	subq	%rax, %r8
	cmpq	$32, %r8
	jb	.LBB23_48
# %bb.36:
	leaq	(%r14,%rcx,4), %rsi
	leaq	(,%r9,4), %rdi
	addq	%r13, %rdi
	cmpq	%rdi, %rsi
	jae	.LBB23_38
# %bb.37:
	leaq	(%rcx,%r9), %rsi
	subq	%rax, %rsi
	leaq	(%r14,%rsi,4), %rsi
	leaq	(,%rax,4), %rdi
	addq	%r13, %rdi
	cmpq	%rsi, %rdi
	jb	.LBB23_48
.LBB23_38:
	movq	%r9, %r11
	movq	%r12, %rbx
	movq	%r8, %r10
	andq	$-32, %r10
	leaq	-32(%r10), %rsi
	movq	%rsi, %r9
	shrq	$5, %r9
	incq	%r9
	movl	%r9d, %r12d
	andl	$3, %r12d
	cmpq	$96, %rsi
	jae	.LBB23_40
# %bb.39:
	xorl	%ebp, %ebp
	jmp	.LBB23_42
.LBB23_40:
	subq	%r12, %r9
	leaq	480(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB23_41:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdi,%rbp,4), %ymm0
	vmovups	-448(%rdi,%rbp,4), %ymm1
	vmovups	-416(%rdi,%rbp,4), %ymm2
	vmovups	-384(%rdi,%rbp,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rbp,4)
	vmovups	%ymm1, -448(%rsi,%rbp,4)
	vmovups	%ymm2, -416(%rsi,%rbp,4)
	vmovups	%ymm3, -384(%rsi,%rbp,4)
	vmovups	-352(%rdi,%rbp,4), %ymm0
	vmovups	-320(%rdi,%rbp,4), %ymm1
	vmovups	-288(%rdi,%rbp,4), %ymm2
	vmovups	-256(%rdi,%rbp,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rbp,4)
	vmovups	%ymm1, -320(%rsi,%rbp,4)
	vmovups	%ymm2, -288(%rsi,%rbp,4)
	vmovups	%ymm3, -256(%rsi,%rbp,4)
	vmovups	-224(%rdi,%rbp,4), %ymm0
	vmovups	-192(%rdi,%rbp,4), %ymm1
	vmovups	-160(%rdi,%rbp,4), %ymm2
	vmovups	-128(%rdi,%rbp,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rbp,4)
	vmovups	%ymm1, -192(%rsi,%rbp,4)
	vmovups	%ymm2, -160(%rsi,%rbp,4)
	vmovups	%ymm3, -128(%rsi,%rbp,4)
	vmovdqu	-96(%rdi,%rbp,4), %ymm0
	vmovdqu	-64(%rdi,%rbp,4), %ymm1
	vmovdqu	-32(%rdi,%rbp,4), %ymm2
	vmovups	(%rdi,%rbp,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rbp,4)
	vmovdqu	%ymm1, -64(%rsi,%rbp,4)
	vmovdqu	%ymm2, -32(%rsi,%rbp,4)
	vmovups	%ymm3, (%rsi,%rbp,4)
	subq	$-128, %rbp
	addq	$-4, %r9
	jne	.LBB23_41
.LBB23_42:
	testq	%r12, %r12
	je	.LBB23_45
# %bb.43:
	leaq	(%rcx,%rbp), %rsi
	leaq	(%r14,%rsi,4), %rsi
	addq	%rax, %rbp
	leaq	(,%rbp,4), %rdi
	addq	%r13, %rdi
	negq	%r12
	movl	$96, %ebp
	.p2align	4, 0x90
.LBB23_44:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%rdi,%rbp), %ymm0
	vmovdqu	-64(%rdi,%rbp), %ymm1
	vmovdqu	-32(%rdi,%rbp), %ymm2
	vmovups	(%rdi,%rbp), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rbp)
	vmovdqu	%ymm1, -64(%rsi,%rbp)
	vmovdqu	%ymm2, -32(%rsi,%rbp)
	vmovups	%ymm3, (%rsi,%rbp)
	subq	$-128, %rbp
	incq	%r12
	jne	.LBB23_44
.LBB23_45:
	addq	%r10, %rcx
	cmpq	%r10, %r8
	movq	%rbx, %r12
	jne	.LBB23_47
# %bb.46:
	movq	%r11, %r9
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	(%rsp), %r8             # 8-byte Reload
	jmp	.LBB23_56
.LBB23_47:
	addq	%r10, %rax
	movq	%r11, %r9
.LBB23_48:
	movl	%r9d, %edi
	subl	%eax, %edi
	movq	%rax, %r8
	notq	%r8
	addq	%r9, %r8
	andq	$7, %rdi
	je	.LBB23_52
# %bb.49:
	movq	%r9, %r11
	leaq	(,%rax,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %r10
	negq	%rdi
	xorl	%ebp, %ebp
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_50:                              # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rbp), %esi
	movl	%esi, (%r10,%rbp)
	decq	%rbx
	addq	$4, %rbp
	cmpq	%rbx, %rdi
	jne	.LBB23_50
# %bb.51:
	subq	%rbx, %rax
	subq	%rbx, %rcx
	movq	%r11, %r9
.LBB23_52:
	movq	16(%rsp), %r11          # 8-byte Reload
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_56
# %bb.53:
	movq	%r9, %rsi
	subq	%rax, %rsi
	leaq	28(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(%r14,%rcx,4), %rbp
	addq	$28, %rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB23_54:                              # =>This Inner Loop Header: Depth=1
	movl	-28(%rdi,%rax,4), %ebx
	movl	%ebx, -28(%rbp,%rax,4)
	movl	-24(%rdi,%rax,4), %ebx
	movl	%ebx, -24(%rbp,%rax,4)
	movl	-20(%rdi,%rax,4), %ebx
	movl	%ebx, -20(%rbp,%rax,4)
	movl	-16(%rdi,%rax,4), %ebx
	movl	%ebx, -16(%rbp,%rax,4)
	movl	-12(%rdi,%rax,4), %ebx
	movl	%ebx, -12(%rbp,%rax,4)
	movl	-8(%rdi,%rax,4), %ebx
	movl	%ebx, -8(%rbp,%rax,4)
	movl	-4(%rdi,%rax,4), %ebx
	movl	%ebx, -4(%rbp,%rax,4)
	movl	(%rdi,%rax,4), %ebx
	movl	%ebx, (%rbp,%rax,4)
	addq	$8, %rax
	cmpq	%rax, %rsi
	jne	.LBB23_54
# %bb.55:
	addq	%rax, %rcx
.LBB23_56:
	subl	%edx, %r9d
	movq	%r9, 128(%rsp)          # 8-byte Spill
	movl	%r9d, 68(%rsp)
	leaq	(%r8,%r8), %r10
	movslq	72(%rsp), %rax
	movq	%rax, 48(%rsp)          # 8-byte Spill
	addq	$-7, %rax
	cmpq	%rax, %r10
	jge	.LBB23_61
# %bb.57:
	leaq	(%rax,%r11,2), %rdx
	leaq	(%r15,%r15), %rdi
	notq	%rdi
	addq	%rdx, %rdi
	movl	%edi, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_62
# %bb.58:
	negq	%rsi
	movq	%r10, %rdx
	.p2align	4, 0x90
.LBB23_59:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rdx
	incq	%rsi
	jne	.LBB23_59
# %bb.60:
	cmpq	$56, %rdi
	jae	.LBB23_63
	jmp	.LBB23_64
.LBB23_61:
	movq	%r10, %rdx
	jmp	.LBB23_64
.LBB23_62:
	movq	%r10, %rdx
	cmpq	$56, %rdi
	jb	.LBB23_64
	.p2align	4, 0x90
.LBB23_63:                              # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rdx
	cmpq	%rax, %rdx
	jl	.LBB23_63
.LBB23_64:
	movq	48(%rsp), %rax          # 8-byte Reload
	cmpq	%rax, %rdx
	movq	%r12, 8(%rsp)           # 8-byte Spill
	jge	.LBB23_87
# %bb.65:
	movq	%rax, %rbx
	subq	%rdx, %rbx
	cmpq	$32, %rbx
	jb	.LBB23_79
# %bb.66:
	leaq	(%r14,%rcx,4), %r8
	leaq	(,%rdx,4), %rdi
	addq	%r13, %rdi
	movq	%rax, %rsi
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	cmpq	%rax, %r8
	jae	.LBB23_69
# %bb.67:
	leaq	(%rcx,%rsi), %rax
	subq	%rdx, %rax
	leaq	(%r14,%rax,4), %rax
	cmpq	%rax, %rdi
	jae	.LBB23_69
# %bb.68:
	movq	%rsi, %rax
	jmp	.LBB23_79
.LBB23_69:
	movq	%rbx, %r12
	andq	$-32, %r12
	leaq	-32(%r12), %rsi
	movq	%rsi, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %eax
	andl	$3, %eax
	cmpq	$96, %rsi
	jae	.LBB23_71
# %bb.70:
	xorl	%r9d, %r9d
	jmp	.LBB23_73
.LBB23_71:
	subq	%rax, %rbp
	leaq	480(,%rdx,4), %r11
	addq	%r13, %r11
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB23_72:                              # =>This Inner Loop Header: Depth=1
	vmovups	-480(%r11,%r9,4), %ymm0
	vmovups	-448(%r11,%r9,4), %ymm1
	vmovups	-416(%r11,%r9,4), %ymm2
	vmovups	-384(%r11,%r9,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%r9,4)
	vmovups	%ymm1, -448(%rsi,%r9,4)
	vmovups	%ymm2, -416(%rsi,%r9,4)
	vmovups	%ymm3, -384(%rsi,%r9,4)
	vmovups	-352(%r11,%r9,4), %ymm0
	vmovups	-320(%r11,%r9,4), %ymm1
	vmovups	-288(%r11,%r9,4), %ymm2
	vmovups	-256(%r11,%r9,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%r9,4)
	vmovups	%ymm1, -320(%rsi,%r9,4)
	vmovups	%ymm2, -288(%rsi,%r9,4)
	vmovups	%ymm3, -256(%rsi,%r9,4)
	vmovups	-224(%r11,%r9,4), %ymm0
	vmovups	-192(%r11,%r9,4), %ymm1
	vmovups	-160(%r11,%r9,4), %ymm2
	vmovups	-128(%r11,%r9,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%r9,4)
	vmovups	%ymm1, -192(%rsi,%r9,4)
	vmovups	%ymm2, -160(%rsi,%r9,4)
	vmovups	%ymm3, -128(%rsi,%r9,4)
	vmovdqu	-96(%r11,%r9,4), %ymm0
	vmovdqu	-64(%r11,%r9,4), %ymm1
	vmovdqu	-32(%r11,%r9,4), %ymm2
	vmovups	(%r11,%r9,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%r9,4)
	vmovdqu	%ymm1, -64(%rsi,%r9,4)
	vmovdqu	%ymm2, -32(%rsi,%r9,4)
	vmovups	%ymm3, (%rsi,%r9,4)
	subq	$-128, %r9
	addq	$-4, %rbp
	jne	.LBB23_72
.LBB23_73:
	testq	%rax, %rax
	je	.LBB23_76
# %bb.74:
	leaq	96(,%r9,4), %rsi
	negq	%rax
	.p2align	4, 0x90
.LBB23_75:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%rdi,%rsi), %ymm0
	vmovdqu	-64(%rdi,%rsi), %ymm1
	vmovdqu	-32(%rdi,%rsi), %ymm2
	vmovups	(%rdi,%rsi), %ymm3
	vmovdqu	%ymm0, -96(%r8,%rsi)
	vmovdqu	%ymm1, -64(%r8,%rsi)
	vmovdqu	%ymm2, -32(%r8,%rsi)
	vmovups	%ymm3, (%r8,%rsi)
	subq	$-128, %rsi
	incq	%rax
	jne	.LBB23_75
.LBB23_76:
	addq	%r12, %rcx
	cmpq	%r12, %rbx
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	48(%rsp), %rax          # 8-byte Reload
	jne	.LBB23_78
# %bb.77:
	movq	8(%rsp), %r12           # 8-byte Reload
	movq	(%rsp), %r8             # 8-byte Reload
	jmp	.LBB23_87
.LBB23_78:
	addq	%r12, %rdx
	movq	8(%rsp), %r12           # 8-byte Reload
.LBB23_79:
	movl	%eax, %esi
	subl	%edx, %esi
	movq	%rdx, %r8
	notq	%r8
	addq	%rax, %r8
	andq	$7, %rsi
	je	.LBB23_83
# %bb.80:
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%eax, %eax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_81:                              # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rax), %edi
	movl	%edi, (%rbp,%rax)
	decq	%rbx
	addq	$4, %rax
	cmpq	%rbx, %rsi
	jne	.LBB23_81
# %bb.82:
	subq	%rbx, %rdx
	subq	%rbx, %rcx
	movq	48(%rsp), %rax          # 8-byte Reload
.LBB23_83:
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_87
# %bb.84:
	subq	%rdx, %rax
	leaq	28(,%rdx,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$28, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_85:                              # =>This Inner Loop Header: Depth=1
	movl	-28(%rsi,%rdx,4), %ebp
	movl	%ebp, -28(%rdi,%rdx,4)
	movl	-24(%rsi,%rdx,4), %ebp
	movl	%ebp, -24(%rdi,%rdx,4)
	movl	-20(%rsi,%rdx,4), %ebp
	movl	%ebp, -20(%rdi,%rdx,4)
	movl	-16(%rsi,%rdx,4), %ebp
	movl	%ebp, -16(%rdi,%rdx,4)
	movl	-12(%rsi,%rdx,4), %ebp
	movl	%ebp, -12(%rdi,%rdx,4)
	movl	-8(%rsi,%rdx,4), %ebp
	movl	%ebp, -8(%rdi,%rdx,4)
	movl	-4(%rsi,%rdx,4), %ebp
	movl	%ebp, -4(%rdi,%rdx,4)
	movl	(%rsi,%rdx,4), %ebp
	movl	%ebp, (%rdi,%rdx,4)
	addq	$8, %rdx
	cmpq	%rdx, %rax
	jne	.LBB23_85
# %bb.86:
	addq	%rdx, %rcx
	movq	48(%rsp), %rax          # 8-byte Reload
.LBB23_87:
	subl	%r10d, %eax
	movq	%rax, 48(%rsp)          # 8-byte Spill
	movl	%eax, 72(%rsp)
	leaq	(%r8,%r8,2), %r10
	movslq	76(%rsp), %rax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	addq	$-7, %rax
	cmpq	%rax, %r10
	jge	.LBB23_92
# %bb.88:
	leaq	(%r11,%r11,2), %rdx
	addq	%rax, %rdx
	leaq	(%r15,%r15,2), %rdi
	notq	%rdi
	addq	%rdx, %rdi
	movl	%edi, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_93
# %bb.89:
	negq	%rsi
	movq	%r10, %rdx
	.p2align	4, 0x90
.LBB23_90:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rdx
	incq	%rsi
	jne	.LBB23_90
# %bb.91:
	cmpq	$56, %rdi
	jae	.LBB23_94
	jmp	.LBB23_95
.LBB23_92:
	movq	%r10, %rdx
	jmp	.LBB23_95
.LBB23_93:
	movq	%r10, %rdx
	cmpq	$56, %rdi
	jb	.LBB23_95
	.p2align	4, 0x90
.LBB23_94:                              # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rdx
	cmpq	%rax, %rdx
	jl	.LBB23_94
.LBB23_95:
	movq	40(%rsp), %rax          # 8-byte Reload
	cmpq	%rax, %rdx
	jge	.LBB23_118
# %bb.96:
	movq	%rax, %rbx
	subq	%rdx, %rbx
	cmpq	$32, %rbx
	jb	.LBB23_110
# %bb.97:
	leaq	(%r14,%rcx,4), %r8
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	movq	%rax, %rsi
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	cmpq	%rax, %r8
	jae	.LBB23_100
# %bb.98:
	leaq	(%rcx,%rsi), %rax
	subq	%rdx, %rax
	leaq	(%r14,%rax,4), %rax
	cmpq	%rax, %r9
	jae	.LBB23_100
# %bb.99:
	movq	%rsi, %rax
	jmp	.LBB23_110
.LBB23_100:
	movq	%rbx, %r12
	andq	$-32, %r12
	leaq	-32(%r12), %rax
	movq	%rax, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %r11d
	andl	$3, %r11d
	cmpq	$96, %rax
	jae	.LBB23_102
# %bb.101:
	xorl	%edi, %edi
	jmp	.LBB23_104
.LBB23_102:
	subq	%r11, %rbp
	leaq	480(,%rdx,4), %rax
	addq	%r13, %rax
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB23_103:                             # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rax,%rdi,4), %ymm0
	vmovups	-448(%rax,%rdi,4), %ymm1
	vmovups	-416(%rax,%rdi,4), %ymm2
	vmovups	-384(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rdi,4)
	vmovups	%ymm1, -448(%rsi,%rdi,4)
	vmovups	%ymm2, -416(%rsi,%rdi,4)
	vmovups	%ymm3, -384(%rsi,%rdi,4)
	vmovups	-352(%rax,%rdi,4), %ymm0
	vmovups	-320(%rax,%rdi,4), %ymm1
	vmovups	-288(%rax,%rdi,4), %ymm2
	vmovups	-256(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rdi,4)
	vmovups	%ymm1, -320(%rsi,%rdi,4)
	vmovups	%ymm2, -288(%rsi,%rdi,4)
	vmovups	%ymm3, -256(%rsi,%rdi,4)
	vmovups	-224(%rax,%rdi,4), %ymm0
	vmovups	-192(%rax,%rdi,4), %ymm1
	vmovups	-160(%rax,%rdi,4), %ymm2
	vmovups	-128(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rdi,4)
	vmovups	%ymm1, -192(%rsi,%rdi,4)
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovdqu	-96(%rax,%rdi,4), %ymm0
	vmovdqu	-64(%rax,%rdi,4), %ymm1
	vmovdqu	-32(%rax,%rdi,4), %ymm2
	vmovups	(%rax,%rdi,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rdi,4)
	vmovdqu	%ymm1, -64(%rsi,%rdi,4)
	vmovdqu	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	subq	$-128, %rdi
	addq	$-4, %rbp
	jne	.LBB23_103
.LBB23_104:
	testq	%r11, %r11
	je	.LBB23_107
# %bb.105:
	leaq	96(,%rdi,4), %rax
	negq	%r11
	.p2align	4, 0x90
.LBB23_106:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%r9,%rax), %ymm0
	vmovdqu	-64(%r9,%rax), %ymm1
	vmovdqu	-32(%r9,%rax), %ymm2
	vmovups	(%r9,%rax), %ymm3
	vmovdqu	%ymm0, -96(%r8,%rax)
	vmovdqu	%ymm1, -64(%r8,%rax)
	vmovdqu	%ymm2, -32(%r8,%rax)
	vmovups	%ymm3, (%r8,%rax)
	subq	$-128, %rax
	incq	%r11
	jne	.LBB23_106
.LBB23_107:
	addq	%r12, %rcx
	cmpq	%r12, %rbx
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	40(%rsp), %rax          # 8-byte Reload
	jne	.LBB23_109
# %bb.108:
	movq	8(%rsp), %r12           # 8-byte Reload
	movq	(%rsp), %r8             # 8-byte Reload
	jmp	.LBB23_118
.LBB23_109:
	addq	%r12, %rdx
	movq	8(%rsp), %r12           # 8-byte Reload
.LBB23_110:
	movl	%eax, %esi
	subl	%edx, %esi
	movq	%rdx, %r8
	notq	%r8
	addq	%rax, %r8
	andq	$7, %rsi
	je	.LBB23_114
# %bb.111:
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%eax, %eax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_112:                             # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rax), %edi
	movl	%edi, (%rbp,%rax)
	decq	%rbx
	addq	$4, %rax
	cmpq	%rbx, %rsi
	jne	.LBB23_112
# %bb.113:
	subq	%rbx, %rdx
	subq	%rbx, %rcx
	movq	40(%rsp), %rax          # 8-byte Reload
.LBB23_114:
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_118
# %bb.115:
	subq	%rdx, %rax
	leaq	28(,%rdx,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$28, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_116:                             # =>This Inner Loop Header: Depth=1
	movl	-28(%rsi,%rdx,4), %ebp
	movl	%ebp, -28(%rdi,%rdx,4)
	movl	-24(%rsi,%rdx,4), %ebp
	movl	%ebp, -24(%rdi,%rdx,4)
	movl	-20(%rsi,%rdx,4), %ebp
	movl	%ebp, -20(%rdi,%rdx,4)
	movl	-16(%rsi,%rdx,4), %ebp
	movl	%ebp, -16(%rdi,%rdx,4)
	movl	-12(%rsi,%rdx,4), %ebp
	movl	%ebp, -12(%rdi,%rdx,4)
	movl	-8(%rsi,%rdx,4), %ebp
	movl	%ebp, -8(%rdi,%rdx,4)
	movl	-4(%rsi,%rdx,4), %ebp
	movl	%ebp, -4(%rdi,%rdx,4)
	movl	(%rsi,%rdx,4), %ebp
	movl	%ebp, (%rdi,%rdx,4)
	addq	$8, %rdx
	cmpq	%rdx, %rax
	jne	.LBB23_116
# %bb.117:
	addq	%rdx, %rcx
	movq	40(%rsp), %rax          # 8-byte Reload
.LBB23_118:
	subl	%r10d, %eax
	movq	%rax, 40(%rsp)          # 8-byte Spill
	movl	%eax, 76(%rsp)
	leaq	(,%r8,4), %r10
	movslq	80(%rsp), %rax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	addq	$-7, %rax
	cmpq	%rax, %r10
	jge	.LBB23_123
# %bb.119:
	leaq	(%rax,%r11,4), %rdx
	leaq	(,%r15,4), %rdi
	notq	%rdi
	addq	%rdx, %rdi
	movl	%edi, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_124
# %bb.120:
	negq	%rsi
	movq	%r10, %rdx
	.p2align	4, 0x90
.LBB23_121:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rdx
	incq	%rsi
	jne	.LBB23_121
# %bb.122:
	cmpq	$56, %rdi
	jae	.LBB23_125
	jmp	.LBB23_126
.LBB23_123:
	movq	%r10, %rdx
	jmp	.LBB23_126
.LBB23_124:
	movq	%r10, %rdx
	cmpq	$56, %rdi
	jb	.LBB23_126
	.p2align	4, 0x90
.LBB23_125:                             # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rdx
	cmpq	%rax, %rdx
	jl	.LBB23_125
.LBB23_126:
	movq	32(%rsp), %rax          # 8-byte Reload
	cmpq	%rax, %rdx
	jge	.LBB23_149
# %bb.127:
	movq	%rax, %rbx
	subq	%rdx, %rbx
	cmpq	$32, %rbx
	jb	.LBB23_141
# %bb.128:
	leaq	(%r14,%rcx,4), %r8
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	movq	%rax, %rsi
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	cmpq	%rax, %r8
	jae	.LBB23_131
# %bb.129:
	leaq	(%rcx,%rsi), %rax
	subq	%rdx, %rax
	leaq	(%r14,%rax,4), %rax
	cmpq	%rax, %r9
	jae	.LBB23_131
# %bb.130:
	movq	%rsi, %rax
	jmp	.LBB23_141
.LBB23_131:
	movq	%rbx, %r12
	andq	$-32, %r12
	leaq	-32(%r12), %rax
	movq	%rax, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %r11d
	andl	$3, %r11d
	cmpq	$96, %rax
	jae	.LBB23_133
# %bb.132:
	xorl	%edi, %edi
	jmp	.LBB23_135
.LBB23_133:
	subq	%r11, %rbp
	leaq	480(,%rdx,4), %rax
	addq	%r13, %rax
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB23_134:                             # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rax,%rdi,4), %ymm0
	vmovups	-448(%rax,%rdi,4), %ymm1
	vmovups	-416(%rax,%rdi,4), %ymm2
	vmovups	-384(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rdi,4)
	vmovups	%ymm1, -448(%rsi,%rdi,4)
	vmovups	%ymm2, -416(%rsi,%rdi,4)
	vmovups	%ymm3, -384(%rsi,%rdi,4)
	vmovups	-352(%rax,%rdi,4), %ymm0
	vmovups	-320(%rax,%rdi,4), %ymm1
	vmovups	-288(%rax,%rdi,4), %ymm2
	vmovups	-256(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rdi,4)
	vmovups	%ymm1, -320(%rsi,%rdi,4)
	vmovups	%ymm2, -288(%rsi,%rdi,4)
	vmovups	%ymm3, -256(%rsi,%rdi,4)
	vmovups	-224(%rax,%rdi,4), %ymm0
	vmovups	-192(%rax,%rdi,4), %ymm1
	vmovups	-160(%rax,%rdi,4), %ymm2
	vmovups	-128(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rdi,4)
	vmovups	%ymm1, -192(%rsi,%rdi,4)
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovdqu	-96(%rax,%rdi,4), %ymm0
	vmovdqu	-64(%rax,%rdi,4), %ymm1
	vmovdqu	-32(%rax,%rdi,4), %ymm2
	vmovups	(%rax,%rdi,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rdi,4)
	vmovdqu	%ymm1, -64(%rsi,%rdi,4)
	vmovdqu	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	subq	$-128, %rdi
	addq	$-4, %rbp
	jne	.LBB23_134
.LBB23_135:
	testq	%r11, %r11
	je	.LBB23_138
# %bb.136:
	leaq	96(,%rdi,4), %rax
	negq	%r11
	.p2align	4, 0x90
.LBB23_137:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%r9,%rax), %ymm0
	vmovdqu	-64(%r9,%rax), %ymm1
	vmovdqu	-32(%r9,%rax), %ymm2
	vmovups	(%r9,%rax), %ymm3
	vmovdqu	%ymm0, -96(%r8,%rax)
	vmovdqu	%ymm1, -64(%r8,%rax)
	vmovdqu	%ymm2, -32(%r8,%rax)
	vmovups	%ymm3, (%r8,%rax)
	subq	$-128, %rax
	incq	%r11
	jne	.LBB23_137
.LBB23_138:
	addq	%r12, %rcx
	cmpq	%r12, %rbx
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	32(%rsp), %rax          # 8-byte Reload
	jne	.LBB23_140
# %bb.139:
	movq	8(%rsp), %r12           # 8-byte Reload
	movq	(%rsp), %r8             # 8-byte Reload
	jmp	.LBB23_149
.LBB23_140:
	addq	%r12, %rdx
	movq	8(%rsp), %r12           # 8-byte Reload
.LBB23_141:
	movl	%eax, %esi
	subl	%edx, %esi
	movq	%rdx, %r8
	notq	%r8
	addq	%rax, %r8
	andq	$7, %rsi
	je	.LBB23_145
# %bb.142:
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%eax, %eax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_143:                             # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rax), %edi
	movl	%edi, (%rbp,%rax)
	decq	%rbx
	addq	$4, %rax
	cmpq	%rbx, %rsi
	jne	.LBB23_143
# %bb.144:
	subq	%rbx, %rdx
	subq	%rbx, %rcx
	movq	32(%rsp), %rax          # 8-byte Reload
.LBB23_145:
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_149
# %bb.146:
	subq	%rdx, %rax
	leaq	28(,%rdx,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$28, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_147:                             # =>This Inner Loop Header: Depth=1
	movl	-28(%rsi,%rdx,4), %ebp
	movl	%ebp, -28(%rdi,%rdx,4)
	movl	-24(%rsi,%rdx,4), %ebp
	movl	%ebp, -24(%rdi,%rdx,4)
	movl	-20(%rsi,%rdx,4), %ebp
	movl	%ebp, -20(%rdi,%rdx,4)
	movl	-16(%rsi,%rdx,4), %ebp
	movl	%ebp, -16(%rdi,%rdx,4)
	movl	-12(%rsi,%rdx,4), %ebp
	movl	%ebp, -12(%rdi,%rdx,4)
	movl	-8(%rsi,%rdx,4), %ebp
	movl	%ebp, -8(%rdi,%rdx,4)
	movl	-4(%rsi,%rdx,4), %ebp
	movl	%ebp, -4(%rdi,%rdx,4)
	movl	(%rsi,%rdx,4), %ebp
	movl	%ebp, (%rdi,%rdx,4)
	addq	$8, %rdx
	cmpq	%rdx, %rax
	jne	.LBB23_147
# %bb.148:
	addq	%rdx, %rcx
	movq	32(%rsp), %rax          # 8-byte Reload
.LBB23_149:
	subl	%r10d, %eax
	movq	%rax, 32(%rsp)          # 8-byte Spill
	movl	%eax, 80(%rsp)
	leaq	(%r8,%r8,4), %r10
	movslq	84(%rsp), %rbp
	leaq	-7(%rbp), %rax
	cmpq	%rax, %r10
	jge	.LBB23_154
# %bb.150:
	leaq	(%r11,%r11,4), %rdx
	addq	%rax, %rdx
	leaq	(%r15,%r15,4), %rdi
	notq	%rdi
	addq	%rdx, %rdi
	movl	%edi, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_155
# %bb.151:
	negq	%rsi
	movq	%r10, %rdx
	.p2align	4, 0x90
.LBB23_152:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rdx
	incq	%rsi
	jne	.LBB23_152
# %bb.153:
	cmpq	$56, %rdi
	jae	.LBB23_156
	jmp	.LBB23_157
.LBB23_154:
	movq	%r10, %rdx
	jmp	.LBB23_157
.LBB23_155:
	movq	%r10, %rdx
	cmpq	$56, %rdi
	jb	.LBB23_157
	.p2align	4, 0x90
.LBB23_156:                             # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rdx
	cmpq	%rax, %rdx
	jl	.LBB23_156
.LBB23_157:
	cmpq	%rbp, %rdx
	jge	.LBB23_179
# %bb.158:
	movq	%rbp, %rbx
	subq	%rdx, %rbx
	cmpq	$32, %rbx
	jb	.LBB23_171
# %bb.159:
	leaq	(%r14,%rcx,4), %r8
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(,%rbp,4), %rax
	addq	%r13, %rax
	cmpq	%rax, %r8
	jae	.LBB23_161
# %bb.160:
	leaq	(%rcx,%rbp), %rax
	subq	%rdx, %rax
	leaq	(%r14,%rax,4), %rax
	cmpq	%rax, %r9
	jb	.LBB23_171
.LBB23_161:
	movq	%rbp, 120(%rsp)         # 8-byte Spill
	movq	%rbx, %r12
	andq	$-32, %r12
	leaq	-32(%r12), %rax
	movq	%rax, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %r11d
	andl	$3, %r11d
	cmpq	$96, %rax
	jae	.LBB23_163
# %bb.162:
	xorl	%edi, %edi
	jmp	.LBB23_165
.LBB23_163:
	subq	%r11, %rbp
	leaq	480(,%rdx,4), %rax
	addq	%r13, %rax
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB23_164:                             # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rax,%rdi,4), %ymm0
	vmovups	-448(%rax,%rdi,4), %ymm1
	vmovups	-416(%rax,%rdi,4), %ymm2
	vmovups	-384(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rdi,4)
	vmovups	%ymm1, -448(%rsi,%rdi,4)
	vmovups	%ymm2, -416(%rsi,%rdi,4)
	vmovups	%ymm3, -384(%rsi,%rdi,4)
	vmovups	-352(%rax,%rdi,4), %ymm0
	vmovups	-320(%rax,%rdi,4), %ymm1
	vmovups	-288(%rax,%rdi,4), %ymm2
	vmovups	-256(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rdi,4)
	vmovups	%ymm1, -320(%rsi,%rdi,4)
	vmovups	%ymm2, -288(%rsi,%rdi,4)
	vmovups	%ymm3, -256(%rsi,%rdi,4)
	vmovups	-224(%rax,%rdi,4), %ymm0
	vmovups	-192(%rax,%rdi,4), %ymm1
	vmovups	-160(%rax,%rdi,4), %ymm2
	vmovups	-128(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rdi,4)
	vmovups	%ymm1, -192(%rsi,%rdi,4)
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovdqu	-96(%rax,%rdi,4), %ymm0
	vmovdqu	-64(%rax,%rdi,4), %ymm1
	vmovdqu	-32(%rax,%rdi,4), %ymm2
	vmovups	(%rax,%rdi,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rdi,4)
	vmovdqu	%ymm1, -64(%rsi,%rdi,4)
	vmovdqu	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	subq	$-128, %rdi
	addq	$-4, %rbp
	jne	.LBB23_164
.LBB23_165:
	testq	%r11, %r11
	je	.LBB23_168
# %bb.166:
	leaq	96(,%rdi,4), %rax
	negq	%r11
	.p2align	4, 0x90
.LBB23_167:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%r9,%rax), %ymm0
	vmovdqu	-64(%r9,%rax), %ymm1
	vmovdqu	-32(%r9,%rax), %ymm2
	vmovups	(%r9,%rax), %ymm3
	vmovdqu	%ymm0, -96(%r8,%rax)
	vmovdqu	%ymm1, -64(%r8,%rax)
	vmovdqu	%ymm2, -32(%r8,%rax)
	vmovups	%ymm3, (%r8,%rax)
	subq	$-128, %rax
	incq	%r11
	jne	.LBB23_167
.LBB23_168:
	addq	%r12, %rcx
	cmpq	%r12, %rbx
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	120(%rsp), %rbp         # 8-byte Reload
	jne	.LBB23_170
# %bb.169:
	movq	8(%rsp), %r12           # 8-byte Reload
	movq	(%rsp), %r8             # 8-byte Reload
	jmp	.LBB23_179
.LBB23_170:
	addq	%r12, %rdx
	movq	8(%rsp), %r12           # 8-byte Reload
.LBB23_171:
	movl	%ebp, %esi
	subl	%edx, %esi
	movq	%rdx, %r8
	notq	%r8
	addq	%rbp, %r8
	andq	$7, %rsi
	je	.LBB23_175
# %bb.172:
	movq	%r11, %r12
	movq	%rbp, %r11
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%eax, %eax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_173:                             # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rax), %edi
	movl	%edi, (%rbp,%rax)
	decq	%rbx
	addq	$4, %rax
	cmpq	%rbx, %rsi
	jne	.LBB23_173
# %bb.174:
	subq	%rbx, %rdx
	subq	%rbx, %rcx
	movq	%r11, %rbp
	movq	%r12, %r11
	movq	8(%rsp), %r12           # 8-byte Reload
.LBB23_175:
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_179
# %bb.176:
	movq	%rbp, %rbx
	movq	%rbp, %rax
	subq	%rdx, %rax
	leaq	28(,%rdx,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$28, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_177:                             # =>This Inner Loop Header: Depth=1
	movl	-28(%rsi,%rdx,4), %ebp
	movl	%ebp, -28(%rdi,%rdx,4)
	movl	-24(%rsi,%rdx,4), %ebp
	movl	%ebp, -24(%rdi,%rdx,4)
	movl	-20(%rsi,%rdx,4), %ebp
	movl	%ebp, -20(%rdi,%rdx,4)
	movl	-16(%rsi,%rdx,4), %ebp
	movl	%ebp, -16(%rdi,%rdx,4)
	movl	-12(%rsi,%rdx,4), %ebp
	movl	%ebp, -12(%rdi,%rdx,4)
	movl	-8(%rsi,%rdx,4), %ebp
	movl	%ebp, -8(%rdi,%rdx,4)
	movl	-4(%rsi,%rdx,4), %ebp
	movl	%ebp, -4(%rdi,%rdx,4)
	movl	(%rsi,%rdx,4), %ebp
	movl	%ebp, (%rdi,%rdx,4)
	addq	$8, %rdx
	cmpq	%rdx, %rax
	jne	.LBB23_177
# %bb.178:
	addq	%rdx, %rcx
	movq	%rbx, %rbp
.LBB23_179:
	subl	%r10d, %ebp
	movl	%ebp, 84(%rsp)
	leaq	(%r8,%r8), %rax
	leaq	(%rax,%rax,2), %r10
	movslq	88(%rsp), %rbx
	leaq	-7(%rbx), %rax
	cmpq	%rax, %r10
	jge	.LBB23_184
# %bb.180:
	leaq	(%r11,%r11,2), %rdx
	leaq	(%rax,%rdx,2), %rdx
	leaq	(%r15,%r15), %rsi
	leaq	(%rsi,%rsi,2), %rdi
	notq	%rdi
	addq	%rdx, %rdi
	movl	%edi, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_185
# %bb.181:
	negq	%rsi
	movq	%r10, %rdx
	.p2align	4, 0x90
.LBB23_182:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rdx
	incq	%rsi
	jne	.LBB23_182
# %bb.183:
	cmpq	$56, %rdi
	jae	.LBB23_186
	jmp	.LBB23_187
.LBB23_184:
	movq	%r10, %rdx
	jmp	.LBB23_187
.LBB23_185:
	movq	%r10, %rdx
	cmpq	$56, %rdi
	jb	.LBB23_187
	.p2align	4, 0x90
.LBB23_186:                             # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rdx
	cmpq	%rax, %rdx
	jl	.LBB23_186
.LBB23_187:
	cmpq	%rbx, %rdx
	movq	%rbp, 120(%rsp)         # 8-byte Spill
	jge	.LBB23_211
# %bb.188:
	movq	%rbx, %rsi
	subq	%rdx, %rbx
	cmpq	$32, %rbx
	jb	.LBB23_189
# %bb.190:
	leaq	(%r14,%rcx,4), %r8
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(,%rsi,4), %rax
	addq	%r13, %rax
	cmpq	%rax, %r8
	jae	.LBB23_193
# %bb.191:
	leaq	(%rcx,%rsi), %rax
	subq	%rdx, %rax
	leaq	(%r14,%rax,4), %rax
	cmpq	%rax, %r9
	jae	.LBB23_193
.LBB23_189:
	movq	%rsi, %rbx
.LBB23_203:
	movl	%ebx, %esi
	subl	%edx, %esi
	movq	%rdx, %r8
	notq	%r8
	addq	%rbx, %r8
	andq	$7, %rsi
	je	.LBB23_207
# %bb.204:
	movq	%r11, %r12
	movq	%rbx, %r11
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%eax, %eax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_205:                             # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rax), %edi
	movl	%edi, (%rbp,%rax)
	decq	%rbx
	addq	$4, %rax
	cmpq	%rbx, %rsi
	jne	.LBB23_205
# %bb.206:
	subq	%rbx, %rdx
	subq	%rbx, %rcx
	movq	%r11, %rbx
	movq	%r12, %r11
	movq	8(%rsp), %r12           # 8-byte Reload
.LBB23_207:
	cmpq	$7, %r8
	movq	(%rsp), %r8             # 8-byte Reload
	jb	.LBB23_211
# %bb.208:
	movq	%rbx, %rax
	subq	%rdx, %rax
	leaq	28(,%rdx,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$28, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_209:                             # =>This Inner Loop Header: Depth=1
	movl	-28(%rsi,%rdx,4), %ebp
	movl	%ebp, -28(%rdi,%rdx,4)
	movl	-24(%rsi,%rdx,4), %ebp
	movl	%ebp, -24(%rdi,%rdx,4)
	movl	-20(%rsi,%rdx,4), %ebp
	movl	%ebp, -20(%rdi,%rdx,4)
	movl	-16(%rsi,%rdx,4), %ebp
	movl	%ebp, -16(%rdi,%rdx,4)
	movl	-12(%rsi,%rdx,4), %ebp
	movl	%ebp, -12(%rdi,%rdx,4)
	movl	-8(%rsi,%rdx,4), %ebp
	movl	%ebp, -8(%rdi,%rdx,4)
	movl	-4(%rsi,%rdx,4), %ebp
	movl	%ebp, -4(%rdi,%rdx,4)
	movl	(%rsi,%rdx,4), %ebp
	movl	%ebp, (%rdi,%rdx,4)
	addq	$8, %rdx
	cmpq	%rdx, %rax
	jne	.LBB23_209
# %bb.210:
	addq	%rdx, %rcx
	jmp	.LBB23_211
.LBB23_193:
	movq	%rsi, 112(%rsp)         # 8-byte Spill
	movq	%rbx, %r12
	andq	$-32, %r12
	leaq	-32(%r12), %rax
	movq	%rax, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %r11d
	andl	$3, %r11d
	cmpq	$96, %rax
	jae	.LBB23_195
# %bb.194:
	xorl	%edi, %edi
	jmp	.LBB23_197
.LBB23_195:
	subq	%r11, %rbp
	leaq	480(,%rdx,4), %rax
	addq	%r13, %rax
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB23_196:                             # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rax,%rdi,4), %ymm0
	vmovups	-448(%rax,%rdi,4), %ymm1
	vmovups	-416(%rax,%rdi,4), %ymm2
	vmovups	-384(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rdi,4)
	vmovups	%ymm1, -448(%rsi,%rdi,4)
	vmovups	%ymm2, -416(%rsi,%rdi,4)
	vmovups	%ymm3, -384(%rsi,%rdi,4)
	vmovups	-352(%rax,%rdi,4), %ymm0
	vmovups	-320(%rax,%rdi,4), %ymm1
	vmovups	-288(%rax,%rdi,4), %ymm2
	vmovups	-256(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rdi,4)
	vmovups	%ymm1, -320(%rsi,%rdi,4)
	vmovups	%ymm2, -288(%rsi,%rdi,4)
	vmovups	%ymm3, -256(%rsi,%rdi,4)
	vmovups	-224(%rax,%rdi,4), %ymm0
	vmovups	-192(%rax,%rdi,4), %ymm1
	vmovups	-160(%rax,%rdi,4), %ymm2
	vmovups	-128(%rax,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rdi,4)
	vmovups	%ymm1, -192(%rsi,%rdi,4)
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovdqu	-96(%rax,%rdi,4), %ymm0
	vmovdqu	-64(%rax,%rdi,4), %ymm1
	vmovdqu	-32(%rax,%rdi,4), %ymm2
	vmovups	(%rax,%rdi,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rdi,4)
	vmovdqu	%ymm1, -64(%rsi,%rdi,4)
	vmovdqu	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	subq	$-128, %rdi
	addq	$-4, %rbp
	jne	.LBB23_196
.LBB23_197:
	testq	%r11, %r11
	je	.LBB23_200
# %bb.198:
	leaq	96(,%rdi,4), %rax
	negq	%r11
	.p2align	4, 0x90
.LBB23_199:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%r9,%rax), %ymm0
	vmovdqu	-64(%r9,%rax), %ymm1
	vmovdqu	-32(%r9,%rax), %ymm2
	vmovups	(%r9,%rax), %ymm3
	vmovdqu	%ymm0, -96(%r8,%rax)
	vmovdqu	%ymm1, -64(%r8,%rax)
	vmovdqu	%ymm2, -32(%r8,%rax)
	vmovups	%ymm3, (%r8,%rax)
	subq	$-128, %rax
	incq	%r11
	jne	.LBB23_199
.LBB23_200:
	addq	%r12, %rcx
	cmpq	%r12, %rbx
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	112(%rsp), %rbx         # 8-byte Reload
	jne	.LBB23_202
# %bb.201:
	movq	8(%rsp), %r12           # 8-byte Reload
	movq	(%rsp), %r8             # 8-byte Reload
.LBB23_211:
	subl	%r10d, %ebx
	movq	%rbx, 112(%rsp)         # 8-byte Spill
	movl	%ebx, 88(%rsp)
	leaq	(,%r8,8), %r9
	movq	%r9, %r10
	subq	%r8, %r10
	movslq	92(%rsp), %rbx
	leaq	-7(%rbx), %rdi
	cmpq	%rdi, %r10
	jge	.LBB23_216
# %bb.212:
	leaq	(,%r11,8), %rdx
	subq	%r11, %rdx
	addq	%rdi, %rdx
	leaq	(,%r15,8), %rax
	subq	%r15, %rax
	notq	%rax
	addq	%rdx, %rax
	movl	%eax, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_217
# %bb.213:
	negq	%rsi
	movq	%r10, %rdx
	.p2align	4, 0x90
.LBB23_214:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rdx
	incq	%rsi
	jne	.LBB23_214
# %bb.215:
	cmpq	$56, %rax
	jae	.LBB23_218
	jmp	.LBB23_219
.LBB23_216:
	movq	%r10, %rdx
	jmp	.LBB23_219
.LBB23_217:
	movq	%r10, %rdx
	cmpq	$56, %rax
	jb	.LBB23_219
	.p2align	4, 0x90
.LBB23_218:                             # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rdx,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rdx
	cmpq	%rdi, %rdx
	jl	.LBB23_218
.LBB23_219:
	cmpq	%rbx, %rdx
	jge	.LBB23_242
# %bb.220:
	movq	%rbx, %r11
	subq	%rdx, %r11
	cmpq	$32, %r11
	jae	.LBB23_222
# %bb.221:
	movq	%r12, %rax
	movq	16(%rsp), %r11          # 8-byte Reload
	jmp	.LBB23_234
.LBB23_222:
	movq	%r9, 24(%rsp)           # 8-byte Spill
	leaq	(%r14,%rcx,4), %r9
	leaq	(,%rdx,4), %rbp
	addq	%r13, %rbp
	leaq	(,%rbx,4), %rax
	addq	%r13, %rax
	cmpq	%rax, %r9
	jae	.LBB23_225
# %bb.223:
	leaq	(%rcx,%rbx), %rax
	subq	%rdx, %rax
	leaq	(%r14,%rax,4), %rax
	cmpq	%rax, %rbp
	jae	.LBB23_225
# %bb.224:
	movq	%r12, %rax
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	24(%rsp), %r9           # 8-byte Reload
	jmp	.LBB23_234
.LBB23_225:
	movq	%rbx, (%rsp)            # 8-byte Spill
	movq	%r11, %rax
	andq	$-32, %rax
	movq	%rax, %rbx
	leaq	-32(%rax), %rsi
	movq	%rsi, %rax
	shrq	$5, %rax
	incq	%rax
	movl	%eax, %r8d
	andl	$3, %r8d
	cmpq	$96, %rsi
	jae	.LBB23_227
# %bb.226:
	xorl	%edi, %edi
	jmp	.LBB23_229
.LBB23_202:
	addq	%r12, %rdx
	movq	8(%rsp), %r12           # 8-byte Reload
	jmp	.LBB23_203
.LBB23_227:
	subq	%r8, %rax
	leaq	480(,%rdx,4), %r12
	addq	%r13, %r12
	leaq	(%r14,%rcx,4), %rsi
	addq	$480, %rsi              # imm = 0x1E0
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB23_228:                             # =>This Inner Loop Header: Depth=1
	vmovups	-480(%r12,%rdi,4), %ymm0
	vmovups	-448(%r12,%rdi,4), %ymm1
	vmovups	-416(%r12,%rdi,4), %ymm2
	vmovups	-384(%r12,%rdi,4), %ymm3
	vmovups	%ymm0, -480(%rsi,%rdi,4)
	vmovups	%ymm1, -448(%rsi,%rdi,4)
	vmovups	%ymm2, -416(%rsi,%rdi,4)
	vmovups	%ymm3, -384(%rsi,%rdi,4)
	vmovups	-352(%r12,%rdi,4), %ymm0
	vmovups	-320(%r12,%rdi,4), %ymm1
	vmovups	-288(%r12,%rdi,4), %ymm2
	vmovups	-256(%r12,%rdi,4), %ymm3
	vmovups	%ymm0, -352(%rsi,%rdi,4)
	vmovups	%ymm1, -320(%rsi,%rdi,4)
	vmovups	%ymm2, -288(%rsi,%rdi,4)
	vmovups	%ymm3, -256(%rsi,%rdi,4)
	vmovups	-224(%r12,%rdi,4), %ymm0
	vmovups	-192(%r12,%rdi,4), %ymm1
	vmovups	-160(%r12,%rdi,4), %ymm2
	vmovups	-128(%r12,%rdi,4), %ymm3
	vmovups	%ymm0, -224(%rsi,%rdi,4)
	vmovups	%ymm1, -192(%rsi,%rdi,4)
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovdqu	-96(%r12,%rdi,4), %ymm0
	vmovdqu	-64(%r12,%rdi,4), %ymm1
	vmovdqu	-32(%r12,%rdi,4), %ymm2
	vmovups	(%r12,%rdi,4), %ymm3
	vmovdqu	%ymm0, -96(%rsi,%rdi,4)
	vmovdqu	%ymm1, -64(%rsi,%rdi,4)
	vmovdqu	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	subq	$-128, %rdi
	addq	$-4, %rax
	jne	.LBB23_228
.LBB23_229:
	testq	%r8, %r8
	je	.LBB23_232
# %bb.230:
	leaq	96(,%rdi,4), %rax
	negq	%r8
	.p2align	4, 0x90
.LBB23_231:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%rbp,%rax), %ymm0
	vmovdqu	-64(%rbp,%rax), %ymm1
	vmovdqu	-32(%rbp,%rax), %ymm2
	vmovups	(%rbp,%rax), %ymm3
	vmovdqu	%ymm0, -96(%r9,%rax)
	vmovdqu	%ymm1, -64(%r9,%rax)
	vmovdqu	%ymm2, -32(%r9,%rax)
	vmovups	%ymm3, (%r9,%rax)
	subq	$-128, %rax
	incq	%r8
	jne	.LBB23_231
.LBB23_232:
	movq	%rbx, %rsi
	addq	%rbx, %rcx
	cmpq	%rbx, %r11
	movq	8(%rsp), %r12           # 8-byte Reload
	movq	16(%rsp), %r11          # 8-byte Reload
	movq	(%rsp), %rbx            # 8-byte Reload
	movq	24(%rsp), %r9           # 8-byte Reload
	je	.LBB23_242
# %bb.233:
	movq	%r12, %rax
	addq	%rsi, %rdx
.LBB23_234:
	movl	%ebx, %esi
	subl	%edx, %esi
	movq	%rdx, %r8
	notq	%r8
	movq	%rbx, %r12
	addq	%rbx, %r8
	andq	$7, %rsi
	je	.LBB23_238
# %bb.235:
	movq	%r9, %r11
	leaq	(,%rdx,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%eax, %eax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_236:                             # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rax), %edi
	movl	%edi, (%rbp,%rax)
	decq	%rbx
	addq	$4, %rax
	cmpq	%rbx, %rsi
	jne	.LBB23_236
# %bb.237:
	subq	%rbx, %rdx
	subq	%rbx, %rcx
	movq	16(%rsp), %rsi          # 8-byte Reload
	movq	8(%rsp), %rax           # 8-byte Reload
	movq	%r11, %r9
	movq	%rsi, %r11
.LBB23_238:
	cmpq	$7, %r8
	movq	%r12, %rbx
	movq	%rax, %r12
	jb	.LBB23_242
# %bb.239:
	movq	%rbx, %rax
	subq	%rdx, %rax
	leaq	28(,%rdx,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$28, %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_240:                             # =>This Inner Loop Header: Depth=1
	movl	-28(%rsi,%rdx,4), %ebp
	movl	%ebp, -28(%rdi,%rdx,4)
	movl	-24(%rsi,%rdx,4), %ebp
	movl	%ebp, -24(%rdi,%rdx,4)
	movl	-20(%rsi,%rdx,4), %ebp
	movl	%ebp, -20(%rdi,%rdx,4)
	movl	-16(%rsi,%rdx,4), %ebp
	movl	%ebp, -16(%rdi,%rdx,4)
	movl	-12(%rsi,%rdx,4), %ebp
	movl	%ebp, -12(%rdi,%rdx,4)
	movl	-8(%rsi,%rdx,4), %ebp
	movl	%ebp, -8(%rdi,%rdx,4)
	movl	-4(%rsi,%rdx,4), %ebp
	movl	%ebp, -4(%rdi,%rdx,4)
	movl	(%rsi,%rdx,4), %ebp
	movl	%ebp, (%rdi,%rdx,4)
	addq	$8, %rdx
	cmpq	%rdx, %rax
	jne	.LBB23_240
# %bb.241:
	addq	%rdx, %rcx
.LBB23_242:
	subl	%r10d, %ebx
	movl	%ebx, 92(%rsp)
	movslq	96(%rsp), %rbp
	leaq	-7(%rbp), %rdx
	cmpq	%rdx, %r9
	jge	.LBB23_247
# %bb.243:
	leaq	(%rdx,%r11,8), %rax
	shlq	$3, %r15
	notq	%r15
	addq	%rax, %r15
	movl	%r15d, %esi
	shrl	$3, %esi
	incl	%esi
	andq	$7, %rsi
	je	.LBB23_248
# %bb.244:
	negq	%rsi
	movq	%r9, %rax
	movq	56(%rsp), %r10          # 8-byte Reload
	.p2align	4, 0x90
.LBB23_245:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r13,%rax,4), %ymm0
	vmovdqu	%ymm0, (%r14,%rcx,4)
	addq	$8, %rcx
	addq	$8, %rax
	incq	%rsi
	jne	.LBB23_245
# %bb.246:
	cmpq	$56, %r15
	jae	.LBB23_249
	jmp	.LBB23_250
.LBB23_247:
	movq	%r9, %rax
	movq	56(%rsp), %r10          # 8-byte Reload
	jmp	.LBB23_250
.LBB23_248:
	movq	%r9, %rax
	movq	56(%rsp), %r10          # 8-byte Reload
	cmpq	$56, %r15
	jb	.LBB23_250
	.p2align	4, 0x90
.LBB23_249:                             # =>This Inner Loop Header: Depth=1
	vmovups	(%r13,%rax,4), %ymm0
	vmovups	%ymm0, (%r14,%rcx,4)
	vmovups	32(%r13,%rax,4), %ymm0
	vmovups	%ymm0, 32(%r14,%rcx,4)
	vmovups	64(%r13,%rax,4), %ymm0
	vmovups	%ymm0, 64(%r14,%rcx,4)
	vmovups	96(%r13,%rax,4), %ymm0
	vmovups	%ymm0, 96(%r14,%rcx,4)
	vmovups	128(%r13,%rax,4), %ymm0
	vmovups	%ymm0, 128(%r14,%rcx,4)
	vmovups	160(%r13,%rax,4), %ymm0
	vmovups	%ymm0, 160(%r14,%rcx,4)
	vmovups	192(%r13,%rax,4), %ymm0
	vmovups	%ymm0, 192(%r14,%rcx,4)
	vmovdqu	224(%r13,%rax,4), %ymm0
	vmovdqu	%ymm0, 224(%r14,%rcx,4)
	addq	$64, %rcx
	addq	$64, %rax
	cmpq	%rdx, %rax
	jl	.LBB23_249
.LBB23_250:
	movq	%rbx, (%rsp)            # 8-byte Spill
	cmpq	%rbp, %rax
	movq	112(%rsp), %rbx         # 8-byte Reload
	jge	.LBB23_270
# %bb.251:
	movq	%r9, 24(%rsp)           # 8-byte Spill
	movq	%rbp, %r9
	subq	%rax, %r9
	cmpq	$32, %r9
	jb	.LBB23_263
# %bb.252:
	leaq	(%r14,%rcx,4), %r8
	leaq	(,%rax,4), %r15
	addq	%r13, %r15
	leaq	(,%rbp,4), %rdx
	addq	%r13, %rdx
	cmpq	%rdx, %r8
	jae	.LBB23_254
# %bb.253:
	leaq	(%rcx,%rbp), %rdx
	subq	%rax, %rdx
	leaq	(%r14,%rdx,4), %rdx
	cmpq	%rdx, %r15
	jb	.LBB23_263
.LBB23_254:
	movq	%rbp, 16(%rsp)          # 8-byte Spill
	movq	%r12, %rbx
	movq	%r9, %r11
	andq	$-32, %r11
	leaq	-32(%r11), %rdx
	movq	%rdx, %rbp
	shrq	$5, %rbp
	incq	%rbp
	movl	%ebp, %r12d
	andl	$3, %r12d
	cmpq	$96, %rdx
	jae	.LBB23_256
# %bb.255:
	xorl	%edx, %edx
	jmp	.LBB23_258
.LBB23_256:
	subq	%r12, %rbp
	leaq	480(,%rax,4), %rsi
	addq	%r13, %rsi
	leaq	(%r14,%rcx,4), %rdi
	addq	$480, %rdi              # imm = 0x1E0
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB23_257:                             # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rsi,%rdx,4), %ymm0
	vmovups	-448(%rsi,%rdx,4), %ymm1
	vmovups	-416(%rsi,%rdx,4), %ymm2
	vmovups	-384(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -480(%rdi,%rdx,4)
	vmovups	%ymm1, -448(%rdi,%rdx,4)
	vmovups	%ymm2, -416(%rdi,%rdx,4)
	vmovups	%ymm3, -384(%rdi,%rdx,4)
	vmovups	-352(%rsi,%rdx,4), %ymm0
	vmovups	-320(%rsi,%rdx,4), %ymm1
	vmovups	-288(%rsi,%rdx,4), %ymm2
	vmovups	-256(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -352(%rdi,%rdx,4)
	vmovups	%ymm1, -320(%rdi,%rdx,4)
	vmovups	%ymm2, -288(%rdi,%rdx,4)
	vmovups	%ymm3, -256(%rdi,%rdx,4)
	vmovups	-224(%rsi,%rdx,4), %ymm0
	vmovups	-192(%rsi,%rdx,4), %ymm1
	vmovups	-160(%rsi,%rdx,4), %ymm2
	vmovups	-128(%rsi,%rdx,4), %ymm3
	vmovups	%ymm0, -224(%rdi,%rdx,4)
	vmovups	%ymm1, -192(%rdi,%rdx,4)
	vmovups	%ymm2, -160(%rdi,%rdx,4)
	vmovups	%ymm3, -128(%rdi,%rdx,4)
	vmovdqu	-96(%rsi,%rdx,4), %ymm0
	vmovdqu	-64(%rsi,%rdx,4), %ymm1
	vmovdqu	-32(%rsi,%rdx,4), %ymm2
	vmovups	(%rsi,%rdx,4), %ymm3
	vmovdqu	%ymm0, -96(%rdi,%rdx,4)
	vmovdqu	%ymm1, -64(%rdi,%rdx,4)
	vmovdqu	%ymm2, -32(%rdi,%rdx,4)
	vmovups	%ymm3, (%rdi,%rdx,4)
	subq	$-128, %rdx
	addq	$-4, %rbp
	jne	.LBB23_257
.LBB23_258:
	testq	%r12, %r12
	je	.LBB23_261
# %bb.259:
	leaq	96(,%rdx,4), %rdx
	negq	%r12
	.p2align	4, 0x90
.LBB23_260:                             # =>This Inner Loop Header: Depth=1
	vmovdqu	-96(%r15,%rdx), %ymm0
	vmovdqu	-64(%r15,%rdx), %ymm1
	vmovdqu	-32(%r15,%rdx), %ymm2
	vmovups	(%r15,%rdx), %ymm3
	vmovdqu	%ymm0, -96(%r8,%rdx)
	vmovdqu	%ymm1, -64(%r8,%rdx)
	vmovdqu	%ymm2, -32(%r8,%rdx)
	vmovups	%ymm3, (%r8,%rdx)
	subq	$-128, %rdx
	incq	%r12
	jne	.LBB23_260
.LBB23_261:
	cmpq	%r11, %r9
	movq	%rbx, %r12
	movq	112(%rsp), %rbx         # 8-byte Reload
	movq	24(%rsp), %r9           # 8-byte Reload
	movq	16(%rsp), %rbp          # 8-byte Reload
	je	.LBB23_270
# %bb.262:
	addq	%r11, %rax
	addq	%r11, %rcx
.LBB23_263:
	movl	%ebp, %esi
	subl	%eax, %esi
	movq	%rax, %r8
	notq	%r8
	movq	%rbp, %r15
	addq	%rbp, %r8
	andq	$7, %rsi
	je	.LBB23_267
# %bb.264:
	movq	%rbx, %r11
	leaq	(,%rax,4), %r9
	addq	%r13, %r9
	leaq	(%r14,%rcx,4), %rbp
	negq	%rsi
	xorl	%edx, %edx
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_265:                             # =>This Inner Loop Header: Depth=1
	movl	(%r9,%rdx), %edi
	movl	%edi, (%rbp,%rdx)
	decq	%rbx
	addq	$4, %rdx
	cmpq	%rbx, %rsi
	jne	.LBB23_265
# %bb.266:
	subq	%rbx, %rax
	subq	%rbx, %rcx
	movq	%r11, %rbx
.LBB23_267:
	movq	24(%rsp), %r9           # 8-byte Reload
	cmpq	$7, %r8
	movq	%r15, %rbp
	jb	.LBB23_270
# %bb.268:
	movq	%rbp, %rdx
	subq	%rax, %rdx
	leaq	28(,%rax,4), %rax
	addq	%r13, %rax
	leaq	(%r14,%rcx,4), %rcx
	addq	$28, %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB23_269:                             # =>This Inner Loop Header: Depth=1
	movl	-28(%rax,%rsi,4), %edi
	movl	%edi, -28(%rcx,%rsi,4)
	movl	-24(%rax,%rsi,4), %edi
	movl	%edi, -24(%rcx,%rsi,4)
	movl	-20(%rax,%rsi,4), %edi
	movl	%edi, -20(%rcx,%rsi,4)
	movl	-16(%rax,%rsi,4), %edi
	movl	%edi, -16(%rcx,%rsi,4)
	movl	-12(%rax,%rsi,4), %edi
	movl	%edi, -12(%rcx,%rsi,4)
	movl	-8(%rax,%rsi,4), %edi
	movl	%edi, -8(%rcx,%rsi,4)
	movl	-4(%rax,%rsi,4), %edi
	movl	%edi, -4(%rcx,%rsi,4)
	movl	(%rax,%rsi,4), %edi
	movl	%edi, (%rcx,%rsi,4)
	addq	$8, %rsi
	cmpq	%rsi, %rdx
	jne	.LBB23_269
.LBB23_270:
	subl	%r9d, %ebp
	movl	%r10d, %r15d
	addl	%r12d, %r15d
	leal	(%r10,%r12), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r12d, %esi
	movq	%r13, %rcx
	vzeroupper
	callq	sort_quick_multi_h
	movq	128(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r12d
	addl	%r15d, %r12d
	leal	(%rax,%r15), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r15d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	movq	48(%rsp), %rax          # 8-byte Reload
	movl	%eax, %r15d
	addl	%r12d, %r15d
	leal	(%rax,%r12), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r12d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	movq	40(%rsp), %rax          # 8-byte Reload
	movl	%eax, %r12d
	addl	%r15d, %r12d
	leal	(%rax,%r15), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r15d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	movq	32(%rsp), %rax          # 8-byte Reload
	movl	%eax, %r15d
	addl	%r12d, %r15d
	leal	(%rax,%r12), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r12d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	movq	120(%rsp), %rax         # 8-byte Reload
	movl	%eax, %r12d
	addl	%r15d, %r12d
	leal	(%rax,%r15), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r15d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	movl	%ebx, %r15d
	addl	%r12d, %r15d
	leal	(%rbx,%r12), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r12d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	movq	(%rsp), %rax            # 8-byte Reload
	movl	%eax, %r12d
	addl	%r15d, %r12d
	leal	(%rax,%r15), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r15d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
	leal	(%r12,%rbp), %edx
	decl	%edx
	movq	%r14, %rdi
	movl	%r12d, %esi
	movq	%r13, %rcx
	callq	sort_quick_multi_h
.LBB23_271:
	addq	$136, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end23:
	.size	sort_quick_multi_h, .Lfunc_end23-sort_quick_multi_h
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_multi        # -- Begin function sort_quick_multi
	.p2align	4, 0x90
	.type	sort_quick_multi,@function
sort_quick_multi:                       # @sort_quick_multi
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %r15d
	movl	%esi, %ebp
	movq	%rdi, %r14
	movl	%edx, %eax
	subl	%esi, %eax
	cltq
	shlq	$2, %rax
	leaq	(%rax,%rax,8), %rdi
	callq	malloc
	movq	%rax, %rbx
	movq	%r14, %rdi
	movl	%ebp, %esi
	movl	%r15d, %edx
	movq	%rax, %rcx
	callq	sort_quick_multi_h
	movq	%rbx, %rdi
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	free                    # TAILCALL
.Lfunc_end24:
	.size	sort_quick_multi, .Lfunc_end24-sort_quick_multi
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_multi_inplace # -- Begin function partition_quick_multi_inplace
	.p2align	4, 0x90
	.type	partition_quick_multi_inplace,@function
partition_quick_multi_inplace:          # @partition_quick_multi_inplace
	.cfi_startproc
# %bb.0:
	cmpq	%rdx, %rsi
	jle	.LBB25_1
.LBB25_9:
	vzeroupper
	retq
.LBB25_1:
	vmovdqu	(%rdi,%rsi,4), %ymm0
	vmovd	%xmm0, %eax
	.p2align	4, 0x90
.LBB25_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB25_4 Depth 2
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm1, %ymm0, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vpshufd	$238, %xmm1, %xmm2      # xmm2 = xmm1[2,3,2,3]
	vpaddd	%xmm2, %xmm1, %xmm1
	vpshufd	$229, %xmm1, %xmm2      # xmm2 = xmm1[1,1,2,3]
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovd	%xmm1, %r11d
	addl	$8, %r11d
	cmpl	$8, %r11d
	ja	.LBB25_7
# %bb.3:                                #   in Loop: Header=BB25_2 Depth=1
	movq	-8(%rcx,%r11,8), %r8
	decq	%r11
	jmp	.LBB25_4
	.p2align	4, 0x90
.LBB25_6:                               #   in Loop: Header=BB25_4 Depth=2
	movq	%r8, 8(%rcx,%r11,8)
	incq	%r11
	cmpq	$8, %r11
	jae	.LBB25_7
.LBB25_4:                               #   Parent Loop BB25_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r8, %r10
	movq	8(%rcx,%r11,8), %r9
	leaq	1(%r9), %r8
	cmpq	%r8, %r10
	je	.LBB25_6
# %bb.5:                                #   in Loop: Header=BB25_4 Depth=2
	movl	(%rdi,%rsi,4), %r10d
	movl	(%rdi,%r9,4), %eax
	movl	%eax, (%rdi,%rsi,4)
	movl	%r10d, (%rdi,%r9,4)
	jmp	.LBB25_6
	.p2align	4, 0x90
.LBB25_7:                               #   in Loop: Header=BB25_2 Depth=1
	cmpq	%rdx, %rsi
	jge	.LBB25_9
# %bb.8:                                #   in Loop: Header=BB25_2 Depth=1
	movl	4(%rdi,%rsi,4), %eax
	incq	%rsi
	jmp	.LBB25_2
.Lfunc_end25:
	.size	partition_quick_multi_inplace, .Lfunc_end25-partition_quick_multi_inplace
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_multi_inplace_h # -- Begin function sort_quick_multi_inplace_h
	.p2align	4, 0x90
	.type	sort_quick_multi_inplace_h,@function
sort_quick_multi_inplace_h:             # @sort_quick_multi_inplace_h
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%r12
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$72, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -40
	.cfi_offset %r12, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movq	%rdx, %rax
	subq	%rsi, %rax
	jle	.LBB26_12
# %bb.1:
	movq	%rdi, %r12
	cmpq	$201, %rax
	jl	.LBB26_10
# %bb.2:
	vmovq	%rsi, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqu	%ymm0, (%rsp)
	vmovdqu	%ymm0, 32(%rsp)
	movq	%rsi, 64(%rsp)
	vmovdqu	(%r12,%rsi,4), %ymm0
	vmovdqu	(%r12,%rsi,4), %xmm1
	vmovd	%xmm1, %ecx
	movq	%rsi, %r8
	.p2align	4, 0x90
.LBB26_3:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_5 Depth 2
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm1, %ymm0, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpaddd	%xmm1, %xmm2, %xmm1
	vpshufd	$238, %xmm1, %xmm2      # xmm2 = xmm1[2,3,2,3]
	vpaddd	%xmm2, %xmm1, %xmm1
	vpshufd	$229, %xmm1, %xmm2      # xmm2 = xmm1[1,1,2,3]
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovd	%xmm1, %ecx
	addl	$8, %ecx
	cmpl	$8, %ecx
	ja	.LBB26_8
# %bb.4:                                #   in Loop: Header=BB26_3 Depth=1
	movq	-8(%rsp,%rcx,8), %rdi
	decq	%rcx
	jmp	.LBB26_5
	.p2align	4, 0x90
.LBB26_7:                               #   in Loop: Header=BB26_5 Depth=2
	movq	%rdi, 8(%rsp,%rcx,8)
	incq	%rcx
	cmpq	$8, %rcx
	jae	.LBB26_8
.LBB26_5:                               #   Parent Loop BB26_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rdi, %rax
	movq	8(%rsp,%rcx,8), %rbx
	leaq	1(%rbx), %rdi
	cmpq	%rdi, %rax
	je	.LBB26_7
# %bb.6:                                #   in Loop: Header=BB26_5 Depth=2
	movl	(%r12,%r8,4), %r9d
	movl	(%r12,%rbx,4), %eax
	movl	%eax, (%r12,%r8,4)
	movl	%r9d, (%r12,%rbx,4)
	jmp	.LBB26_7
	.p2align	4, 0x90
.LBB26_8:                               #   in Loop: Header=BB26_3 Depth=1
	cmpq	%rdx, %r8
	jge	.LBB26_11
# %bb.9:                                #   in Loop: Header=BB26_3 Depth=1
	movl	4(%r12,%r8,4), %ecx
	incq	%r8
	jmp	.LBB26_3
.LBB26_10:
	movq	%r12, %rdi
                                        # kill: def $esi killed $esi killed $rsi
                                        # kill: def $edx killed $edx killed $rdx
	addq	$72, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	jmp	sort_quick_optimized    # TAILCALL
.LBB26_11:
	.cfi_def_cfa_offset 112
	movq	(%rsp), %r14
	leaq	-1(%r14), %rdx
	movq	%r12, %rdi
	vzeroupper
	callq	sort_quick_multi_inplace_h
	movq	8(%rsp), %r15
	leaq	-1(%r15), %rdx
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	sort_quick_multi_inplace_h
	movq	16(%rsp), %r14
	leaq	-1(%r14), %rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	sort_quick_multi_inplace_h
	movq	24(%rsp), %r15
	leaq	-1(%r15), %rdx
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	sort_quick_multi_inplace_h
	movq	32(%rsp), %r14
	leaq	-1(%r14), %rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	sort_quick_multi_inplace_h
	movq	40(%rsp), %r15
	leaq	-1(%r15), %rdx
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	sort_quick_multi_inplace_h
	movq	48(%rsp), %r14
	leaq	-1(%r14), %rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	sort_quick_multi_inplace_h
	movq	56(%rsp), %r15
	leaq	-1(%r15), %rdx
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	sort_quick_multi_inplace_h
	movq	64(%rsp), %rdx
	decq	%rdx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	sort_quick_multi_inplace_h
.LBB26_12:
	addq	$72, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end26:
	.size	sort_quick_multi_inplace_h, .Lfunc_end26-sort_quick_multi_inplace_h
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_multi_inplace # -- Begin function sort_quick_multi_inplace
	.p2align	4, 0x90
	.type	sort_quick_multi_inplace,@function
sort_quick_multi_inplace:               # @sort_quick_multi_inplace
	.cfi_startproc
# %bb.0:
	movslq	%esi, %rsi
	incl	%edx
	movslq	%edx, %rdx
	jmp	sort_quick_multi_inplace_h # TAILCALL
.Lfunc_end27:
	.size	sort_quick_multi_inplace, .Lfunc_end27-sort_quick_multi_inplace
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_multix4 # -- Begin function partition_quick_multix4
	.p2align	4, 0x90
	.type	partition_quick_multix4,@function
partition_quick_multix4:                # @partition_quick_multix4
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset %rbx, -16
	movl	%edx, %r9d
	movslq	%esi, %rax
	subl	%esi, %r9d
	jle	.LBB28_3
# %bb.1:
	vmovdqu	(%rdi,%rax,4), %xmm0
	movslq	%edx, %r10
	vmovd	%xmm0, %r11d
	leaq	1(%rax), %rsi
	.p2align	4, 0x90
.LBB28_2:                               # =>This Inner Loop Header: Depth=1
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %xmm1
	vpcmpgtd	%xmm1, %xmm0, %xmm1
	vpshufd	$238, %xmm1, %xmm2      # xmm2 = xmm1[2,3,2,3]
	vpaddd	%xmm1, %xmm2, %xmm1
	vpshufd	$229, %xmm1, %xmm2      # xmm2 = xmm1[1,1,2,3]
	vpaddd	%xmm1, %xmm2, %xmm1
	vmovd	%xmm1, %edx
	addl	$4, %edx
	movslq	(%rcx,%rdx,4), %rbx
	movl	%r11d, (%r8,%rbx,4)
	incl	(%rcx,%rdx,4)
	cmpq	%rsi, %r10
	je	.LBB28_3
# %bb.5:                                #   in Loop: Header=BB28_2 Depth=1
	movl	(%rdi,%rsi,4), %r11d
	incq	%rsi
	jmp	.LBB28_2
.LBB28_3:
	movl	(%rcx), %esi
	cmpl	$8, %esi
	jl	.LBB28_4
# %bb.11:
	leaq	(%rdi,%rax,4), %r10
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB28_12:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r8,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%r10,%rdx,4)
	addq	$8, %rdx
	movslq	(%rcx), %rsi
	leaq	-7(%rsi), %rbx
	cmpq	%rbx, %rdx
	jl	.LBB28_12
# %bb.6:
	addq	%rdx, %rax
	movslq	%esi, %rbx
	cmpq	%rbx, %rdx
	jl	.LBB28_8
	jmp	.LBB28_9
.LBB28_4:
	xorl	%edx, %edx
	movslq	%esi, %rbx
	cmpq	%rbx, %rdx
	jge	.LBB28_9
	.p2align	4, 0x90
.LBB28_8:                               # =>This Inner Loop Header: Depth=1
	movl	(%r8,%rdx,4), %esi
	movl	%esi, (%rdi,%rax,4)
	incq	%rax
	incq	%rdx
	movslq	(%rcx), %rsi
	cmpq	%rsi, %rdx
	jl	.LBB28_8
.LBB28_9:
	movslq	%r9d, %r9
	movl	%esi, (%rcx)
	movl	4(%rcx), %esi
	leal	-7(%rsi), %edx
	cmpl	%edx, %r9d
	jge	.LBB28_10
# %bb.13:
	movq	%r9, %rdx
	.p2align	4, 0x90
.LBB28_14:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r8,%rdx,4), %ymm0
	vmovdqu	%ymm0, (%rdi,%rax,4)
	addq	$8, %rax
	addq	$8, %rdx
	movslq	4(%rcx), %rsi
	leaq	-7(%rsi), %rbx
	cmpq	%rbx, %rdx
	jl	.LBB28_14
# %bb.15:
	movslq	%esi, %rbx
	cmpq	%rbx, %rdx
	jge	.LBB28_17
	.p2align	4, 0x90
.LBB28_16:                              # =>This Inner Loop Header: Depth=1
	movl	(%r8,%rdx,4), %esi
	movl	%esi, (%rdi,%rax,4)
	incq	%rax
	incq	%rdx
	movslq	4(%rcx), %rsi
	cmpq	%rsi, %rdx
	jl	.LBB28_16
.LBB28_17:
	subl	%r9d, %esi
	movl	%esi, 4(%rcx)
	leaq	(%r9,%r9), %r10
	movslq	8(%rcx), %rdx
	movq	%rdx, %rsi
	addq	$-7, %rsi
	cmpq	%rsi, %r10
	jge	.LBB28_18
# %bb.19:
	movq	%r10, %rsi
	.p2align	4, 0x90
.LBB28_20:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r8,%rsi,4), %ymm0
	vmovdqu	%ymm0, (%rdi,%rax,4)
	addq	$8, %rax
	addq	$8, %rsi
	movslq	8(%rcx), %rdx
	leaq	-7(%rdx), %rbx
	cmpq	%rbx, %rsi
	jl	.LBB28_20
# %bb.21:
	movslq	%edx, %rbx
	cmpq	%rbx, %rsi
	jge	.LBB28_23
	.p2align	4, 0x90
.LBB28_22:                              # =>This Inner Loop Header: Depth=1
	movl	(%r8,%rsi,4), %edx
	movl	%edx, (%rdi,%rax,4)
	incq	%rax
	incq	%rsi
	movslq	8(%rcx), %rdx
	cmpq	%rdx, %rsi
	jl	.LBB28_22
.LBB28_23:
	subl	%r10d, %edx
	movl	%edx, 8(%rcx)
	leaq	(%r9,%r9,2), %r10
	movslq	12(%rcx), %rdx
	movq	%rdx, %rsi
	addq	$-7, %rsi
	cmpq	%rsi, %r10
	jge	.LBB28_24
# %bb.25:
	movq	%r10, %rsi
	.p2align	4, 0x90
.LBB28_26:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r8,%rsi,4), %ymm0
	vmovdqu	%ymm0, (%rdi,%rax,4)
	addq	$8, %rax
	addq	$8, %rsi
	movslq	12(%rcx), %rdx
	leaq	-7(%rdx), %rbx
	cmpq	%rbx, %rsi
	jl	.LBB28_26
# %bb.27:
	movslq	%edx, %rbx
	cmpq	%rbx, %rsi
	jge	.LBB28_29
	.p2align	4, 0x90
.LBB28_28:                              # =>This Inner Loop Header: Depth=1
	movl	(%r8,%rsi,4), %edx
	movl	%edx, (%rdi,%rax,4)
	incq	%rax
	incq	%rsi
	movslq	12(%rcx), %rdx
	cmpq	%rdx, %rsi
	jl	.LBB28_28
.LBB28_29:
	subl	%r10d, %edx
	movl	%edx, 12(%rcx)
	shlq	$2, %r9
	movslq	16(%rcx), %rdx
	movq	%rdx, %rsi
	addq	$-7, %rsi
	cmpq	%rsi, %r9
	jge	.LBB28_30
# %bb.31:
	movq	%r9, %rsi
	.p2align	4, 0x90
.LBB28_32:                              # =>This Inner Loop Header: Depth=1
	vmovdqu	(%r8,%rsi,4), %ymm0
	vmovdqu	%ymm0, (%rdi,%rax,4)
	addq	$8, %rax
	addq	$8, %rsi
	movslq	16(%rcx), %rdx
	leaq	-7(%rdx), %rbx
	cmpq	%rbx, %rsi
	jl	.LBB28_32
# %bb.33:
	movslq	%edx, %rbx
	cmpq	%rbx, %rsi
	jl	.LBB28_34
	jmp	.LBB28_36
.LBB28_10:
	movq	%r9, %rdx
	movslq	%esi, %rbx
	cmpq	%rbx, %rdx
	jl	.LBB28_16
	jmp	.LBB28_17
.LBB28_18:
	movq	%r10, %rsi
	movslq	%edx, %rbx
	cmpq	%rbx, %rsi
	jl	.LBB28_22
	jmp	.LBB28_23
.LBB28_24:
	movq	%r10, %rsi
	movslq	%edx, %rbx
	cmpq	%rbx, %rsi
	jl	.LBB28_28
	jmp	.LBB28_29
.LBB28_30:
	movq	%r9, %rsi
	movslq	%edx, %rbx
	cmpq	%rbx, %rsi
	jge	.LBB28_36
.LBB28_34:
	leaq	(%rdi,%rax,4), %rax
	.p2align	4, 0x90
.LBB28_35:                              # =>This Inner Loop Header: Depth=1
	movl	(%r8,%rsi,4), %edx
	movl	%edx, (%rax)
	incq	%rsi
	movslq	16(%rcx), %rdx
	addq	$4, %rax
	cmpq	%rdx, %rsi
	jl	.LBB28_35
.LBB28_36:
	subl	%r9d, %edx
	movl	%edx, 16(%rcx)
	popq	%rbx
	.cfi_def_cfa_offset 8
	vzeroupper
	retq
.Lfunc_end28:
	.size	partition_quick_multix4, .Lfunc_end28-partition_quick_multix4
	.cfi_endproc
                                        # -- End function
	.globl	hsum_epi32_avx          # -- Begin function hsum_epi32_avx
	.p2align	4, 0x90
	.type	hsum_epi32_avx,@function
hsum_epi32_avx:                         # @hsum_epi32_avx
	.cfi_startproc
# %bb.0:
	vpshufd	$238, %xmm0, %xmm1      # xmm1 = xmm0[2,3,2,3]
	vpaddd	%xmm0, %xmm1, %xmm0
	vpshufd	$229, %xmm0, %xmm1      # xmm1 = xmm0[1,1,2,3]
	vpaddd	%xmm0, %xmm1, %xmm0
	vmovd	%xmm0, %eax
	retq
.Lfunc_end29:
	.size	hsum_epi32_avx, .Lfunc_end29-hsum_epi32_avx
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_multix4_h    # -- Begin function sort_quick_multix4_h
	.p2align	4, 0x90
	.type	sort_quick_multix4_h,@function
sort_quick_multix4_h:                   # @sort_quick_multix4_h
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%r12
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$24, %rsp
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -40
	.cfi_offset %r12, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movl	%edx, %eax
	subl	%esi, %eax
	jle	.LBB30_3
# %bb.1:
	movl	%esi, %ebx
	movq	%rdi, %r15
	cmpl	$41, %eax
	jl	.LBB30_4
# %bb.2:
	movq	%rcx, %r14
	leal	1(%rax), %ecx
	movl	$0, (%rsp)
	movl	%ecx, 4(%rsp)
	leal	(%rax,%rax), %esi
	addl	$2, %esi
	movl	%esi, 8(%rsp)
	leal	(%rcx,%rcx,2), %ecx
	movl	%ecx, 12(%rsp)
	leal	4(,%rax,4), %eax
	movl	%eax, 16(%rsp)
	incl	%edx
	movq	%rsp, %rcx
	movq	%r15, %rdi
	movl	%ebx, %esi
	movq	%r14, %r8
	callq	partition_quick_multix4
	movl	(%rsp), %r12d
	leal	(%r12,%rbx), %edx
	decl	%edx
                                        # kill: def $r12d killed $r12d killed $r12 def $r12
	addl	%ebx, %r12d
	movq	%r15, %rdi
	movl	%ebx, %esi
	movq	%r14, %rcx
	callq	sort_quick_multix4_h
	movl	4(%rsp), %ebx
	leal	(%rbx,%r12), %edx
	decl	%edx
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	addl	%r12d, %ebx
	movq	%r15, %rdi
	movl	%r12d, %esi
	movq	%r14, %rcx
	callq	sort_quick_multix4_h
	movl	8(%rsp), %r12d
	leal	(%r12,%rbx), %edx
	decl	%edx
                                        # kill: def $r12d killed $r12d killed $r12 def $r12
	addl	%ebx, %r12d
	movq	%r15, %rdi
	movl	%ebx, %esi
	movq	%r14, %rcx
	callq	sort_quick_multix4_h
	movl	12(%rsp), %ebx
	leal	(%rbx,%r12), %edx
	decl	%edx
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	addl	%r12d, %ebx
	movq	%r15, %rdi
	movl	%r12d, %esi
	movq	%r14, %rcx
	callq	sort_quick_multix4_h
	movl	16(%rsp), %eax
	leal	(%rax,%rbx), %edx
	decl	%edx
	movq	%r15, %rdi
	movl	%ebx, %esi
	movq	%r14, %rcx
	callq	sort_quick_multix4_h
.LBB30_3:
	addq	$24, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.LBB30_4:
	.cfi_def_cfa_offset 64
	movq	%r15, %rdi
	movl	%ebx, %esi
	addq	$24, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	jmp	sort_quick_optimized    # TAILCALL
.Lfunc_end30:
	.size	sort_quick_multix4_h, .Lfunc_end30-sort_quick_multix4_h
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_multix4      # -- Begin function sort_quick_multix4
	.p2align	4, 0x90
	.type	sort_quick_multix4,@function
sort_quick_multix4:                     # @sort_quick_multix4
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %r15d
	movl	%esi, %ebp
	movq	%rdi, %r14
	movl	%edx, %eax
	subl	%esi, %eax
	cltq
	shlq	$2, %rax
	leaq	(%rax,%rax,4), %rdi
	callq	malloc
	movq	%rax, %rbx
	movq	%r14, %rdi
	movl	%ebp, %esi
	movl	%r15d, %edx
	movq	%rax, %rcx
	callq	sort_quick_multix4_h
	movq	%rbx, %rdi
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	free                    # TAILCALL
.Lfunc_end31:
	.size	sort_quick_multix4, .Lfunc_end31-sort_quick_multix4
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_simd    # -- Begin function partition_quick_simd
	.p2align	4, 0x90
	.type	partition_quick_simd,@function
partition_quick_simd:                   # @partition_quick_simd
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	andq	$-32, %rsp
	subq	$64, %rsp
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %r10d
	subl	%esi, %r10d
	cmpl	$1, %r10d
	jne	.LBB32_2
# %bb.1:
	movslq	%esi, %r9
	movslq	%edx, %r8
	movl	(%rdi,%r9,4), %ebx
	movl	(%rdi,%r8,4), %eax
	cmpl	%eax, %ebx
	movl	%eax, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, (%rdi,%r9,4)
	cmovll	%eax, %ebx
	movl	%ebx, (%rdi,%r8,4)
	addl	%esi, %edx
	movl	%edx, %eax
	shrl	$31, %eax
	addl	%edx, %eax
	sarl	%eax
	jmp	.LBB32_12
.LBB32_2:
	leal	(%rdx,%rsi), %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	sarl	%eax
	movslq	%esi, %rcx
	movl	(%rdi,%rcx,4), %r11d
	movslq	%eax, %r14
	movl	(%rdi,%r14,4), %r9d
	movslq	%edx, %r8
	movl	(%rdi,%r8,4), %r15d
	cmpl	%r9d, %r11d
	movl	%r9d, %ebx
	cmovll	%r11d, %ebx
	cmovll	%r9d, %r11d
	cmpl	%r15d, %r11d
	movl	%r15d, %r9d
	cmovll	%r11d, %r9d
	cmovll	%r15d, %r11d
	cmpl	%r9d, %ebx
	movl	%r9d, %r15d
	cmovll	%ebx, %r15d
	cmovgel	%ebx, %r9d
	movl	%r15d, (%rdi,%rcx,4)
	cmpl	$2, %r10d
	jne	.LBB32_4
# %bb.3:
	movl	%r9d, (%rdi,%r14,4)
	movl	%r11d, (%rdi,%r8,4)
	jmp	.LBB32_12
.LBB32_4:
	movl	%r9d, (%rdi,%r8,4)
	movl	%r11d, (%rdi,%r14,4)
	leal	-7(%rdx), %eax
	cmpl	%esi, %eax
	jle	.LBB32_5
# %bb.13:
	vmovd	%r9d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	movslq	%eax, %r10
	movq	%rcx, %r15
	.p2align	4, 0x90
.LBB32_14:                              # =>This Inner Loop Header: Depth=1
	vpcmpgtd	(%rdi,%r15,4), %ymm0, %ymm1
	vpsrld	$31, %ymm1, %ymm1
	vmovdqu	(%rdi,%r15,4), %xmm2
	vmovdqa	%ymm1, (%rsp)
	vmovd	%xmm1, %ebx
	movl	(%rdi,%rcx,4), %r11d
	vmovd	%xmm2, %esi
	testl	%ebx, %ebx
	movl	%r11d, %eax
	cmovnel	%esi, %eax
	movl	%eax, (%rdi,%rcx,4)
	cmovnel	%r11d, %esi
	movl	%esi, (%rdi,%r15,4)
	addq	%rcx, %rbx
	vpextrd	$1, %xmm1, %eax
	movl	(%rdi,%rbx,4), %r11d
	movl	4(%rdi,%r15,4), %esi
	testl	%eax, %eax
	movl	%r11d, %ecx
	cmovnel	%esi, %ecx
	movl	%ecx, (%rdi,%rbx,4)
	cmovnel	%r11d, %esi
	movl	%esi, 4(%rdi,%r15,4)
	vpextrd	$2, %xmm1, %ecx
	addq	%rbx, %rax
	movl	(%rdi,%rax,4), %r11d
	movl	8(%rdi,%r15,4), %ebx
	testl	%ecx, %ecx
	movl	%r11d, %esi
	cmovnel	%ebx, %esi
	movl	%esi, (%rdi,%rax,4)
	cmovnel	%r11d, %ebx
	movl	%ebx, 8(%rdi,%r15,4)
	addq	%rax, %rcx
	vpextrd	$3, %xmm1, %eax
	movl	(%rdi,%rcx,4), %r11d
	movl	12(%rdi,%r15,4), %ebx
	testl	%eax, %eax
	movl	%r11d, %esi
	cmovnel	%ebx, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovnel	%r11d, %ebx
	movl	%ebx, 12(%rdi,%r15,4)
	addq	%rcx, %rax
	vextracti128	$1, %ymm1, %xmm1
	vmovd	%xmm1, %ecx
	movl	(%rdi,%rax,4), %r11d
	movl	16(%rdi,%r15,4), %ebx
	testl	%ecx, %ecx
	movl	%r11d, %esi
	cmovnel	%ebx, %esi
	movl	%esi, (%rdi,%rax,4)
	cmovnel	%r11d, %ebx
	movl	%ebx, 16(%rdi,%r15,4)
	vpextrd	$1, %xmm1, %esi
	addq	%rax, %rcx
	movl	(%rdi,%rcx,4), %r11d
	movl	20(%rdi,%r15,4), %ebx
	testl	%esi, %esi
	movl	%r11d, %eax
	cmovnel	%ebx, %eax
	movl	%eax, (%rdi,%rcx,4)
	cmovnel	%r11d, %ebx
	movl	%ebx, 20(%rdi,%r15,4)
	addq	%rcx, %rsi
	vpextrd	$2, %xmm1, %ecx
	movl	(%rdi,%rsi,4), %r11d
	movl	24(%rdi,%r15,4), %ebx
	testl	%ecx, %ecx
	movl	%r11d, %eax
	cmovnel	%ebx, %eax
	movl	%eax, (%rdi,%rsi,4)
	cmovnel	%r11d, %ebx
	movl	%ebx, 24(%rdi,%r15,4)
	addq	%rsi, %rcx
	movslq	28(%rsp), %r11
	testq	%r11, %r11
	movl	(%rdi,%rcx,4), %eax
	movl	28(%rdi,%r15,4), %ebx
	movl	%eax, %esi
	cmovnel	%ebx, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovnel	%eax, %ebx
	movl	%ebx, 28(%rdi,%r15,4)
	addq	%r11, %rcx
	addq	$8, %r15
	cmpq	%r10, %r15
	jl	.LBB32_14
# %bb.6:
	cmpq	%r8, %r15
	jl	.LBB32_7
	jmp	.LBB32_11
.LBB32_5:
	movq	%rcx, %r15
	cmpq	%r8, %r15
	jge	.LBB32_11
.LBB32_7:
	subl	%r15d, %edx
	movq	%r15, %r10
	notq	%r10
	addq	%r8, %r10
	andq	$3, %rdx
	je	.LBB32_9
	.p2align	4, 0x90
.LBB32_8:                               # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%r15,4), %ebx
	xorl	%r14d, %r14d
	cmpl	%ebx, %r9d
	setg	%r14b
	movl	(%rdi,%rcx,4), %r11d
	movl	%r11d, %esi
	cmovgl	%ebx, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovgl	%r11d, %ebx
	movl	%ebx, (%rdi,%r15,4)
	addq	%r14, %rcx
	incq	%r15
	decq	%rdx
	jne	.LBB32_8
.LBB32_9:
	cmpq	$3, %r10
	jb	.LBB32_11
	.p2align	4, 0x90
.LBB32_10:                              # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%r15,4), %eax
	xorl	%edx, %edx
	cmpl	%eax, %r9d
	setg	%dl
	movl	(%rdi,%rcx,4), %ebx
	movl	%ebx, %esi
	cmovgl	%eax, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovgl	%ebx, %eax
	movl	%eax, (%rdi,%r15,4)
	addq	%rcx, %rdx
	movl	4(%rdi,%r15,4), %eax
	xorl	%ecx, %ecx
	cmpl	%eax, %r9d
	setg	%cl
	movl	(%rdi,%rdx,4), %ebx
	movl	%ebx, %esi
	cmovgl	%eax, %esi
	movl	%esi, (%rdi,%rdx,4)
	cmovgl	%ebx, %eax
	movl	%eax, 4(%rdi,%r15,4)
	addq	%rdx, %rcx
	movl	8(%rdi,%r15,4), %eax
	xorl	%edx, %edx
	cmpl	%eax, %r9d
	setg	%dl
	movl	(%rdi,%rcx,4), %ebx
	movl	%ebx, %esi
	cmovgl	%eax, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovgl	%ebx, %eax
	movl	%eax, 8(%rdi,%r15,4)
	addq	%rcx, %rdx
	movl	12(%rdi,%r15,4), %eax
	xorl	%ecx, %ecx
	cmpl	%eax, %r9d
	setg	%cl
	movl	(%rdi,%rdx,4), %ebx
	movl	%ebx, %esi
	cmovgl	%eax, %esi
	movl	%esi, (%rdi,%rdx,4)
	cmovgl	%ebx, %eax
	movl	%eax, 12(%rdi,%r15,4)
	addq	%rdx, %rcx
	addq	$4, %r15
	cmpq	%r15, %r8
	jne	.LBB32_10
.LBB32_11:
	movl	(%rdi,%rcx,4), %eax
	movl	(%rdi,%r8,4), %edx
	movl	%edx, (%rdi,%rcx,4)
	movl	%eax, (%rdi,%r8,4)
	movl	%ecx, %eax
.LBB32_12:
	leaq	-24(%rbp), %rsp
	popq	%rbx
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	vzeroupper
	retq
.Lfunc_end32:
	.size	partition_quick_simd, .Lfunc_end32-partition_quick_simd
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_simd         # -- Begin function sort_quick_simd
	.p2align	4, 0x90
	.type	sort_quick_simd,@function
sort_quick_simd:                        # @sort_quick_simd
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	%edx, %esi
	jge	.LBB33_3
# %bb.1:
	movl	%edx, %r14d
	movl	%esi, %ebp
	movq	%rdi, %r15
	.p2align	4, 0x90
.LBB33_2:                               # =>This Inner Loop Header: Depth=1
	movq	%r15, %rdi
	movl	%ebp, %esi
	movl	%r14d, %edx
	callq	partition_quick_simd
	movl	%eax, %ebx
	leal	-1(%rbx), %edx
	movq	%r15, %rdi
	movl	%ebp, %esi
	callq	sort_quick_simd
	incl	%ebx
	movl	%ebx, %ebp
	cmpl	%r14d, %ebx
	jl	.LBB33_2
.LBB33_3:
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end33:
	.size	sort_quick_simd, .Lfunc_end33-sort_quick_simd
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_optimized_swap_arith # -- Begin function partition_quick_optimized_swap_arith
	.p2align	4, 0x90
	.type	partition_quick_optimized_swap_arith,@function
partition_quick_optimized_swap_arith:   # @partition_quick_optimized_swap_arith
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %r11d
	subl	%esi, %r11d
	cmpl	$1, %r11d
	jne	.LBB34_2
# %bb.1:
	movslq	%esi, %r9
	movslq	%edx, %r8
	movl	(%rdi,%r9,4), %ebx
	movl	(%rdi,%r8,4), %eax
	cmpl	%eax, %ebx
	movl	%eax, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, (%rdi,%r9,4)
	cmovll	%eax, %ebx
	movl	%ebx, (%rdi,%r8,4)
	addl	%esi, %edx
	movl	%edx, %eax
	shrl	$31, %eax
	addl	%edx, %eax
	sarl	%eax
	jmp	.LBB34_14
.LBB34_2:
	leal	(%rdx,%rsi), %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	sarl	%eax
	movslq	%esi, %r9
	movl	(%rdi,%r9,4), %ecx
	movslq	%eax, %r14
	movl	(%rdi,%r14,4), %ebp
	movslq	%edx, %r8
	movl	(%rdi,%r8,4), %r15d
	cmpl	%ebp, %ecx
	movl	%ebp, %ebx
	cmovll	%ecx, %ebx
	cmovll	%ebp, %ecx
	cmpl	%r15d, %ecx
	movl	%r15d, %r10d
	cmovll	%ecx, %r10d
	cmovll	%r15d, %ecx
	cmpl	%r10d, %ebx
	movl	%r10d, %ebp
	cmovll	%ebx, %ebp
	cmovgel	%ebx, %r10d
	movl	%ebp, (%rdi,%r9,4)
	cmpl	$2, %r11d
	jne	.LBB34_4
# %bb.3:
	movl	%r10d, (%rdi,%r14,4)
	jmp	.LBB34_13
.LBB34_4:
	movl	%r10d, (%rdi,%r8,4)
	movl	%ecx, (%rdi,%r14,4)
	subl	%esi, %edx
	jle	.LBB34_5
# %bb.6:
	movq	%r9, %rcx
	notq	%rcx
	testb	$1, %dl
	jne	.LBB34_8
# %bb.7:
                                        # implicit-def: $rax
	movq	%r9, %rdx
	addq	%r8, %rcx
	jne	.LBB34_10
	jmp	.LBB34_12
.LBB34_5:
	movq	%r9, %rax
	jmp	.LBB34_12
.LBB34_8:
	xorl	%eax, %eax
	cmpl	(%rdi,%r9,4), %r10d
	setg	%al
	addq	%r9, %rax
	movq	%r9, %rdx
	incq	%rdx
	movq	%rax, %r9
	addq	%r8, %rcx
	je	.LBB34_12
.LBB34_10:
	movq	%r9, %rax
	.p2align	4, 0x90
.LBB34_11:                              # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rdx,4), %ebx
	xorl	%ecx, %ecx
	cmpl	%ebx, %r10d
	setg	%cl
	movl	(%rdi,%rax,4), %ebp
	movl	%ebx, %esi
	cmovgl	%ebp, %esi
	addl	%ebx, %ebp
	subl	%esi, %ebp
	movl	%esi, (%rdi,%rdx,4)
	movl	%ebp, (%rdi,%rax,4)
	addq	%rax, %rcx
	movl	4(%rdi,%rdx,4), %ebx
	xorl	%eax, %eax
	cmpl	%ebx, %r10d
	setg	%al
	movl	(%rdi,%rcx,4), %ebp
	movl	%ebx, %esi
	cmovgl	%ebp, %esi
	addl	%ebx, %ebp
	subl	%esi, %ebp
	movl	%esi, 4(%rdi,%rdx,4)
	movl	%ebp, (%rdi,%rcx,4)
	addq	%rcx, %rax
	addq	$2, %rdx
	cmpq	%rdx, %r8
	jne	.LBB34_11
.LBB34_12:
	movl	(%rdi,%rax,4), %ecx
	movl	(%rdi,%r8,4), %edx
	movl	%edx, (%rdi,%rax,4)
.LBB34_13:
	movl	%ecx, (%rdi,%r8,4)
.LBB34_14:
                                        # kill: def $eax killed $eax killed $rax
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end34:
	.size	partition_quick_optimized_swap_arith, .Lfunc_end34-partition_quick_optimized_swap_arith
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_optimized_swap_arith # -- Begin function sort_quick_optimized_swap_arith
	.p2align	4, 0x90
	.type	sort_quick_optimized_swap_arith,@function
sort_quick_optimized_swap_arith:        # @sort_quick_optimized_swap_arith
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	pushq	%rax
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $esi killed $esi def $rsi
	cmpl	%esi, %edx
	jle	.LBB35_13
# %bb.1:
	movl	%edx, %r14d
	movq	%rdi, %r13
	movslq	%edx, %r15
	movq	%r15, %r12
	negq	%r12
	jmp	.LBB35_2
	.p2align	4, 0x90
.LBB35_4:                               #   in Loop: Header=BB35_2 Depth=1
	movslq	%esi, %rax
	movl	(%r13,%rax,4), %ecx
	movl	(%r13,%r15,4), %edx
	cmpl	%edx, %ecx
	movl	%edx, %edi
	cmovll	%ecx, %edi
	movl	%edi, (%r13,%rax,4)
	cmovll	%edx, %ecx
	movl	%ecx, (%r13,%r15,4)
	leal	(%rsi,%r14), %eax
	movl	%eax, %ebx
	shrl	$31, %ebx
	addl	%eax, %ebx
	sarl	%ebx
.LBB35_12:                              #   in Loop: Header=BB35_2 Depth=1
	leal	-1(%rbx), %edx
	movq	%r13, %rdi
                                        # kill: def $esi killed $esi killed $rsi
	callq	sort_quick_optimized_swap_arith
	incl	%ebx
	movl	%ebx, %esi
	cmpl	%r14d, %ebx
	jge	.LBB35_13
.LBB35_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB35_14 Depth 2
	movl	%r14d, %edx
	subl	%esi, %edx
	jle	.LBB35_13
# %bb.3:                                #   in Loop: Header=BB35_2 Depth=1
	cmpl	$1, %edx
	je	.LBB35_4
# %bb.5:                                #   in Loop: Header=BB35_2 Depth=1
	leal	(%rsi,%r14), %eax
	movl	%eax, %ebx
	shrl	$31, %ebx
	addl	%eax, %ebx
	sarl	%ebx
	movslq	%esi, %rbp
	movl	(%r13,%rbp,4), %edi
	movslq	%ebx, %r8
	movl	(%r13,%r8,4), %eax
	movl	(%r13,%r15,4), %r9d
	cmpl	%eax, %edi
	movl	%eax, %r10d
	cmovll	%edi, %r10d
	cmovll	%eax, %edi
	cmpl	%r9d, %edi
	movl	%r9d, %r11d
	cmovll	%edi, %r11d
	cmovll	%r9d, %edi
	cmpl	%r11d, %r10d
	movl	%r11d, %r9d
	cmovll	%r10d, %r9d
	cmovgel	%r10d, %r11d
	movl	%r9d, (%r13,%rbp,4)
	cmpl	$2, %edx
	jne	.LBB35_7
# %bb.6:                                #   in Loop: Header=BB35_2 Depth=1
	movl	%r11d, (%r13,%r8,4)
	movl	%edi, (%r13,%r15,4)
	jmp	.LBB35_12
	.p2align	4, 0x90
.LBB35_7:                               #   in Loop: Header=BB35_2 Depth=1
	movl	%r11d, (%r13,%r15,4)
	movl	%edi, (%r13,%r8,4)
	movq	%rbp, %rdi
	notq	%rdi
	testb	$1, %dl
	jne	.LBB35_9
# %bb.8:                                #   in Loop: Header=BB35_2 Depth=1
                                        # implicit-def: $rbx
	movq	%rbp, %rdx
	cmpq	%r12, %rdi
	jne	.LBB35_14
	jmp	.LBB35_11
.LBB35_9:                               #   in Loop: Header=BB35_2 Depth=1
	xorl	%ebx, %ebx
	cmpl	(%r13,%rbp,4), %r11d
	setg	%bl
	addq	%rbp, %rbx
	movq	%rbp, %rdx
	incq	%rdx
	movq	%rbx, %rbp
	cmpq	%r12, %rdi
	je	.LBB35_11
	.p2align	4, 0x90
.LBB35_14:                              #   Parent Loop BB35_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r13,%rdx,4), %ecx
	xorl	%edi, %edi
	cmpl	%ecx, %r11d
	setg	%dil
	movl	(%r13,%rbp,4), %eax
	movl	%ecx, %ebx
	cmovgl	%eax, %ebx
	addl	%ecx, %eax
	subl	%ebx, %eax
	movl	%ebx, (%r13,%rdx,4)
	movl	%eax, (%r13,%rbp,4)
	addq	%rbp, %rdi
	movl	4(%r13,%rdx,4), %ebp
	xorl	%ebx, %ebx
	cmpl	%ebp, %r11d
	setg	%bl
	movl	(%r13,%rdi,4), %ecx
	movl	%ebp, %eax
	cmovgl	%ecx, %eax
	addl	%ebp, %ecx
	subl	%eax, %ecx
	movl	%eax, 4(%r13,%rdx,4)
	movl	%ecx, (%r13,%rdi,4)
	addq	%rdi, %rbx
	addq	$2, %rdx
	movq	%rbx, %rbp
	cmpq	%rdx, %r15
	jne	.LBB35_14
.LBB35_11:                              #   in Loop: Header=BB35_2 Depth=1
	movl	(%r13,%rbx,4), %eax
	movl	(%r13,%r15,4), %ecx
	movl	%ecx, (%r13,%rbx,4)
	movl	%eax, (%r13,%r15,4)
	jmp	.LBB35_12
.LBB35_13:
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end35:
	.size	sort_quick_optimized_swap_arith, .Lfunc_end35-sort_quick_optimized_swap_arith
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_optimized_swap_array # -- Begin function partition_quick_optimized_swap_array
	.p2align	4, 0x90
	.type	partition_quick_optimized_swap_array,@function
partition_quick_optimized_swap_array:   # @partition_quick_optimized_swap_array
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %r10d
	subl	%esi, %r10d
	cmpl	$1, %r10d
	jne	.LBB36_2
# %bb.1:
	movslq	%esi, %r9
	movslq	%edx, %r8
	movl	(%rdi,%r9,4), %ebx
	movl	(%rdi,%r8,4), %eax
	cmpl	%eax, %ebx
	movl	%eax, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, (%rdi,%r9,4)
	cmovll	%eax, %ebx
	movl	%ebx, (%rdi,%r8,4)
	addl	%esi, %edx
	movl	%edx, %eax
	shrl	$31, %eax
	addl	%edx, %eax
	sarl	%eax
	jmp	.LBB36_11
.LBB36_2:
	leal	(%rdx,%rsi), %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	sarl	%eax
	movslq	%esi, %rcx
	movl	(%rdi,%rcx,4), %r11d
	movslq	%eax, %r14
	movl	(%rdi,%r14,4), %ebp
	movslq	%edx, %r8
	movl	(%rdi,%r8,4), %r15d
	cmpl	%ebp, %r11d
	movl	%ebp, %ebx
	cmovll	%r11d, %ebx
	cmovll	%ebp, %r11d
	cmpl	%r15d, %r11d
	movl	%r15d, %r9d
	cmovll	%r11d, %r9d
	cmovll	%r15d, %r11d
	cmpl	%r9d, %ebx
	movl	%r9d, %ebp
	cmovll	%ebx, %ebp
	cmovgel	%ebx, %r9d
	movl	%ebp, (%rdi,%rcx,4)
	cmpl	$2, %r10d
	jne	.LBB36_4
# %bb.3:
	movl	%r9d, (%rdi,%r14,4)
	movl	%r11d, (%rdi,%r8,4)
	jmp	.LBB36_11
.LBB36_4:
	movl	%r9d, (%rdi,%r8,4)
	movl	%r11d, (%rdi,%r14,4)
	subl	%esi, %edx
	jle	.LBB36_5
# %bb.6:
	movq	%rcx, %rsi
	notq	%rsi
	testb	$1, %dl
	jne	.LBB36_8
# %bb.7:
	movq	%rcx, %rax
	addq	%r8, %rsi
	jne	.LBB36_12
	jmp	.LBB36_10
.LBB36_5:
	movq	%rcx, %rax
	jmp	.LBB36_10
.LBB36_8:
	movl	(%rdi,%rcx,4), %edx
	xorl	%eax, %eax
	xorl	%ebp, %ebp
	cmpl	%edx, %r9d
	setg	%al
	setle	%bpl
	movl	%edx, x(%rip)
	movl	(%rdi,%rcx,4), %edx
	movl	%edx, x+4(%rip)
	movl	x(,%rbp,4), %edx
	movl	%edx, (%rdi,%rcx,4)
	movl	x(,%rax,4), %edx
	movl	%edx, (%rdi,%rcx,4)
	addq	%rcx, %rax
	incq	%rcx
	addq	%r8, %rsi
	je	.LBB36_10
	.p2align	4, 0x90
.LBB36_12:                              # =>This Inner Loop Header: Depth=1
	xorl	%edx, %edx
	xorl	%esi, %esi
	cmpl	(%rdi,%rcx,4), %r9d
	setg	%dl
	setle	%sil
	movl	(%rdi,%rax,4), %ebp
	movl	%ebp, x(%rip)
	movl	(%rdi,%rcx,4), %ebp
	movl	%ebp, x+4(%rip)
	movl	x(,%rsi,4), %esi
	movl	%esi, (%rdi,%rcx,4)
	movl	x(,%rdx,4), %esi
	movl	%esi, (%rdi,%rax,4)
	addq	%rdx, %rax
	xorl	%edx, %edx
	xorl	%esi, %esi
	cmpl	4(%rdi,%rcx,4), %r9d
	setg	%dl
	setle	%sil
	movl	(%rdi,%rax,4), %ebp
	movl	%ebp, x(%rip)
	movl	4(%rdi,%rcx,4), %ebp
	movl	%ebp, x+4(%rip)
	movl	x(,%rsi,4), %esi
	movl	%esi, 4(%rdi,%rcx,4)
	movl	x(,%rdx,4), %esi
	movl	%esi, (%rdi,%rax,4)
	addq	%rdx, %rax
	addq	$2, %rcx
	cmpq	%rcx, %r8
	jne	.LBB36_12
.LBB36_10:
	movl	(%rdi,%rax,4), %ecx
	movl	(%rdi,%r8,4), %edx
	movl	%edx, (%rdi,%rax,4)
	movl	%ecx, (%rdi,%r8,4)
.LBB36_11:
                                        # kill: def $eax killed $eax killed $rax
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end36:
	.size	partition_quick_optimized_swap_array, .Lfunc_end36-partition_quick_optimized_swap_array
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_optimized_swap_array # -- Begin function sort_quick_optimized_swap_array
	.p2align	4, 0x90
	.type	sort_quick_optimized_swap_array,@function
sort_quick_optimized_swap_array:        # @sort_quick_optimized_swap_array
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpl	%esi, %edx
	jle	.LBB37_3
# %bb.1:
	movl	%edx, %r14d
	movl	%esi, %ebp
	movq	%rdi, %r15
	.p2align	4, 0x90
.LBB37_2:                               # =>This Inner Loop Header: Depth=1
	movq	%r15, %rdi
	movl	%ebp, %esi
	movl	%r14d, %edx
	callq	partition_quick_optimized_swap_array
	movl	%eax, %ebx
	leal	-1(%rbx), %edx
	movq	%r15, %rdi
	movl	%ebp, %esi
	callq	sort_quick_optimized_swap_array
	incl	%ebx
	movl	%ebx, %ebp
	cmpl	%r14d, %ebx
	jl	.LBB37_2
.LBB37_3:
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end37:
	.size	sort_quick_optimized_swap_array, .Lfunc_end37-sort_quick_optimized_swap_array
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_optimized_swap_asm # -- Begin function partition_quick_optimized_swap_asm
	.p2align	4, 0x90
	.type	partition_quick_optimized_swap_asm,@function
partition_quick_optimized_swap_asm:     # @partition_quick_optimized_swap_asm
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r12
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -32
	.cfi_offset %r12, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %rax
	movl	%edx, %r10d
	subl	%eax, %r10d
	cmpl	$1, %r10d
	jne	.LBB38_2
# %bb.1:
	movslq	%eax, %r9
	movslq	%edx, %r8
	movl	(%rdi,%r9,4), %ebx
	movl	(%rdi,%r8,4), %ecx
	cmpl	%ecx, %ebx
	movl	%ecx, %esi
	cmovll	%ebx, %esi
	movl	%esi, (%rdi,%r9,4)
	cmovll	%ecx, %ebx
	movl	%ebx, (%rdi,%r8,4)
	jmp	.LBB38_4
.LBB38_2:
	leal	(%rdx,%rax), %ebx
	movl	%ebx, %esi
	shrl	$31, %esi
	addl	%ebx, %esi
	sarl	%esi
	movslq	%eax, %r11
	movl	(%rdi,%r11,4), %ecx
	movslq	%esi, %r8
	movl	(%rdi,%r8,4), %ebx
	movslq	%edx, %r9
	movl	(%rdi,%r9,4), %ebp
	cmpl	%ebx, %ecx
	movl	%ebx, %esi
	cmovll	%ecx, %esi
	cmovll	%ebx, %ecx
	cmpl	%ebp, %ecx
	movl	%ebp, %ebx
	cmovll	%ecx, %ebx
	cmovll	%ebp, %ecx
	cmpl	%ebx, %esi
	movl	%ebx, %ebp
	cmovll	%esi, %ebp
	cmovgel	%esi, %ebx
	movl	%ebp, (%rdi,%r11,4)
	cmpl	$2, %r10d
	jne	.LBB38_6
# %bb.3:
	movl	%ebx, (%rdi,%r8,4)
	movl	%ecx, (%rdi,%r9,4)
.LBB38_4:
	addq	%rax, %rdx
	movq	%rdx, %rax
	shrq	$63, %rax
	addq	%rdx, %rax
	sarq	%rax
                                        # kill: def $eax killed $eax killed $rax
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB38_6:
	.cfi_def_cfa_offset 32
	movl	%ebx, (%rdi,%r9,4)
	movl	%ecx, (%rdi,%r8,4)
	#APP
	movq	%rax, %r12
.Ltmp0:

	movl	(%rdi,%r12,4), %r8d
	movl	(%rdi,%rax,4), %r9d
	xorq	%r10, %r10
	cmpl	%ebx, %r8d
	setl	%r10b
	movl	%r8d, %r11d
	cmovll	%r9d, %r11d
	movl	%r11d, (%rdi,%r12,4)
	cmovll	%r8d, %r9d
	movl	%r9d, (%rdi,%rax,4)
	addq	%r10, %rax

	incq	%r12
	cmpq	%r12, %rdx
	jne	.Ltmp0

	movl	(%rdi,%rax,4), %r9d
	movl	(%rdi,%rdx,4), %r8d
	movl	%r9d, (%rdi,%rdx,4)
	movl	%r8d, (%rdi,%rax,4)

	#NO_APP
                                        # kill: def $eax killed $eax killed $rax
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end38:
	.size	partition_quick_optimized_swap_asm, .Lfunc_end38-partition_quick_optimized_swap_asm
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_optimized_swap_asm # -- Begin function sort_quick_optimized_swap_asm
	.p2align	4, 0x90
	.type	sort_quick_optimized_swap_asm,@function
sort_quick_optimized_swap_asm:          # @sort_quick_optimized_swap_asm
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	pushq	%rax
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	cmpq	%rsi, %rdx
	jle	.LBB39_28
# %bb.1:
	movq	%rdx, %r14
	movq	%rdi, %rbx
	movabsq	$-4294967296, %rbp      # imm = 0xFFFFFFFF00000000
	movabsq	$4294967296, %r12       # imm = 0x100000000
	movslq	%r14d, %r15
	jmp	.LBB39_2
	.p2align	4, 0x90
.LBB39_4:                               #   in Loop: Header=BB39_2 Depth=1
	movslq	%esi, %rcx
	movl	(%rbx,%rcx,4), %eax
	movl	(%rbx,%r15,4), %edx
	cmpl	%edx, %eax
	movl	%edx, %edi
	cmovll	%eax, %edi
	movl	%edi, (%rbx,%rcx,4)
	cmovll	%edx, %eax
.LBB39_7:                               #   in Loop: Header=BB39_2 Depth=1
	movl	%eax, (%rbx,%r15,4)
	leaq	(%rsi,%r14), %rax
	movq	%rax, %r13
	shrq	$63, %r13
	addq	%rax, %r13
	sarq	%r13
.LBB39_9:                               #   in Loop: Header=BB39_2 Depth=1
	shlq	$32, %r13
	movq	%r13, %rdx
	addq	%rbp, %rdx
	sarq	$32, %rdx
	movq	%rbx, %rdi
	callq	sort_quick_optimized_swap_asm
	addq	%r12, %r13
	sarq	$32, %r13
	movq	%r13, %rsi
	cmpq	%r14, %r13
	jge	.LBB39_28
.LBB39_2:                               # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	subq	%rsi, %rdi
	jle	.LBB39_10
# %bb.3:                                #   in Loop: Header=BB39_2 Depth=1
	movl	%r14d, %ecx
	subl	%esi, %ecx
	cmpl	$1, %ecx
	je	.LBB39_4
# %bb.5:                                #   in Loop: Header=BB39_2 Depth=1
	leal	(%rsi,%r14), %eax
	movl	%eax, %edx
	shrl	$31, %edx
	addl	%eax, %edx
	sarl	%edx
	movslq	%esi, %r9
	movl	(%rbx,%r9,4), %eax
	movslq	%edx, %r8
	movl	(%rbx,%r8,4), %edi
	movl	(%rbx,%r15,4), %edx
	cmpl	%edi, %eax
	movl	%edi, %ebp
	cmovll	%eax, %ebp
	cmovll	%edi, %eax
	cmpl	%edx, %eax
	movl	%edx, %edi
	cmovll	%eax, %edi
	cmovll	%edx, %eax
	cmpl	%edi, %ebp
	movl	%edi, %edx
	cmovll	%ebp, %edx
	cmovgel	%ebp, %edi
	movl	%edx, (%rbx,%r9,4)
	cmpl	$2, %ecx
	jne	.LBB39_8
# %bb.6:                                #   in Loop: Header=BB39_2 Depth=1
	movl	%edi, (%rbx,%r8,4)
	movabsq	$-4294967296, %rbp      # imm = 0xFFFFFFFF00000000
	jmp	.LBB39_7
	.p2align	4, 0x90
.LBB39_8:                               #   in Loop: Header=BB39_2 Depth=1
	movl	%edi, (%rbx,%r15,4)
	movl	%eax, (%rbx,%r8,4)
	movq	%rsi, %r13
	movq	%r12, %rax
	#APP
	movq	%r13, %r12
.Ltmp1:

	movl	(%rbx,%r12,4), %r8d
	movl	(%rbx,%r13,4), %r9d
	xorq	%r10, %r10
	cmpl	%edi, %r8d
	setl	%r10b
	movl	%r8d, %r11d
	cmovll	%r9d, %r11d
	movl	%r11d, (%rbx,%r12,4)
	cmovll	%r8d, %r9d
	movl	%r9d, (%rbx,%r13,4)
	addq	%r10, %r13

	incq	%r12
	cmpq	%r12, %r14
	jne	.Ltmp1

	movl	(%rbx,%r13,4), %r9d
	movl	(%rbx,%r14,4), %r8d
	movl	%r9d, (%rbx,%r14,4)
	movl	%r8d, (%rbx,%r13,4)

	#NO_APP
	movq	%rax, %r12
	movabsq	$-4294967296, %rbp      # imm = 0xFFFFFFFF00000000
	jmp	.LBB39_9
.LBB39_10:
	incl	%edi
	cmpl	$2, %edi
	jl	.LBB39_28
# %bb.11:
	leaq	(%rbx,%rsi,4), %rax
	movl	%edi, %r10d
	decq	%r10
	movl	%r10d, %r8d
	andl	$1, %r8d
	movl	$1, %ecx
	cmpl	$2, %edi
	jne	.LBB39_12
.LBB39_22:
	testq	%r8, %r8
	je	.LBB39_28
# %bb.23:
	movl	(%rax,%rcx,4), %edx
	movq	%rcx, %rsi
	shlq	$32, %rsi
	addq	%rbp, %rsi
	.p2align	4, 0x90
.LBB39_24:                              # =>This Inner Loop Header: Depth=1
	movq	%rsi, %rdi
	sarq	$30, %rdi
	movl	(%rax,%rdi), %edi
	cmpl	%edx, %edi
	jle	.LBB39_27
# %bb.25:                               #   in Loop: Header=BB39_24 Depth=1
	movl	%edi, (%rax,%rcx,4)
	addq	%rbp, %rsi
	cmpq	$1, %rcx
	leaq	-1(%rcx), %rcx
	jg	.LBB39_24
# %bb.26:
	xorl	%ecx, %ecx
.LBB39_27:
	movslq	%ecx, %rcx
	movl	%edx, (%rax,%rcx,4)
.LBB39_28:
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB39_12:
	.cfi_def_cfa_offset 64
	movabsq	$8589934592, %r9        # imm = 0x200000000
	subq	%r8, %r10
	movl	$1, %ecx
	xorl	%r11d, %r11d
	jmp	.LBB39_13
	.p2align	4, 0x90
.LBB39_21:                              #   in Loop: Header=BB39_13 Depth=1
	movslq	%esi, %rdx
	movl	%ebx, (%rax,%rdx,4)
	addq	$2, %rcx
	addq	%r9, %r11
	addq	%r9, %r12
	addq	$-2, %r10
	je	.LBB39_22
.LBB39_13:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB39_14 Depth 2
                                        #     Child Loop BB39_18 Depth 2
	movl	(%rax,%rcx,4), %ebx
	movq	%r11, %rdi
	movq	%rcx, %rsi
	.p2align	4, 0x90
.LBB39_14:                              #   Parent Loop BB39_13 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rdi, %rdx
	sarq	$30, %rdx
	movl	(%rax,%rdx), %edx
	cmpl	%ebx, %edx
	jle	.LBB39_17
# %bb.15:                               #   in Loop: Header=BB39_14 Depth=2
	movl	%edx, (%rax,%rsi,4)
	addq	%rbp, %rdi
	cmpq	$1, %rsi
	leaq	-1(%rsi), %rsi
	jg	.LBB39_14
# %bb.16:                               #   in Loop: Header=BB39_13 Depth=1
	xorl	%esi, %esi
.LBB39_17:                              #   in Loop: Header=BB39_13 Depth=1
	movslq	%esi, %rdx
	movl	%ebx, (%rax,%rdx,4)
	leaq	1(%rcx), %rsi
	movl	4(%rax,%rcx,4), %ebx
	movq	%r12, %rdi
	.p2align	4, 0x90
.LBB39_18:                              #   Parent Loop BB39_13 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rdi, %rdx
	sarq	$30, %rdx
	movl	(%rax,%rdx), %edx
	cmpl	%ebx, %edx
	jle	.LBB39_21
# %bb.19:                               #   in Loop: Header=BB39_18 Depth=2
	movl	%edx, (%rax,%rsi,4)
	addq	%rbp, %rdi
	cmpq	$1, %rsi
	leaq	-1(%rsi), %rsi
	jg	.LBB39_18
# %bb.20:                               #   in Loop: Header=BB39_13 Depth=1
	xorl	%esi, %esi
	jmp	.LBB39_21
.Lfunc_end39:
	.size	sort_quick_optimized_swap_asm, .Lfunc_end39-sort_quick_optimized_swap_asm
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_optimized_swap_cmov # -- Begin function partition_quick_optimized_swap_cmov
	.p2align	4, 0x90
	.type	partition_quick_optimized_swap_cmov,@function
partition_quick_optimized_swap_cmov:    # @partition_quick_optimized_swap_cmov
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %r10d
	subl	%esi, %r10d
	cmpl	$1, %r10d
	jne	.LBB40_2
# %bb.1:
	movslq	%esi, %r9
	movslq	%edx, %r8
	movl	(%rdi,%r9,4), %ebx
	movl	(%rdi,%r8,4), %eax
	cmpl	%eax, %ebx
	movl	%eax, %ecx
	cmovll	%ebx, %ecx
	movl	%ecx, (%rdi,%r9,4)
	cmovll	%eax, %ebx
	movl	%ebx, (%rdi,%r8,4)
	addl	%esi, %edx
	movl	%edx, %eax
	shrl	$31, %eax
	addl	%edx, %eax
	sarl	%eax
	jmp	.LBB40_9
.LBB40_2:
	leal	(%rdx,%rsi), %ecx
	movl	%ecx, %eax
	shrl	$31, %eax
	addl	%ecx, %eax
	sarl	%eax
	movslq	%esi, %rcx
	movl	(%rdi,%rcx,4), %r11d
	movslq	%eax, %r14
	movl	(%rdi,%r14,4), %ebp
	movslq	%edx, %r8
	movl	(%rdi,%r8,4), %r15d
	cmpl	%ebp, %r11d
	movl	%ebp, %ebx
	cmovll	%r11d, %ebx
	cmovll	%ebp, %r11d
	cmpl	%r15d, %r11d
	movl	%r15d, %r9d
	cmovll	%r11d, %r9d
	cmovll	%r15d, %r11d
	cmpl	%r9d, %ebx
	movl	%r9d, %ebp
	cmovll	%ebx, %ebp
	cmovgel	%ebx, %r9d
	movl	%ebp, (%rdi,%rcx,4)
	cmpl	$2, %r10d
	jne	.LBB40_4
# %bb.3:
	movl	%r9d, (%rdi,%r14,4)
	movl	%r11d, (%rdi,%r8,4)
	jmp	.LBB40_9
.LBB40_4:
	movl	%r9d, (%rdi,%r8,4)
	movl	%r11d, (%rdi,%r14,4)
	subl	%esi, %edx
	jle	.LBB40_8
# %bb.5:
	movq	%rcx, %r10
	notq	%r10
	addq	%r8, %r10
	movq	%rcx, %rax
	andq	$3, %rdx
	je	.LBB40_7
	.p2align	4, 0x90
.LBB40_6:                               # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rax,4), %ebp
	xorl	%ebx, %ebx
	cmpl	%ebp, %r9d
	setg	%bl
	movl	(%rdi,%rcx,4), %r11d
	movl	%r11d, %esi
	cmovgl	%ebp, %esi
	movl	%esi, (%rdi,%rcx,4)
	cmovgl	%r11d, %ebp
	movl	%ebp, (%rdi,%rax,4)
	addq	%rbx, %rcx
	incq	%rax
	decq	%rdx
	jne	.LBB40_6
.LBB40_7:
	cmpq	$3, %r10
	jb	.LBB40_8
	.p2align	4, 0x90
.LBB40_10:                              # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rax,4), %edx
	xorl	%esi, %esi
	cmpl	%edx, %r9d
	setg	%sil
	movl	(%rdi,%rcx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%edx, %ebp
	movl	%ebp, (%rdi,%rcx,4)
	cmovgl	%ebx, %edx
	movl	%edx, (%rdi,%rax,4)
	addq	%rcx, %rsi
	movl	4(%rdi,%rax,4), %ecx
	xorl	%edx, %edx
	cmpl	%ecx, %r9d
	setg	%dl
	movl	(%rdi,%rsi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%ecx, %ebp
	movl	%ebp, (%rdi,%rsi,4)
	cmovgl	%ebx, %ecx
	movl	%ecx, 4(%rdi,%rax,4)
	addq	%rsi, %rdx
	movl	8(%rdi,%rax,4), %ecx
	xorl	%esi, %esi
	cmpl	%ecx, %r9d
	setg	%sil
	movl	(%rdi,%rdx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%ecx, %ebp
	movl	%ebp, (%rdi,%rdx,4)
	cmovgl	%ebx, %ecx
	movl	%ecx, 8(%rdi,%rax,4)
	addq	%rdx, %rsi
	movl	12(%rdi,%rax,4), %edx
	xorl	%ecx, %ecx
	cmpl	%edx, %r9d
	setg	%cl
	movl	(%rdi,%rsi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%edx, %ebp
	movl	%ebp, (%rdi,%rsi,4)
	cmovgl	%ebx, %edx
	movl	%edx, 12(%rdi,%rax,4)
	addq	%rsi, %rcx
	addq	$4, %rax
	cmpq	%rax, %r8
	jne	.LBB40_10
.LBB40_8:
	movl	(%rdi,%rcx,4), %eax
	movl	(%rdi,%r8,4), %edx
	movl	%edx, (%rdi,%rcx,4)
	movl	%eax, (%rdi,%r8,4)
	movl	%ecx, %eax
.LBB40_9:
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end40:
	.size	partition_quick_optimized_swap_cmov, .Lfunc_end40-partition_quick_optimized_swap_cmov
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_optimized_swap_cmov # -- Begin function sort_quick_optimized_swap_cmov
	.p2align	4, 0x90
	.type	sort_quick_optimized_swap_cmov,@function
sort_quick_optimized_swap_cmov:         # @sort_quick_optimized_swap_cmov
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	pushq	%rax
	.cfi_def_cfa_offset 64
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $esi killed $esi def $rsi
	cmpl	%esi, %edx
	jle	.LBB41_12
# %bb.1:
	movl	%edx, %ebx
	movq	%rdi, %r13
	movslq	%edx, %r15
	movq	%rbx, (%rsp)            # 8-byte Spill
	jmp	.LBB41_2
	.p2align	4, 0x90
.LBB41_4:                               #   in Loop: Header=BB41_2 Depth=1
	movslq	%esi, %rax
	movl	(%r13,%rax,4), %ecx
	movl	(%r13,%r15,4), %edx
	cmpl	%edx, %ecx
	movl	%edx, %edi
	cmovll	%ecx, %edi
	movl	%edi, (%r13,%rax,4)
	cmovll	%edx, %ecx
	movl	%ecx, (%r13,%r15,4)
	leal	(%rsi,%rbx), %eax
	movl	%eax, %r12d
	shrl	$31, %r12d
	addl	%eax, %r12d
	sarl	%r12d
.LBB41_11:                              #   in Loop: Header=BB41_2 Depth=1
	leal	-1(%r12), %edx
	movq	%r13, %rdi
                                        # kill: def $esi killed $esi killed $rsi
	callq	sort_quick_optimized_swap_cmov
	incl	%r12d
	movl	%r12d, %esi
	cmpl	%ebx, %r12d
	jge	.LBB41_12
.LBB41_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB41_8 Depth 2
                                        #     Child Loop BB41_13 Depth 2
	movl	%ebx, %r14d
	subl	%esi, %r14d
	jle	.LBB41_12
# %bb.3:                                #   in Loop: Header=BB41_2 Depth=1
	cmpl	$1, %r14d
	je	.LBB41_4
# %bb.5:                                #   in Loop: Header=BB41_2 Depth=1
	leal	(%rsi,%rbx), %eax
	movl	%eax, %r12d
	shrl	$31, %r12d
	addl	%eax, %r12d
	sarl	%r12d
	movslq	%esi, %rcx
	movl	(%r13,%rcx,4), %edx
	movslq	%r12d, %r8
	movl	(%r13,%r8,4), %eax
	movl	(%r13,%r15,4), %r9d
	cmpl	%eax, %edx
	movl	%eax, %r10d
	cmovll	%edx, %r10d
	cmovll	%eax, %edx
	cmpl	%r9d, %edx
	movl	%r9d, %r11d
	cmovll	%edx, %r11d
	cmovll	%r9d, %edx
	cmpl	%r11d, %r10d
	movl	%r11d, %r9d
	cmovll	%r10d, %r9d
	cmovgel	%r10d, %r11d
	movl	%r9d, (%r13,%rcx,4)
	cmpl	$2, %r14d
	jne	.LBB41_7
# %bb.6:                                #   in Loop: Header=BB41_2 Depth=1
	movl	%r11d, (%r13,%r8,4)
	movl	%edx, (%r13,%r15,4)
	jmp	.LBB41_11
	.p2align	4, 0x90
.LBB41_7:                               #   in Loop: Header=BB41_2 Depth=1
	movl	%r11d, (%r13,%r15,4)
	movl	%edx, (%r13,%r8,4)
	movq	%rcx, %r8
	notq	%r8
	addq	%r15, %r8
	movq	%rcx, %rdx
	andq	$3, %r14
	je	.LBB41_9
	.p2align	4, 0x90
.LBB41_8:                               #   Parent Loop BB41_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r13,%rdx,4), %eax
	xorl	%ebx, %ebx
	cmpl	%eax, %r11d
	setg	%bl
	movl	(%r13,%rcx,4), %ebp
	movl	%ebp, %edi
	cmovgl	%eax, %edi
	movl	%edi, (%r13,%rcx,4)
	cmovgl	%ebp, %eax
	movl	%eax, (%r13,%rdx,4)
	addq	%rbx, %rcx
	incq	%rdx
	decq	%r14
	jne	.LBB41_8
.LBB41_9:                               #   in Loop: Header=BB41_2 Depth=1
	cmpq	$3, %r8
	jb	.LBB41_10
	.p2align	4, 0x90
.LBB41_13:                              #   Parent Loop BB41_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%r13,%rdx,4), %eax
	xorl	%edi, %edi
	cmpl	%eax, %r11d
	setg	%dil
	movl	(%r13,%rcx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rcx,4)
	cmovgl	%ebx, %eax
	movl	%eax, (%r13,%rdx,4)
	addq	%rcx, %rdi
	movl	4(%r13,%rdx,4), %eax
	xorl	%ecx, %ecx
	cmpl	%eax, %r11d
	setg	%cl
	movl	(%r13,%rdi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rdi,4)
	cmovgl	%ebx, %eax
	movl	%eax, 4(%r13,%rdx,4)
	addq	%rdi, %rcx
	movl	8(%r13,%rdx,4), %eax
	xorl	%edi, %edi
	cmpl	%eax, %r11d
	setg	%dil
	movl	(%r13,%rcx,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rcx,4)
	cmovgl	%ebx, %eax
	movl	%eax, 8(%r13,%rdx,4)
	addq	%rcx, %rdi
	movl	12(%r13,%rdx,4), %eax
	xorl	%ecx, %ecx
	cmpl	%eax, %r11d
	setg	%cl
	movl	(%r13,%rdi,4), %ebx
	movl	%ebx, %ebp
	cmovgl	%eax, %ebp
	movl	%ebp, (%r13,%rdi,4)
	cmovgl	%ebx, %eax
	movl	%eax, 12(%r13,%rdx,4)
	addq	%rdi, %rcx
	addq	$4, %rdx
	cmpq	%rdx, %r15
	jne	.LBB41_13
.LBB41_10:                              #   in Loop: Header=BB41_2 Depth=1
	movl	(%r13,%rcx,4), %eax
	movl	(%r13,%r15,4), %edx
	movl	%edx, (%r13,%rcx,4)
	movl	%eax, (%r13,%r15,4)
	movl	%ecx, %r12d
	movq	(%rsp), %rbx            # 8-byte Reload
	jmp	.LBB41_11
.LBB41_12:
	addq	$8, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end41:
	.size	sort_quick_optimized_swap_cmov, .Lfunc_end41-sort_quick_optimized_swap_cmov
	.cfi_endproc
                                        # -- End function
	.globl	partition_quick_standard # -- Begin function partition_quick_standard
	.p2align	4, 0x90
	.type	partition_quick_standard,@function
partition_quick_standard:               # @partition_quick_standard
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	.cfi_offset %rbx, -24
	.cfi_offset %rbp, -16
                                        # kill: def $edx killed $edx def $rdx
                                        # kill: def $esi killed $esi def $rsi
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$2, %eax
	jl	.LBB42_2
# %bb.1:
	leal	(%rdx,%rsi), %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	movslq	%esi, %r8
	movl	(%rdi,%r8,4), %eax
	movslq	%ecx, %r10
	movl	(%rdi,%r10,4), %r9d
	movslq	%edx, %r11
	movl	(%rdi,%r11,4), %ecx
	cmpl	%r9d, %eax
	movl	%r9d, %ebx
	cmovlel	%eax, %ebx
	movl	%r9d, %ebp
	cmovgel	%eax, %ebp
	cmpl	%ecx, %ebx
	cmovgl	%ecx, %ebx
	cmpl	%ecx, %ebp
	cmovll	%ecx, %ebp
	addl	%eax, %r9d
	addl	%ecx, %r9d
	subl	%ebx, %r9d
	subl	%ebp, %r9d
	movl	%ebx, (%rdi,%r8,4)
	movl	%r9d, (%rdi,%r11,4)
	movl	%ebp, (%rdi,%r10,4)
	leal	-1(%rsi), %eax
	movl	%edx, %ecx
	subl	%esi, %ecx
	jg	.LBB42_5
.LBB42_4:
	movslq	%edx, %r10
	jmp	.LBB42_19
.LBB42_2:
	movslq	%edx, %rax
	movl	(%rdi,%rax,4), %r9d
	leal	-1(%rsi), %eax
	movl	%edx, %ecx
	subl	%esi, %ecx
	jle	.LBB42_4
.LBB42_5:
	movslq	%esi, %rsi
	movslq	%edx, %r10
	movq	%rsi, %r8
	notq	%r8
	addq	%r10, %r8
	andq	$3, %rcx
	jne	.LBB42_6
.LBB42_9:
	cmpq	$3, %r8
	jae	.LBB42_10
.LBB42_19:
	movslq	%eax, %rcx
	incl	%eax
	movl	4(%rdi,%rcx,4), %edx
	movl	(%rdi,%r10,4), %esi
	movl	%esi, 4(%rdi,%rcx,4)
	movl	%edx, (%rdi,%r10,4)
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
	.p2align	4, 0x90
.LBB42_8:                               #   in Loop: Header=BB42_6 Depth=1
	.cfi_def_cfa_offset 24
	incq	%rsi
	decq	%rcx
	je	.LBB42_9
.LBB42_6:                               # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rsi,4), %edx
	cmpl	%r9d, %edx
	jg	.LBB42_8
# %bb.7:                                #   in Loop: Header=BB42_6 Depth=1
	movslq	%eax, %rbp
	incl	%eax
	movl	4(%rdi,%rbp,4), %ebx
	movl	%edx, 4(%rdi,%rbp,4)
	movl	%ebx, (%rdi,%rsi,4)
	jmp	.LBB42_8
	.p2align	4, 0x90
.LBB42_18:                              #   in Loop: Header=BB42_10 Depth=1
	addq	$4, %rsi
	cmpq	%rsi, %r10
	je	.LBB42_19
.LBB42_10:                              # =>This Inner Loop Header: Depth=1
	movl	(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jle	.LBB42_11
# %bb.12:                               #   in Loop: Header=BB42_10 Depth=1
	movl	4(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jle	.LBB42_13
.LBB42_14:                              #   in Loop: Header=BB42_10 Depth=1
	movl	8(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jle	.LBB42_15
.LBB42_16:                              #   in Loop: Header=BB42_10 Depth=1
	movl	12(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB42_18
	jmp	.LBB42_17
	.p2align	4, 0x90
.LBB42_11:                              #   in Loop: Header=BB42_10 Depth=1
	movslq	%eax, %rdx
	incl	%eax
	movl	4(%rdi,%rdx,4), %ebp
	movl	%ecx, 4(%rdi,%rdx,4)
	movl	%ebp, (%rdi,%rsi,4)
	movl	4(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB42_14
.LBB42_13:                              #   in Loop: Header=BB42_10 Depth=1
	movslq	%eax, %rdx
	incl	%eax
	movl	4(%rdi,%rdx,4), %ebp
	movl	%ecx, 4(%rdi,%rdx,4)
	movl	%ebp, 4(%rdi,%rsi,4)
	movl	8(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB42_16
.LBB42_15:                              #   in Loop: Header=BB42_10 Depth=1
	movslq	%eax, %rdx
	incl	%eax
	movl	4(%rdi,%rdx,4), %ebp
	movl	%ecx, 4(%rdi,%rdx,4)
	movl	%ebp, 8(%rdi,%rsi,4)
	movl	12(%rdi,%rsi,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB42_18
.LBB42_17:                              #   in Loop: Header=BB42_10 Depth=1
	movslq	%eax, %rdx
	incl	%eax
	movl	4(%rdi,%rdx,4), %ebp
	movl	%ecx, 4(%rdi,%rdx,4)
	movl	%ebp, 12(%rdi,%rsi,4)
	jmp	.LBB42_18
.Lfunc_end42:
	.size	partition_quick_standard, .Lfunc_end42-partition_quick_standard
	.cfi_endproc
                                        # -- End function
	.globl	sort_quick_standard     # -- Begin function sort_quick_standard
	.p2align	4, 0x90
	.type	sort_quick_standard,@function
sort_quick_standard:                    # @sort_quick_standard
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%r12
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r12, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
                                        # kill: def $esi killed $esi def $rsi
	cmpl	%esi, %edx
	jle	.LBB43_21
# %bb.1:
	movl	%edx, %r14d
	movq	%rdi, %rbx
	movslq	%edx, %r12
	jmp	.LBB43_2
	.p2align	4, 0x90
.LBB43_20:                              #   in Loop: Header=BB43_2 Depth=1
	movslq	%r10d, %r15
	movl	4(%rbx,%r15,4), %eax
	movl	(%rbx,%r12,4), %ecx
	movl	%ecx, 4(%rbx,%r15,4)
	movl	%eax, (%rbx,%r12,4)
	movq	%rbx, %rdi
                                        # kill: def $esi killed $esi killed $rsi
	movl	%r15d, %edx
	callq	sort_quick_standard
	addl	$2, %r15d
	movl	%r15d, %esi
	cmpl	%r14d, %r15d
	jge	.LBB43_21
.LBB43_2:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB43_7 Depth 2
                                        #     Child Loop BB43_11 Depth 2
	movl	%r14d, %eax
	subl	%esi, %eax
	jle	.LBB43_21
# %bb.3:                                #   in Loop: Header=BB43_2 Depth=1
	cmpl	$1, %eax
	jne	.LBB43_4
# %bb.5:                                #   in Loop: Header=BB43_2 Depth=1
	movl	(%rbx,%r12,4), %r9d
	movslq	%esi, %rax
	jmp	.LBB43_6
	.p2align	4, 0x90
.LBB43_4:                               #   in Loop: Header=BB43_2 Depth=1
	leal	(%rsi,%r14), %eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	movslq	%esi, %rax
	movl	(%rbx,%rax,4), %r10d
	movslq	%ecx, %r8
	movl	(%rbx,%r8,4), %r9d
	movl	(%rbx,%r12,4), %edi
	cmpl	%r9d, %r10d
	movl	%r9d, %ecx
	cmovlel	%r10d, %ecx
	movl	%r9d, %edx
	cmovgel	%r10d, %edx
	cmpl	%edi, %ecx
	cmovgl	%edi, %ecx
	cmpl	%edi, %edx
	cmovll	%edi, %edx
	addl	%r10d, %r9d
	addl	%edi, %r9d
	subl	%ecx, %r9d
	subl	%edx, %r9d
	movl	%ecx, (%rbx,%rax,4)
	movl	%r9d, (%rbx,%r12,4)
	movl	%edx, (%rbx,%r8,4)
.LBB43_6:                               #   in Loop: Header=BB43_2 Depth=1
	leal	-1(%rsi), %r10d
	movl	%r14d, %edi
	subl	%eax, %edi
	movq	%rax, %r8
	notq	%r8
	addq	%r12, %r8
	andq	$3, %rdi
	jne	.LBB43_7
.LBB43_10:                              #   in Loop: Header=BB43_2 Depth=1
	cmpq	$3, %r8
	jae	.LBB43_11
	jmp	.LBB43_20
	.p2align	4, 0x90
.LBB43_9:                               #   in Loop: Header=BB43_7 Depth=2
	incq	%rax
	decq	%rdi
	je	.LBB43_10
.LBB43_7:                               #   Parent Loop BB43_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB43_9
# %bb.8:                                #   in Loop: Header=BB43_7 Depth=2
	movslq	%r10d, %rdx
	incl	%r10d
	movl	4(%rbx,%rdx,4), %r11d
	movl	%ecx, 4(%rbx,%rdx,4)
	movl	%r11d, (%rbx,%rax,4)
	jmp	.LBB43_9
	.p2align	4, 0x90
.LBB43_19:                              #   in Loop: Header=BB43_11 Depth=2
	addq	$4, %rax
	cmpq	%rax, %r12
	je	.LBB43_20
.LBB43_11:                              #   Parent Loop BB43_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movl	(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jle	.LBB43_12
# %bb.13:                               #   in Loop: Header=BB43_11 Depth=2
	movl	4(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jle	.LBB43_14
.LBB43_15:                              #   in Loop: Header=BB43_11 Depth=2
	movl	8(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jle	.LBB43_16
.LBB43_17:                              #   in Loop: Header=BB43_11 Depth=2
	movl	12(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB43_19
	jmp	.LBB43_18
	.p2align	4, 0x90
.LBB43_12:                              #   in Loop: Header=BB43_11 Depth=2
	movslq	%r10d, %rdx
	incl	%r10d
	movl	4(%rbx,%rdx,4), %edi
	movl	%ecx, 4(%rbx,%rdx,4)
	movl	%edi, (%rbx,%rax,4)
	movl	4(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB43_15
.LBB43_14:                              #   in Loop: Header=BB43_11 Depth=2
	movslq	%r10d, %rdx
	incl	%r10d
	movl	4(%rbx,%rdx,4), %edi
	movl	%ecx, 4(%rbx,%rdx,4)
	movl	%edi, 4(%rbx,%rax,4)
	movl	8(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB43_17
.LBB43_16:                              #   in Loop: Header=BB43_11 Depth=2
	movslq	%r10d, %rdx
	incl	%r10d
	movl	4(%rbx,%rdx,4), %edi
	movl	%ecx, 4(%rbx,%rdx,4)
	movl	%edi, 8(%rbx,%rax,4)
	movl	12(%rbx,%rax,4), %ecx
	cmpl	%r9d, %ecx
	jg	.LBB43_19
.LBB43_18:                              #   in Loop: Header=BB43_11 Depth=2
	movslq	%r10d, %rdx
	incl	%r10d
	movl	4(%rbx,%rdx,4), %edi
	movl	%ecx, 4(%rbx,%rdx,4)
	movl	%edi, 12(%rbx,%rax,4)
	jmp	.LBB43_19
.LBB43_21:
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end43:
	.size	sort_quick_standard, .Lfunc_end43-sort_quick_standard
	.cfi_endproc
                                        # -- End function
	.globl	shuffle_data            # -- Begin function shuffle_data
	.p2align	4, 0x90
	.type	shuffle_data,@function
shuffle_data:                           # @shuffle_data
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	pushq	%rax
	.cfi_def_cfa_offset 48
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	testl	%esi, %esi
	je	.LBB44_6
# %bb.1:
	movl	%esi, %r14d
	movq	%rdi, %rbx
	movslq	%esi, %r15
	xorl	%ebp, %ebp
	jmp	.LBB44_2
	.p2align	4, 0x90
.LBB44_4:                               #   in Loop: Header=BB44_2 Depth=1
	xorl	%edx, %edx
	divq	%r15
.LBB44_5:                               #   in Loop: Header=BB44_2 Depth=1
	movl	(%rbx,%rdx,4), %eax
	movl	(%rbx,%rbp,4), %ecx
	movl	%ecx, (%rbx,%rdx,4)
	movl	%eax, (%rbx,%rbp,4)
	incq	%rbp
	cmpq	%r15, %rbp
	jae	.LBB44_6
.LBB44_2:                               # =>This Inner Loop Header: Depth=1
	callq	rand
	cltq
	addq	%rbp, %rax
	movq	%rax, %rcx
	orq	%r15, %rcx
	shrq	$32, %rcx
	jne	.LBB44_4
# %bb.3:                                #   in Loop: Header=BB44_2 Depth=1
                                        # kill: def $eax killed $eax killed $rax
	xorl	%edx, %edx
	divl	%r14d
                                        # kill: def $edx killed $edx def $rdx
	jmp	.LBB44_5
.LBB44_6:
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end44:
	.size	shuffle_data, .Lfunc_end44-shuffle_data
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2               # -- Begin function bench
.LCPI45_0:
	.long	1232348160              # float 1.0E+6
.LCPI45_1:
	.long	1148846080              # float 1000
	.text
	.globl	bench
	.p2align	4, 0x90
	.type	bench,@function
bench:                                  # @bench
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$40, %rsp
	.cfi_def_cfa_offset 96
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, 16(%rsp)          # 8-byte Spill
	movq	%rdi, %rsi
	cmpl	$10, %edx
	jne	.LBB45_1
# %bb.16:
	movl	$.L.str, %edi
	xorl	%eax, %eax
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	printf                  # TAILCALL
.LBB45_1:
	.cfi_def_cfa_offset 96
	movq	%rcx, %rbx
	movq	%rsi, 32(%rsp)          # 8-byte Spill
	movslq	%edx, %r15
	movl	%edx, 12(%rsp)          # 4-byte Spill
	subl	$1, %edx
	movl	%edx, 28(%rsp)          # 4-byte Spill
	movslq	%edx, %r12
	jae	.LBB45_2
# %bb.6:
	xorl	%r14d, %r14d
	xorl	%r13d, %r13d
	.p2align	4, 0x90
.LBB45_7:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB45_12 Depth 2
	rdtsc
	movq	%rdx, %rbp
	shlq	$32, %rbp
	orq	%rax, %rbp
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movl	28(%rsp), %edx          # 4-byte Reload
	callq	*16(%rsp)               # 8-byte Folded Reload
	rdtsc
	shlq	$32, %rdx
	orq	%rax, %rdx
	subq	%rbp, %rdx
	js	.LBB45_8
# %bb.9:                                #   in Loop: Header=BB45_7 Depth=1
	vcvtsi2ss	%rdx, %xmm1, %xmm0
	jmp	.LBB45_10
	.p2align	4, 0x90
.LBB45_8:                               #   in Loop: Header=BB45_7 Depth=1
	movq	%rdx, %rax
	shrq	%rax
	movl	%edx, %ecx
	andl	$1, %ecx
	orq	%rax, %rcx
	vcvtsi2ss	%rcx, %xmm1, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB45_10:                              #   in Loop: Header=BB45_7 Depth=1
	vdivss	.LCPI45_0(%rip), %xmm0, %xmm0
	vmulss	.LCPI45_1(%rip), %xmm0, %xmm0
	vmovss	%xmm0, individual_times(,%r14,4)
	testq	%r14, %r14
	je	.LBB45_11
.LBB45_13:                              #   in Loop: Header=BB45_7 Depth=1
	addq	%rdx, %r13
	cmpq	$400000000, %r13        # imm = 0x17D78400
	setl	%al
	cmpq	$9, %r14
	setb	%cl
	cmpq	$998, %r14              # imm = 0x3E6
	leaq	1(%r14), %r14
	ja	.LBB45_32
# %bb.14:                               #   in Loop: Header=BB45_7 Depth=1
	orb	%al, %cl
	jne	.LBB45_7
	jmp	.LBB45_32
	.p2align	4, 0x90
.LBB45_11:                              #   in Loop: Header=BB45_7 Depth=1
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB45_12:                              #   Parent Loop BB45_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%r12, %rbp
	jge	.LBB45_13
# %bb.15:                               #   in Loop: Header=BB45_12 Depth=2
	movl	(%rbx,%rbp,4), %eax
	cmpl	4(%rbx,%rbp,4), %eax
	leaq	1(%rbp), %rbp
	jle	.LBB45_12
	jmp	.LBB45_26
.LBB45_2:
	xorl	%r14d, %r14d
	xorl	%r13d, %r13d
	.p2align	4, 0x90
.LBB45_3:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB45_4 Depth 2
                                        #     Child Loop BB45_24 Depth 2
	xorl	%ebp, %ebp
	jmp	.LBB45_4
	.p2align	4, 0x90
.LBB45_17:                              #   in Loop: Header=BB45_4 Depth=2
	xorl	%edx, %edx
	divq	%r15
.LBB45_18:                              #   in Loop: Header=BB45_4 Depth=2
	movl	(%rbx,%rdx,4), %eax
	movl	(%rbx,%rbp,4), %ecx
	movl	%ecx, (%rbx,%rdx,4)
	movl	%eax, (%rbx,%rbp,4)
	incq	%rbp
	cmpq	%r15, %rbp
	jae	.LBB45_19
.LBB45_4:                               #   Parent Loop BB45_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	callq	rand
	cltq
	addq	%rbp, %rax
	movq	%rax, %rcx
	orq	%r15, %rcx
	shrq	$32, %rcx
	jne	.LBB45_17
# %bb.5:                                #   in Loop: Header=BB45_4 Depth=2
                                        # kill: def $eax killed $eax killed $rax
	xorl	%edx, %edx
	divl	12(%rsp)                # 4-byte Folded Reload
                                        # kill: def $edx killed $edx def $rdx
	jmp	.LBB45_18
	.p2align	4, 0x90
.LBB45_19:                              #   in Loop: Header=BB45_3 Depth=1
	rdtsc
	movq	%rdx, %rbp
	shlq	$32, %rbp
	orq	%rax, %rbp
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movl	28(%rsp), %edx          # 4-byte Reload
	callq	*16(%rsp)               # 8-byte Folded Reload
	rdtsc
	shlq	$32, %rdx
	orq	%rax, %rdx
	subq	%rbp, %rdx
	js	.LBB45_20
# %bb.21:                               #   in Loop: Header=BB45_3 Depth=1
	vcvtsi2ss	%rdx, %xmm1, %xmm0
	jmp	.LBB45_22
	.p2align	4, 0x90
.LBB45_20:                              #   in Loop: Header=BB45_3 Depth=1
	movq	%rdx, %rax
	shrq	%rax
	movl	%edx, %ecx
	andl	$1, %ecx
	orq	%rax, %rcx
	vcvtsi2ss	%rcx, %xmm1, %xmm0
	vaddss	%xmm0, %xmm0, %xmm0
.LBB45_22:                              #   in Loop: Header=BB45_3 Depth=1
	vdivss	.LCPI45_0(%rip), %xmm0, %xmm0
	vmulss	.LCPI45_1(%rip), %xmm0, %xmm0
	vmovss	%xmm0, individual_times(,%r14,4)
	testq	%r14, %r14
	je	.LBB45_23
.LBB45_30:                              #   in Loop: Header=BB45_3 Depth=1
	addq	%rdx, %r13
	cmpq	$400000000, %r13        # imm = 0x17D78400
	setl	%al
	cmpq	$9, %r14
	setb	%cl
	cmpq	$998, %r14              # imm = 0x3E6
	leaq	1(%r14), %r14
	ja	.LBB45_32
# %bb.31:                               #   in Loop: Header=BB45_3 Depth=1
	orb	%al, %cl
	jne	.LBB45_3
	jmp	.LBB45_32
	.p2align	4, 0x90
.LBB45_23:                              #   in Loop: Header=BB45_3 Depth=1
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB45_24:                              #   Parent Loop BB45_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%r12, %rbp
	jge	.LBB45_30
# %bb.25:                               #   in Loop: Header=BB45_24 Depth=2
	movl	(%rbx,%rbp,4), %eax
	cmpl	4(%rbx,%rbp,4), %eax
	leaq	1(%rbp), %rbp
	jle	.LBB45_24
.LBB45_26:
	movl	$.L.str.3, %edi
	xorl	%eax, %eax
	callq	printf
	cmpl	$0, 12(%rsp)            # 4-byte Folded Reload
	js	.LBB45_29
# %bb.27:
	incq	%r15
	xorl	%r14d, %r14d
.LBB45_28:                              # =>This Inner Loop Header: Depth=1
	movl	(%rbx,%r14,4), %esi
	movl	$.L.str.2, %edi
	xorl	%eax, %eax
	callq	printf
	incq	%r14
	cmpq	%r14, %r15
	jne	.LBB45_28
.LBB45_29:
	movl	$.L.str.4, %edi
	movq	32(%rsp), %rsi          # 8-byte Reload
	movl	%ebp, %edx
	xorl	%eax, %eax
	callq	printf
	movl	$1, %edi
	callq	exit
.LBB45_32:
	vcvtsi2ss	%r13, %xmm1, %xmm0
	vmulss	.LCPI45_1(%rip), %xmm0, %xmm0
	vdivss	.LCPI45_0(%rip), %xmm0, %xmm1
	vcvtsi2ss	%r14d, %xmm2, %xmm0
	vdivss	%xmm0, %xmm1, %xmm3
	leaq	-1(%r14), %rcx
	movl	%r14d, %eax
	andl	$7, %eax
	cmpq	$7, %rcx
	jae	.LBB45_79
# %bb.33:
	vxorps	%xmm1, %xmm1, %xmm1
	xorl	%ecx, %ecx
	jmp	.LBB45_34
.LBB45_79:
	subq	%rax, %r14
	vxorps	%xmm1, %xmm1, %xmm1
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB45_80:                              # =>This Inner Loop Header: Depth=1
	vsubss	individual_times(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+4(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+8(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+12(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+16(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+20(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+24(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	vsubss	individual_times+28(,%rcx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	addq	$8, %rcx
	cmpq	%rcx, %r14
	jne	.LBB45_80
.LBB45_34:
	testq	%rax, %rax
	je	.LBB45_37
# %bb.35:
	leaq	individual_times(,%rcx,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB45_36:                              # =>This Inner Loop Header: Depth=1
	vsubss	(%rcx,%rdx,4), %xmm3, %xmm2
	vmulss	%xmm2, %xmm2, %xmm2
	vaddss	%xmm2, %xmm1, %xmm1
	incq	%rdx
	cmpq	%rdx, %rax
	jne	.LBB45_36
.LBB45_37:
	vmovss	%xmm3, 12(%rsp)         # 4-byte Spill
	vdivss	%xmm0, %xmm1, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm0
	jb	.LBB45_39
# %bb.38:
	vsqrtss	%xmm0, %xmm0, %xmm0
	jmp	.LBB45_40
.LBB45_39:
	callq	sqrtf
.LBB45_40:
	vmovss	%xmm0, 16(%rsp)         # 4-byte Spill
	movl	$20, %edi
	callq	malloc
	movq	%rax, %rbx
	vmovss	12(%rsp), %xmm0         # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm0, %xmm0, %xmm0
	vmovss	16(%rsp), %xmm1         # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vcvtss2sd	%xmm1, %xmm1, %xmm1
	movl	$.L.str.5, %esi
	movq	%rax, %rdi
	movb	$2, %al
	callq	sprintf
	cmpb	$0, (%rbx)
	je	.LBB45_41
# %bb.42:
	cmpb	$0, 1(%rbx)
	je	.LBB45_43
.LBB45_44:
	cmpb	$0, 2(%rbx)
	je	.LBB45_45
.LBB45_46:
	cmpb	$0, 3(%rbx)
	je	.LBB45_47
.LBB45_48:
	cmpb	$0, 4(%rbx)
	je	.LBB45_49
.LBB45_50:
	cmpb	$0, 5(%rbx)
	je	.LBB45_51
.LBB45_52:
	cmpb	$0, 6(%rbx)
	je	.LBB45_53
.LBB45_54:
	cmpb	$0, 7(%rbx)
	je	.LBB45_55
.LBB45_56:
	cmpb	$0, 8(%rbx)
	je	.LBB45_57
.LBB45_58:
	cmpb	$0, 9(%rbx)
	je	.LBB45_59
.LBB45_60:
	cmpb	$0, 10(%rbx)
	je	.LBB45_61
.LBB45_62:
	cmpb	$0, 11(%rbx)
	je	.LBB45_63
.LBB45_64:
	cmpb	$0, 12(%rbx)
	je	.LBB45_65
.LBB45_66:
	cmpb	$0, 13(%rbx)
	je	.LBB45_67
.LBB45_68:
	cmpb	$0, 14(%rbx)
	je	.LBB45_69
.LBB45_70:
	cmpb	$0, 15(%rbx)
	je	.LBB45_71
.LBB45_72:
	cmpb	$0, 16(%rbx)
	je	.LBB45_73
.LBB45_74:
	cmpb	$0, 17(%rbx)
	je	.LBB45_75
.LBB45_76:
	cmpb	$0, 18(%rbx)
	je	.LBB45_77
.LBB45_78:
	movl	$.L.str, %edi
	movq	%rbx, %rsi
	xorl	%eax, %eax
	callq	printf
	movq	%rbx, %rdi
	addq	$40, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	free                    # TAILCALL
.LBB45_41:
	.cfi_def_cfa_offset 96
	movb	$32, (%rbx)
	cmpb	$0, 1(%rbx)
	jne	.LBB45_44
.LBB45_43:
	movb	$32, 1(%rbx)
	cmpb	$0, 2(%rbx)
	jne	.LBB45_46
.LBB45_45:
	movb	$32, 2(%rbx)
	cmpb	$0, 3(%rbx)
	jne	.LBB45_48
.LBB45_47:
	movb	$32, 3(%rbx)
	cmpb	$0, 4(%rbx)
	jne	.LBB45_50
.LBB45_49:
	movb	$32, 4(%rbx)
	cmpb	$0, 5(%rbx)
	jne	.LBB45_52
.LBB45_51:
	movb	$32, 5(%rbx)
	cmpb	$0, 6(%rbx)
	jne	.LBB45_54
.LBB45_53:
	movb	$32, 6(%rbx)
	cmpb	$0, 7(%rbx)
	jne	.LBB45_56
.LBB45_55:
	movb	$32, 7(%rbx)
	cmpb	$0, 8(%rbx)
	jne	.LBB45_58
.LBB45_57:
	movb	$32, 8(%rbx)
	cmpb	$0, 9(%rbx)
	jne	.LBB45_60
.LBB45_59:
	movb	$32, 9(%rbx)
	cmpb	$0, 10(%rbx)
	jne	.LBB45_62
.LBB45_61:
	movb	$32, 10(%rbx)
	cmpb	$0, 11(%rbx)
	jne	.LBB45_64
.LBB45_63:
	movb	$32, 11(%rbx)
	cmpb	$0, 12(%rbx)
	jne	.LBB45_66
.LBB45_65:
	movb	$32, 12(%rbx)
	cmpb	$0, 13(%rbx)
	jne	.LBB45_68
.LBB45_67:
	movb	$32, 13(%rbx)
	cmpb	$0, 14(%rbx)
	jne	.LBB45_70
.LBB45_69:
	movb	$32, 14(%rbx)
	cmpb	$0, 15(%rbx)
	jne	.LBB45_72
.LBB45_71:
	movb	$32, 15(%rbx)
	cmpb	$0, 16(%rbx)
	jne	.LBB45_74
.LBB45_73:
	movb	$32, 16(%rbx)
	cmpb	$0, 17(%rbx)
	jne	.LBB45_76
.LBB45_75:
	movb	$32, 17(%rbx)
	cmpb	$0, 18(%rbx)
	jne	.LBB45_78
.LBB45_77:
	movb	$32, 18(%rbx)
	jmp	.LBB45_78
.Lfunc_end45:
	.size	bench, .Lfunc_end45-bench
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5               # -- Begin function main
.LCPI46_0:
	.quad	4                       # 0x4
	.quad	5                       # 0x5
	.quad	6                       # 0x6
	.quad	7                       # 0x7
.LCPI46_1:
	.quad	0                       # 0x0
	.quad	1                       # 0x1
	.quad	2                       # 0x2
	.quad	3                       # 0x3
.LCPI46_2:
	.long	0                       # 0x0
	.long	2                       # 0x2
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	4                       # 0x4
	.long	6                       # 0x6
	.long	6                       # 0x6
	.long	7                       # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI46_3:
	.long	9                       # 0x9
.LCPI46_4:
	.long	17                      # 0x11
.LCPI46_5:
	.long	25                      # 0x19
.LCPI46_6:
	.long	33                      # 0x21
.LCPI46_7:
	.long	41                      # 0x29
.LCPI46_8:
	.long	49                      # 0x31
.LCPI46_9:
	.long	57                      # 0x39
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI46_10:
	.quad	64                      # 0x40
	.text
	.globl	main
	.p2align	4, 0x90
	.type	main,@function
main:                                   # @main
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	subq	$8000000, %rsp          # imm = 0x7A1200
	.cfi_def_cfa_offset 8000032
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %rbp, -16
	movl	$.Lstr, %edi
	callq	puts
	movl	$.Lstr.22, %edi
	callq	puts
	callq	clock
	movl	%eax, %edi
	callq	srand
	vmovdqa	.LCPI46_0(%rip), %ymm0  # ymm0 = [4,5,6,7]
	vmovdqa	.LCPI46_1(%rip), %ymm1  # ymm1 = [0,1,2,3]
	movl	$56, %eax
	vmovdqa	.LCPI46_2(%rip), %ymm2  # ymm2 = [0,2,4,6,4,6,6,7]
	vpcmpeqd	%ymm3, %ymm3, %ymm3
	vpbroadcastd	.LCPI46_3(%rip), %ymm4 # ymm4 = [9,9,9,9,9,9,9,9]
	vpbroadcastd	.LCPI46_4(%rip), %ymm5 # ymm5 = [17,17,17,17,17,17,17,17]
	vpbroadcastd	.LCPI46_5(%rip), %ymm6 # ymm6 = [25,25,25,25,25,25,25,25]
	vpbroadcastd	.LCPI46_6(%rip), %ymm7 # ymm7 = [33,33,33,33,33,33,33,33]
	vpbroadcastd	.LCPI46_7(%rip), %ymm8 # ymm8 = [41,41,41,41,41,41,41,41]
	vpbroadcastd	.LCPI46_8(%rip), %ymm9 # ymm9 = [49,49,49,49,49,49,49,49]
	vpbroadcastd	.LCPI46_9(%rip), %ymm10 # ymm10 = [57,57,57,57,57,57,57,57]
	vpbroadcastq	.LCPI46_10(%rip), %ymm11 # ymm11 = [64,64,64,64]
	.p2align	4, 0x90
.LBB46_1:                               # =>This Inner Loop Header: Depth=1
	vpermd	%ymm1, %ymm2, %ymm12
	vpermd	%ymm0, %ymm2, %ymm13
	vinserti128	$1, %xmm13, %ymm12, %ymm12
	vpsubd	%ymm3, %ymm12, %ymm13
	vmovdqu	%ymm13, -224(%rsp,%rax,4)
	vpaddd	%ymm4, %ymm12, %ymm13
	vmovdqu	%ymm13, -192(%rsp,%rax,4)
	vpaddd	%ymm5, %ymm12, %ymm13
	vmovdqu	%ymm13, -160(%rsp,%rax,4)
	vpaddd	%ymm6, %ymm12, %ymm13
	vmovdqu	%ymm13, -128(%rsp,%rax,4)
	vpaddd	%ymm7, %ymm12, %ymm13
	vmovdqu	%ymm13, -96(%rsp,%rax,4)
	vpaddd	%ymm8, %ymm12, %ymm13
	vmovdqu	%ymm13, -64(%rsp,%rax,4)
	vpaddd	%ymm9, %ymm12, %ymm13
	vmovdqu	%ymm13, -32(%rsp,%rax,4)
	vpaddd	%ymm10, %ymm12, %ymm12
	vmovdqu	%ymm12, (%rsp,%rax,4)
	vpaddq	%ymm1, %ymm11, %ymm1
	vpaddq	%ymm0, %ymm11, %ymm0
	addq	$64, %rax
	cmpq	$2000056, %rax          # imm = 0x1E84B8
	jne	.LBB46_1
# %bb.2:
	movq	$-24, %r14
	movq	%rsp, %rbx
	.p2align	4, 0x90
.LBB46_3:                               # =>This Inner Loop Header: Depth=1
	movl	.L__const.main.array_sizes+24(%r14), %ebp
	movl	$.L.str.8, %edi
	movl	$sort_quick_standard, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	vzeroupper
	callq	bench
	movl	$.L.str.9, %edi
	movl	$sort_quick_optimized, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.10, %edi
	movl	$sort_quick_simd, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.11, %edi
	movl	$sort_quick_optimized_swap_arith, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.12, %edi
	movl	$sort_quick_optimized_swap_cmov, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.13, %edi
	movl	$sort_quick_optimized_swap_array, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.14, %edi
	movl	$sort_quick_optimized_swap_asm, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.15, %edi
	movl	$sort_quick_multi, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.16, %edi
	movl	$sort_quick_multix4, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.17, %edi
	movl	$sort_quick_multi_inplace, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.18, %edi
	movl	$sort_quick_optimized_dual, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.19, %edi
	movl	$sort_quick_block, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.20, %edi
	movl	$sort_merge_standard, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$.L.str.21, %edi
	movl	$sort_merge_optimized, %esi
	movl	%ebp, %edx
	movq	%rbx, %rcx
	callq	bench
	movl	$10, %edi
	callq	putchar
	addq	$4, %r14
	jne	.LBB46_3
# %bb.4:
	xorl	%eax, %eax
	addq	$8000000, %rsp          # imm = 0x7A1200
	.cfi_def_cfa_offset 32
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end46:
	.size	main, .Lfunc_end46-main
	.cfi_endproc
                                        # -- End function
	.type	INSERTION_SORT_THRESH_BLOCK,@object # @INSERTION_SORT_THRESH_BLOCK
	.section	.rodata,"a",@progbits
	.globl	INSERTION_SORT_THRESH_BLOCK
	.p2align	2
INSERTION_SORT_THRESH_BLOCK:
	.long	20                      # 0x14
	.size	INSERTION_SORT_THRESH_BLOCK, 4

	.type	blocksize,@object       # @blocksize
	.globl	blocksize
	.p2align	2
blocksize:
	.long	128                     # 0x80
	.size	blocksize, 4

	.type	b,@object               # @b
	.comm	b,4000000,16
	.type	x,@object               # @x
	.comm	x,8,4
	.type	DATA_AMOUNT,@object     # @DATA_AMOUNT
	.globl	DATA_AMOUNT
	.p2align	2
DATA_AMOUNT:
	.long	2000000                 # 0x1e8480
	.size	DATA_AMOUNT, 4

	.type	MIN_RUNS_PER_BENCH,@object # @MIN_RUNS_PER_BENCH
	.globl	MIN_RUNS_PER_BENCH
	.p2align	2
MIN_RUNS_PER_BENCH:
	.long	10                      # 0xa
	.size	MIN_RUNS_PER_BENCH, 4

	.type	MAX_RUNS_PER_BENCH,@object # @MAX_RUNS_PER_BENCH
	.globl	MAX_RUNS_PER_BENCH
	.p2align	2
MAX_RUNS_PER_BENCH:
	.long	1000                    # 0x3e8
	.size	MAX_RUNS_PER_BENCH, 4

	.type	.L.str,@object          # @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.asciz	"%s"
	.size	.L.str, 3

	.type	individual_times,@object # @individual_times
	.comm	individual_times,4000,16
	.type	.L.str.2,@object        # @.str.2
.L.str.2:
	.asciz	"%d "
	.size	.L.str.2, 4

	.type	.L.str.3,@object        # @.str.3
.L.str.3:
	.asciz	"\nArray "
	.size	.L.str.3, 8

	.type	.L.str.4,@object        # @.str.4
.L.str.4:
	.asciz	"Integrity check failed for %s at index %d\n"
	.size	.L.str.4, 43

	.type	.L.str.5,@object        # @.str.5
.L.str.5:
	.asciz	"%.2f\302\261%.2f"
	.size	.L.str.5, 11

	.type	.L__const.main.array_sizes,@object # @__const.main.array_sizes
	.section	.rodata,"a",@progbits
	.p2align	4
.L__const.main.array_sizes:
	.long	10                      # 0xa
	.long	100                     # 0x64
	.long	1000                    # 0x3e8
	.long	10000                   # 0x2710
	.long	100000                  # 0x186a0
	.long	1000000                 # 0xf4240
	.size	.L__const.main.array_sizes, 24

	.type	.L.str.8,@object        # @.str.8
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.8:
	.asciz	"QStd              "
	.size	.L.str.8, 19

	.type	.L.str.9,@object        # @.str.9
.L.str.9:
	.asciz	"QOpt              "
	.size	.L.str.9, 19

	.type	.L.str.10,@object       # @.str.10
.L.str.10:
	.asciz	"QSIMD             "
	.size	.L.str.10, 19

	.type	.L.str.11,@object       # @.str.11
.L.str.11:
	.asciz	"QArith            "
	.size	.L.str.11, 19

	.type	.L.str.12,@object       # @.str.12
.L.str.12:
	.asciz	"QCMov             "
	.size	.L.str.12, 19

	.type	.L.str.13,@object       # @.str.13
.L.str.13:
	.asciz	"QArray            "
	.size	.L.str.13, 19

	.type	.L.str.14,@object       # @.str.14
.L.str.14:
	.asciz	"QAsm              "
	.size	.L.str.14, 19

	.type	.L.str.15,@object       # @.str.15
.L.str.15:
	.asciz	"QMult             "
	.size	.L.str.15, 19

	.type	.L.str.16,@object       # @.str.16
.L.str.16:
	.asciz	"QMultx4           "
	.size	.L.str.16, 19

	.type	.L.str.17,@object       # @.str.17
.L.str.17:
	.asciz	"QMultInplace      "
	.size	.L.str.17, 19

	.type	.L.str.18,@object       # @.str.18
.L.str.18:
	.asciz	"QDual             "
	.size	.L.str.18, 19

	.type	.L.str.19,@object       # @.str.19
.L.str.19:
	.asciz	"QBlock            "
	.size	.L.str.19, 19

	.type	.L.str.20,@object       # @.str.20
.L.str.20:
	.asciz	"MSStd             "
	.size	.L.str.20, 19

	.type	.L.str.21,@object       # @.str.21
.L.str.21:
	.asciz	"MSOpt             "
	.size	.L.str.21, 19

	.type	.Lstr,@object           # @str
.Lstr:
	.asciz	"Starting"
	.size	.Lstr, 9

	.type	.Lstr.22,@object        # @str.22
.Lstr.22:
	.asciz	"Generating random data"
	.size	.Lstr.22, 23

	.ident	"clang version 10.0.0-4ubuntu1 "
	.section	".note.GNU-stack","",@progbits
	.addrsig
	.addrsig_sym sort_quick_block
	.addrsig_sym sort_merge_optimized
	.addrsig_sym sort_merge_standard
	.addrsig_sym sort_quick_optimized
	.addrsig_sym sort_quick_optimized_dual
	.addrsig_sym sort_quick_multi
	.addrsig_sym sort_quick_multi_inplace
	.addrsig_sym sort_quick_multix4
	.addrsig_sym sort_quick_simd
	.addrsig_sym sort_quick_optimized_swap_arith
	.addrsig_sym sort_quick_optimized_swap_array
	.addrsig_sym sort_quick_optimized_swap_asm
	.addrsig_sym sort_quick_optimized_swap_cmov
	.addrsig_sym sort_quick_standard
